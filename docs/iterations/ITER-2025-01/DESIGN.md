# è®¾è®¡æ–‡æ¡£ï¼ˆITER-2025-01ï¼‰

> æœ¬æ–‡æ¡£åŸºäº `docs/templates/DESIGN_TEMPLATE.md` å¡«å†™ï¼Œèšç„¦çº¢ç¯åœ MVP çš„ç³»ç»Ÿ/æ¦‚è¦/è¯¦ç»†è®¾è®¡ï¼Œå ä½ç¬¦å¾…æ¶æ„è¯„å®¡è¡¥å…¨ã€‚

## å…ƒæ•°æ®
| å­—æ®µ | å†…å®¹ |
| --- | --- |
| æ–‡æ¡£ç‰ˆæœ¬ | v2.1ï¼ˆä»£ç æ˜ å°„å¯¹é½ï¼‰ |
| åŸç‰ˆæœ¬ | v2.0ï¼ˆç³»ç»Ÿæ€§é‡æ„ï¼‰ï¼Œv1.0ï¼ˆåˆç‰ˆï¼‰ |
| è¿­ä»£ç¼–å· | ITER-2025-01 |
| æ¶æ„è´Ÿè´£äºº | ç®—æ³•æ¶æ„å¸ˆï¼ˆAIï¼‰ |
| çŠ¶æ€ | âœ… é‡æ„å®Œæˆï¼ŒğŸŸ¡ å®ç°85%å®Œæˆ |
| æœ€åæ›´æ–°æ—¶é—´ | 2025-12-16ï¼ˆä»£ç æ˜ å°„å¯¹é½ + å®ç°çŠ¶æ€æ ‡è®°ï¼‰ |
| å…³è”éœ€æ±‚ | `docs/iterations/ITER-2025-01/REQUIREMENT.md` |
| å…³è”å¼€å‘ | `docs/iterations/ITER-2025-01/DEVELOPMENT.md` |
| å…³è”æµ‹è¯• | `docs/iterations/ITER-2025-01/TESTING.md` |
| å…³è”ç®—æ³•æ–¹æ¡ˆ | `docs/archive/design/ALGORITHM_DESIGN_OPTIONS.md` |
| é‡æ„è¿½è¸ª | `docs/archive/design/DESIGN_REFACTOR_TRACKER.md` |
| å®¡æ‰¹è®°å½• | 2025-12-03 ç®—æ³•ç»†åŒ–å®Œæˆï¼Œé‡‡ç”¨æ–¹æ¡ˆ1ï¼ˆå¤šé˜¶æ®µGAT+ç¡¬çº¦æŸï¼‰<br/>2025-12-03 ç³»ç»Ÿæ€§é‡æ„å®Œæˆï¼Œä¿®æ­£10ä¸ªè®¾è®¡é—®é¢˜ |

## 1. ç³»ç»Ÿè®¾è®¡

### 1.1 æ¶æ„æ¦‚è§ˆ

> **é‡å¤§ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šé‡ç»˜å®Œæ•´æ¶æ„å›¾ï¼Œæ·»åŠ æ‰€æœ‰ç¼ºå¤±æ¨¡å—ï¼ˆmemoryã€è‡ªè®­ç»ƒã€ç›‘æ§ï¼‰ï¼Œæ˜ç¡®æ•°æ®æµå‘å’Œæ§åˆ¶æµã€‚  
> è§£å†³é—®é¢˜8-9ï¼šæ¶æ„å›¾ä¸åŒ…ç»“æ„ä¸ä¸€è‡´

- æœ¬è¿­ä»£äº¤ä»˜ "çº¢ç¯åœ" æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹é—­ç¯ï¼š`æ•°æ®æ‘„å– â†’ åœºæ™¯å›¾æ„å»º â†’ å¤šé˜¶æ®µæ³¨æ„åŠ›GNN + Memory â†’ è§„åˆ™å¼•æ“ â†’ çº¦æŸæŸå¤± â†’ è‡ªè®­ç»ƒ â†’ è¿è§„è¯„åˆ†&è§£é‡Š â†’ ç›‘æ§/æŠ¥å‘Š`ã€‚
- æŠ€æœ¯æ ˆç»Ÿä¸€ä¸º Python 3.12 + PyTorch 2.4+ï¼ˆé€šè¿‡ Conda ç®¡ç†ï¼‰ï¼Œé»˜è®¤éƒ¨ç½²åœ¨ RTX 4090 GPU ä¸Šï¼Œæ‰€æœ‰æ¨¡å—å°è£…ä¸ºä¸šåŠ¡åŒ…ï¼ŒCLI å·¥å…·é©±åŠ¨è®­ç»ƒ/æµ‹è¯•ã€‚
- æ ¸å¿ƒæœåŠ¡è¿è¡Œåœ¨ç¦»çº¿æ‰¹å¤„ç†æ¨¡å¼ï¼Œåç»­å¯æ‰©å±•ä¸ºé•¿é©»æ¨ç†æœåŠ¡ã€‚

**å®Œæ•´ç³»ç»Ÿæ¶æ„å›¾**ï¼ˆMermaidï¼‰ï¼š

```mermaid
graph TB
    %% é…ç½®å±‚
    CONFIG[configs/mvp.yaml<br/>é…ç½®ç®¡ç†] --> TRAIN[tools/train_red_light.py<br/>è®­ç»ƒç¼–æ’å™¨]
    CONFIG --> TEST[tools/test_red_light.py<br/>æµ‹è¯•ç¼–æ’å™¨]
    
    %% æ•°æ®å±‚
    TRAIN --> DATA[src/data/traffic.py<br/>æ•°æ®åŠ è½½å™¨]
    TEST --> DATA
    DATA --> GRAPH[src/graph/builder.py<br/>åœºæ™¯å›¾æ„å»º]
    
    %% æ¨¡å‹å±‚ï¼ˆä¸‰é˜¶æ®µæ³¨æ„åŠ›ï¼‰
    GRAPH --> MODEL[src/models/gat_attention.py<br/>å¤šé˜¶æ®µæ³¨æ„åŠ›GAT]
    
    subgraph MODEL_DETAIL [æ¨¡å‹å†…éƒ¨ç»“æ„]
        GAT[é˜¶æ®µ1: å±€éƒ¨GAT<br/>3å±‚Ã—8å¤´]
        GLOBAL[é˜¶æ®µ2: å…¨å±€æ³¨æ„åŠ›<br/>è™šæ‹ŸèŠ‚ç‚¹]
        RULE_FOCUS[é˜¶æ®µ3: è§„åˆ™èšç„¦<br/>rule embedding]
        MEMORY[Memory Bank<br/>å¯é€‰æ¨¡å—]
        
        GAT --> GLOBAL
        GLOBAL --> MEMORY
        MEMORY --> RULE_FOCUS
        RULE_FOCUS --> SCORE[Scoring Head<br/>å¼‚å¸¸åˆ†æ•°]
    end
    
    MODEL --> GAT
    
    %% è§„åˆ™å¼•æ“
    GRAPH --> RULE_ENGINE[src/rules/red_light.py<br/>è§„åˆ™è¯„åˆ†å¼•æ“]
    
    %% æŸå¤±è®¡ç®—
    SCORE --> LOSS[src/loss/constraint.py<br/>çº¦æŸæŸå¤±è®¡ç®—]
    RULE_ENGINE --> LOSS
    GAT -.æ³¨æ„åŠ›æƒé‡.-> LOSS
    RULE_FOCUS -.è§„åˆ™æ³¨æ„åŠ›.-> LOSS
    
    %% è‡ªè®­ç»ƒå¾ªç¯
    LOSS --> TRAIN
    SCORE --> PSEUDO[src/self_training/pseudo_labeler.py<br/>ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨]
    RULE_ENGINE --> PSEUDO
    PSEUDO -.ä¼ªæ ‡ç­¾æ•°æ®.-> DATA
    
    %% å¯è§£é‡Šæ€§
    SCORE --> EXPLAIN[src/explain/attention_viz.py<br/>æ³¨æ„åŠ›å¯è§†åŒ–]
    GAT -.æ³¨æ„åŠ›æƒé‡.-> EXPLAIN
    RULE_FOCUS -.è§„åˆ™æ³¨æ„åŠ›.-> EXPLAIN
    EXPLAIN --> REPORT[reports/*.png<br/>æ³¨æ„åŠ›çƒ­åŠ›å›¾]
    
    %% ç›‘æ§ç³»ç»Ÿ
    LOSS -.æŒ‡æ ‡.-> MONITOR[src/monitoring/meters.py<br/>Prometheusç›‘æ§]
    PSEUDO -.ç»Ÿè®¡.-> MONITOR
    MONITOR --> METRICS[/metricsç«¯ç‚¹<br/>Grafanaä»ªè¡¨æ¿]
    
    %% è¾“å‡º
    TRAIN --> CHECKPOINT[artifacts/checkpoints/<br/>æ¨¡å‹æƒé‡]
    TEST --> REPORT
    MONITOR --> LOGS[logs/<br/>ç»“æ„åŒ–æ—¥å¿—]
    
    %% å›¾ä¾‹
    classDef config fill:#e1f5ff,stroke:#0066cc
    classDef data fill:#fff4e1,stroke:#cc8800
    classDef model fill:#ffe1e1,stroke:#cc0000
    classDef monitor fill:#e1ffe1,stroke:#00cc00
    
    class CONFIG,TRAIN,TEST config
    class DATA,GRAPH data
    class MODEL,GAT,GLOBAL,RULE_FOCUS,MEMORY,SCORE,RULE_ENGINE,LOSS,PSEUDO model
    class EXPLAIN,MONITOR,METRICS,LOGS,REPORT,CHECKPOINT monitor
```

**å›¾ä¾‹è¯´æ˜**ï¼š
- å®çº¿ç®­å¤´ï¼ˆâ†’ï¼‰ï¼šæ•°æ®æµ
- è™šçº¿ç®­å¤´ï¼ˆ-.->ï¼‰ï¼šæ§åˆ¶æµ/å…ƒæ•°æ®æµ
- è“è‰²ï¼šé…ç½®ä¸ç¼–æ’å±‚
- é»„è‰²ï¼šæ•°æ®å¤„ç†å±‚
- çº¢è‰²ï¼šæ¨¡å‹ä¸ç®—æ³•å±‚
- ç»¿è‰²ï¼šç›‘æ§ä¸è¾“å‡ºå±‚

### 1.2 ä¸šåŠ¡ä¸æ•°æ®æµç¨‹
1. `DataIngestor` è¯»å–é…ç½®ï¼ŒåŠ è½½åˆæˆ/BDD100K/Cityscapes æ ·æœ¬ï¼Œæ ‡å‡†åŒ–è½¦è¾†ã€äº¤é€šç¯ã€åœæ­¢çº¿å®ä½“ã€‚
2. `GraphBuilder` å°†å®ä½“è½¬æ¢ä¸ºç‰¹å¾å¼ é‡ä¸é‚»æ¥çŸ©é˜µï¼Œæ³¨å…¥æ—¶ç©ºä½ç½®åŠåœæ­¢çº¿è·ç¦»ï¼Œè¾“å‡º `GraphBatch`.
3. `GATAttention` ç¼–ç å›¾ä¿¡æ¯ï¼Œå¹¶é€šè¿‡è®°å¿†æ¨¡å—æ‰§è¡Œæ³¨æ„åŠ›æ£€ç´¢ï¼Œç”ŸæˆèŠ‚ç‚¹è¡¨å¾å’Œæ³¨æ„åŠ›æƒé‡ã€‚
4. `RuleEngine` æ ¹æ® DSL è§„åˆ™è®¡ç®—çº¢ç¯åœè¿è§„åˆ†æ•°ï¼Œä¸æ¨¡å‹è¾“å‡ºå…±åŒè¾“å…¥çº¦æŸæŸå¤±ã€‚
5. `AnomalyScorer` æ±‡æ€»æ¨¡å‹åˆ†æ•°ã€è§„åˆ™ç»“æœï¼Œå½¢æˆè¿è§„è¯æ®é“¾ï¼ˆé€Ÿåº¦ã€ä½ç½®ã€æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼‰ã€‚
6. `Monitoring` è®°å½• lossã€è¿è§„æ•°ã€æ³¨æ„åŠ›ä¸€è‡´æ€§æŒ‡æ ‡ï¼Œå¹¶å°†å¯è§†åŒ–/æ—¥å¿—å†™å…¥ `reports/`.
7. CLI å·¥å…· orchestrate è®­ç»ƒ/æµ‹è¯•ï¼Œè¾“å‡ºæŒ‡æ ‡ã€checkpointã€å¯è§£é‡ŠæŠ¥å‘Šã€‚

### 1.3 æ³¨æ„åŠ›ä¸è¯­ä¹‰æ³¨å…¥ç­–ç•¥
- å¤šå¤´ GAT è´Ÿè´£å±€éƒ¨å…³ç³»å»ºæ¨¡ï¼Œ`memory_bank` + `AttentionRetriever` å­¦ä¹ æ­£å¸¸é©¾é©¶åŸå‹ï¼›æ³¨æ„åŠ›æƒé‡åœ¨æ¨ç†é˜¶æ®µå¯¼å‡ºç”¨äºå¯è§†åŒ–ã€‚
- è§„åˆ™æ³¨å…¥é€šè¿‡ DSLï¼ˆ`pydantic` æ ¡éªŒï¼‰å®šä¹‰ç¯è‰²ã€é€Ÿåº¦é˜ˆå€¼ã€åœæ­¢çº¿è·ç¦»ï¼Œè®­ç»ƒæ—¶åŠ å…¥ `constraint_loss`ï¼Œæ¨ç†æ—¶ç”Ÿæˆ rule_scoreã€‚
- è®¾å®š attention-consistency lossï¼Œç¡®ä¿é«˜æ³¨æ„åŠ›èŠ‚ç‚¹ä¸è¿è§„è¯æ®ä¸€è‡´ï¼›æƒé‡å’Œ rule_score éœ€å†™å…¥æ—¥å¿—ä¾› QA/ä¸šåŠ¡å¤æ ¸ã€‚

## 2. æ¦‚è¦è®¾è®¡

### 2.1 ä»£ç åŒ…ä¸èŒè´£ï¼ˆå®ç°çŠ¶æ€æ ‡è®°ï¼‰
| åŒ…è·¯å¾„ | ä¸»è¦èŒè´£ | è¾“å…¥ / è¾“å‡º | ä¾èµ– | å®ç°çŠ¶æ€ | å¤‡æ³¨ |
| --- | --- | --- | --- | --- | --- |
| `src/traffic_rules/config/` | Pythoné…ç½®ç±»ä¸åŠ è½½å™¨ | ç¯å¢ƒå˜é‡ â†’ `ProjectConfig` | `pydantic` | âœ… å·²å®Œæˆ | æ‰€æœ‰æ¨¡å—é€šè¿‡ä¾èµ–æ³¨å…¥è·å–é…ç½® |
| `src/traffic_rules/data/traffic_dataset.py` | æ•°æ®åŠ è½½ã€å¢å¼ºã€åœæ­¢çº¿è·ç¦»è®¡ç®— | æ–‡ä»¶è·¯å¾„ â†’ `SceneContext` | `torch`, `numpy`, `pillow` | ğŸŸ¡ éƒ¨åˆ†å®Œæˆ | syntheticå®Œæˆï¼ŒBDD100K/Cityscapeså¾…å®ç° |
| `src/traffic_rules/graph/builder.py` | ç”Ÿæˆç‰¹å¾çŸ©é˜µä¸é‚»æ¥çŸ©é˜µ | `SceneContext` â†’ `GraphBatch` | `torch`, `networkx` | âœ… å·²å®Œæˆ | 10ç»´ç‰¹å¾ç¼–ç ã€å¼‚æ„å›¾è¾¹æ„å»º |
| `src/traffic_rules/models/multi_stage_gat.py` | ä¸‰é˜¶æ®µGATä¸»æ¨¡å‹ + scoring head | `GraphBatch` â†’ è¾“å‡ºå­—å…¸ | `torch` | âœ… å·²å®Œæˆ | åŒ…å«LocalGAT/Global/RuleFocusä¸‰é˜¶æ®µ |
| `src/traffic_rules/models/local_gat.py` | å±€éƒ¨GATç¼–ç å™¨ï¼ˆ3å±‚Ã—8å¤´ï¼‰ | `(x, edge_index)` â†’ `h_local, Î±_gat` | `torch` | âœ… å·²å®Œæˆ | åŸºäºPyGçš„GATå®ç° |
| `src/traffic_rules/models/global_attention.py` | å…¨å±€è™šæ‹ŸèŠ‚ç‚¹æ³¨æ„åŠ› | `h_local` â†’ `h_global, attn` | `torch` | âœ… å·²å®Œæˆ | 4å¤´æ³¨æ„åŠ› |
| `src/traffic_rules/models/rule_attention.py` | è§„åˆ™èšç„¦æ³¨æ„åŠ› | `(h_global, entity_types)` â†’ `h_rule, Î²` | `torch` | âœ… å·²å®Œæˆ | è§„åˆ™embeddingå¼•å¯¼ |
| `src/traffic_rules/memory/memory_bank.py` | Memory Bankï¼šæ£€ç´¢ã€æŒä¹…åŒ– | `embeddings` â†’ `memory_context` | `torch` | ğŸŸ¡ éƒ¨åˆ†å®Œæˆ | åŸºç¡€å®Œæˆï¼Œç¼ºK-Meansåˆå§‹åŒ–/EMAæ›´æ–° |
| `src/traffic_rules/rules/red_light.py` | çº¢ç¯åœè§„åˆ™è¯„åˆ†å¼•æ“ | `(ç¯æ€, è·ç¦», é€Ÿåº¦)` â†’ `rule_score` | `torch` | âœ… å·²å®Œæˆ | Gumbel-Softmaxè½¯åŒ–ï¼Œæ¢¯åº¦å¯å¯¼ |
| `src/traffic_rules/loss/constraint.py` | çº¦æŸæŸå¤±ï¼ˆå››é¡¹ï¼šrecon/rule/attn/regï¼‰ | `(model_scores, rule_scores, ...)` â†’ `(loss, dict)` | `torch` | âœ… å·²å®Œæˆ | åŒå±‚æ³¨æ„åŠ›ç›‘ç£æŸå¤± |
| `src/traffic_rules/explain/attention_viz.py` | æ³¨æ„åŠ›çƒ­åŠ›å›¾ç»˜åˆ¶ | `(image, entities, attn)` â†’ `annotated_image` | `cv2`, `matplotlib` | ğŸŸ¡ éƒ¨åˆ†å®Œæˆ | åŸºç¡€çƒ­åŠ›å›¾å®Œæˆï¼Œæ‰¹é‡æ¸²æŸ“å¾…è¡¥ |
| `src/traffic_rules/self_training/pseudo_labeler.py` | ä¼ªæ ‡ç­¾ç”Ÿæˆå™¨ï¼ˆä¸‰ç­–ç•¥ï¼‰ | `(model_scores, rule_scores)` â†’ `List[PseudoLabel]` | `torch`, `pandas` | ğŸŸ¡ éƒ¨åˆ†å®Œæˆ | ç­–ç•¥å®Œæˆï¼Œæœªé›†æˆåˆ°è®­ç»ƒå¾ªç¯ |
| `src/traffic_rules/monitoring/metrics.py` | è¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼ˆAUC/F1ç­‰ï¼‰ | `(model_scores, rule_scores)` â†’ `Dict[metrics]` | `sklearn` | âœ… å·²å®Œæˆ | å®Œæ•´çš„åˆ†ç±»æŒ‡æ ‡ |
| `src/traffic_rules/monitoring/visualizer.py` | è®­ç»ƒæ›²çº¿ç»˜åˆ¶ | `history` â†’ PNGå›¾ç‰‡ | `matplotlib` | âœ… å·²å®Œæˆ | 4å­å›¾å¯è§†åŒ– |
| `src/traffic_rules/monitoring/gradient_monitor.py` | æ¢¯åº¦ç›‘æ§ï¼ˆçˆ†ç‚¸/æ¶ˆå¤±æ£€æµ‹ï¼‰ | `model` â†’ `grad_stats` | `torch` | âœ… å·²å®Œæˆ | å®æ—¶æ¢¯åº¦å¥åº·æ£€æŸ¥ |
| `tools/train_red_light.py` | è®­ç»ƒCLIç¼–æ’å™¨ | å‘½ä»¤è¡Œå‚æ•° â†’ checkpoint/æ›²çº¿ | `typer`, `tqdm` | âœ… å·²å®Œæˆ | å®Œæ•´è®­ç»ƒå¾ªç¯ï¼Œç¼ºè‡ªè®­ç»ƒé›†æˆ |
| `tools/test_red_light.py` | æµ‹è¯•CLI | `(checkpoint, data)` â†’ JSONè¯æ®é“¾ | `typer` | âœ… å·²å®Œæˆ | è¾“å‡ºJSONæŠ¥å‘Šï¼Œç¼ºä¸‰åœºæ™¯åˆ†ç±» |

### 2.2 åŒ…ä¹‹é—´çš„è”ç³»

> **é‡å¤§ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šè¡¥å……å®Œæ•´çš„æ¨¡å—ä¾èµ–å…³ç³»ï¼Œç¡®ä¿ä¸æ¶æ„å›¾ä¸€è‡´ã€‚

**ä¾èµ–å±‚æ¬¡**ï¼ˆä»åº•å±‚åˆ°é¡¶å±‚ï¼‰ï¼š

**Layer 0ï¼šåŸºç¡€è®¾æ–½**
- `src/config`ï¼šé…ç½®ç®¡ç†ï¼ˆæ— ä¾èµ–ï¼‰

**Layer 1ï¼šæ•°æ®å¤„ç†**
- `src/data/traffic.py`ï¼šä¾èµ–`config`
- `src/rules/red_light.py`ï¼šä¾èµ–`config`ï¼ˆè§„åˆ™é˜ˆå€¼é…ç½®ï¼‰

**Layer 2ï¼šå›¾å¤„ç†**
- `src/graph/builder.py`ï¼šä¾èµ–`data`, `config`

**Layer 3ï¼šæ¨¡å‹æ ¸å¿ƒ**
- `src/models/gat_attention.py`ï¼šä¾èµ–`graph`, `config`
- `src/memory/bank.py`ï¼šä¾èµ–`models`ï¼ˆå¯é€‰ï¼Œä¸æ¨¡å‹è§£è€¦ï¼‰

**Layer 4ï¼šæŸå¤±ä¸è§„åˆ™**
- `src/loss/constraint.py`ï¼šä¾èµ–`models`, `rules`

**Layer 5ï¼šé«˜å±‚æœåŠ¡**
- `src/explain/attention_viz.py`ï¼šä¾èµ–`models`
- `src/self_training/pseudo_labeler.py`ï¼šä¾èµ–`models`, `rules`, `loss`
- `src/monitoring/meters.py`ï¼šä¾èµ–`loss`, `self_training`ï¼ˆé€šè¿‡äº‹ä»¶è®¢é˜…ï¼‰

**Layer 6ï¼šç¼–æ’å±‚**
- `src/tools/train_red_light.py`ï¼šä¾èµ–æ‰€æœ‰ä¸‹å±‚æ¨¡å—
- `src/tools/test_red_light.py`ï¼šä¾èµ–`models`, `rules`, `explain`, `monitoring`

**æ¨¡å—ä¾èµ–DAG**ï¼š

```
config
  â”œâ”€â”€ data â”€â”€â”
  â”œâ”€â”€ rules â”€â”¼â”€â”€â”
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚
            â”‚â”‚  â”‚
         graph  â”‚
            â”‚   â”‚
         models â”€â”¤
            â”‚   â”‚â”‚
         memory â”‚â”œâ”€â”€ loss â”€â”€â”
         (å¯é€‰) â”‚â”‚  â”‚       â”‚
            â””â”€â”€â”€â”¼â”´â”€â”€â”¤       â”‚
                â”‚   â”‚       â”‚
            explain â”‚   self_training
                â”‚   â”‚       â”‚
            monitoring â”€â”€â”€â”€â”€â”¤
                â”‚           â”‚
            train_tool â”€â”€â”€â”€â”€â”¤
            test_tool â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®è®¾è®¡åŸåˆ™**ï¼š
1. âœ… **å•å‘ä¾èµ–**ï¼šæ— å¾ªç¯ä¾èµ–ï¼ˆDAGç»“æ„ï¼‰
2. âœ… **å±‚æ¬¡æ¸…æ™°**ï¼šä¸Šå±‚ä¾èµ–ä¸‹å±‚ï¼Œä¸å…è®¸è·¨å±‚ä¾èµ–
3. âœ… **æ¾è€¦åˆ**ï¼šé€šè¿‡æ¥å£å’Œé…ç½®æ³¨å…¥ï¼Œæ¨¡å—å¯æ›¿æ¢
4. âœ… **å¯é€‰æ¨¡å—**ï¼šmemoryå’Œself_trainingå¯é€šè¿‡é…ç½®ç¦ç”¨

### 2.3 å…³é”®æ¥å£ï¼ˆå®é™…ä»£ç ç­¾åï¼‰

> ä»¥ä¸‹æ¥å£ç­¾åä»å®é™…ä»£ç ä¸­æå–ï¼ˆ2025-12-16ï¼‰ï¼Œç¡®ä¿æ–‡æ¡£ä¸ä»£ç ä¸€è‡´

#### æ•°æ®å±‚
```python
class TrafficLightDataset(Dataset):
    def __init__(
        self,
        data_root: str = "data/synthetic",
        mode: str = "synthetic",  # 'synthetic' | 'bdd100k' | 'cityscapes'
        split: str = "train",     # 'train' | 'val' | 'test'
        max_samples: Optional[int] = None,
        augment: bool = False,
    )
    
    def __getitem__(self, idx: int) -> SceneContext
    def __len__(self) -> int
```

#### å›¾æ„å»ºå±‚
```python
class GraphBuilder:
    def __init__(
        self,
        feature_dim: int = 10,
        r_car_car: float = 30.0,
        r_car_light: float = 50.0,
        r_car_stop: float = 100.0,
    )
    
    def build(self, scene: SceneContext) -> GraphBatch
    # GraphBatchåŒ…å«ï¼šx: [N, 10], edge_index: [2, E], entity_types: [N]
```

#### æ¨¡å‹å±‚
```python
class MultiStageAttentionGAT(nn.Module):
    def __init__(
        self,
        input_dim: int = 10,
        hidden_dim: int = 128,
        num_gat_layers: int = 3,
        num_heads: int = 8,
        num_global_heads: int = 4,
        dropout: float = 0.1,
    )
    
    def forward(
        self,
        x: torch.Tensor,              # [N, 10] èŠ‚ç‚¹ç‰¹å¾
        edge_index: torch.Tensor,     # [2, E] è¾¹ç´¢å¼•
        entity_types: torch.Tensor,   # [N] å®ä½“ç±»å‹
        entity_masks: Optional[torch.Tensor] = None,
        return_attention: bool = False,
    ) -> Dict[str, torch.Tensor]
    # è¿”å›: {'scores': [N_car], 'gat_attention': [E], 'rule_attention': [N_car]}
```

#### è§„åˆ™å¼•æ“
```python
class RedLightRuleEngine:
    def __init__(self, config: Optional[RuleConfig] = None)
    
    def evaluate(
        self,
        light_probs: torch.Tensor,    # [B, 3] ç¯æ€æ¦‚ç‡
        distances: torch.Tensor,      # [B] åœæ­¢çº¿è·ç¦»
        velocities: torch.Tensor,     # [B] é€Ÿåº¦
        training: bool = True,
    ) -> torch.Tensor  # [B] è§„åˆ™åˆ†æ•°
```

#### æŸå¤±å‡½æ•°
```python
class StagedConstraintLoss(nn.Module):
    def __init__(self, config: Optional[LossConfig] = None)
    
    def forward(
        self,
        model_scores: torch.Tensor,    # [N_car] æ¨¡å‹å¼‚å¸¸åˆ†æ•°
        rule_scores: torch.Tensor,     # [N_car] è§„åˆ™åˆ†æ•°
        alpha_gat: torch.Tensor,       # [E] GATæ³¨æ„åŠ›
        beta_rule: torch.Tensor,       # [N_car] è§„åˆ™æ³¨æ„åŠ›
        edge_index: torch.Tensor,      # [2, E]
        entity_types: torch.Tensor,    # [N]
        model_parameters: List[nn.Parameter],
    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]
    # è¿”å›: (loss_total, {'recon': ..., 'rule': ..., 'attn': ..., 'reg': ...})
```

#### è‡ªè®­ç»ƒ
```python
class PseudoLabeler:
    def generate_rule_priority(
        self,
        model_scores: torch.Tensor,
        rule_scores: torch.Tensor,
        attention_max: torch.Tensor,
        scene_ids: List[str],
        entity_ids: List[str],
    ) -> List[PseudoLabel]
    
    def save_to_disk(self, output_dir: Path) -> Path
```

#### å¯è§£é‡Šæ€§
```python
def visualize_attention(
    image: np.ndarray,              # [H, W, 3]
    entities: List[Entity],
    attention_weights: torch.Tensor,  # [N]
    focal_entity_idx: int,
    save_path: Optional[str] = None,
) -> np.ndarray  # æ ‡æ³¨åçš„å›¾åƒ
```

## 3. è¯¦ç»†è®¾è®¡

> **ç®—æ³•æ–¹æ¡ˆé€‰æ‹©**ï¼šç»è¯„å®¡ï¼Œé‡‡ç”¨"å¤šé˜¶æ®µæ³¨æ„åŠ›å¢å¼ºGAT + ç¡¬çº¦æŸè§„åˆ™èåˆ"æ–¹æ¡ˆï¼ˆè¯¦è§ `docs/archive/design/ALGORITHM_DESIGN_OPTIONS.md` æ–¹æ¡ˆ1ï¼‰ã€‚æœ¬èŠ‚ä¸ºæ ¸å¿ƒç®—æ³•çš„å·¥ç¨‹å®ç°ç»†èŠ‚ã€‚

### 3.1 æ•°æ®æ‘„å–å±‚

#### 3.1.1 æ•°æ®æºä¸æ ¼å¼
- **æ•°æ®æº**ï¼š`/data/traffic/{synthetic,bdd100k,cityscapes}`
  - åˆæˆæ•°æ®ï¼šç”± `scripts/prepare_synthetic_data.py` ç”Ÿæˆâ‰¥100ä¸ªåœºæ™¯ï¼ŒåŒ…å«çº¢ç¯åœ/é—¯/ç»¿ç¯é€šè¿‡
  - çœŸå®æ•°æ®ï¼šBDD100K/Cityscapes å­é›†ï¼ˆ10-20ä¸ªæ ·æœ¬ç”¨äºéªŒè¯ï¼‰
- **åœºæ™¯è¡¨ç¤º**ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«
  ```python
  {
    'image': np.ndarray,  # [H, W, 3]
    'entities': List[Entity],  # è½¦è¾†ã€äº¤é€šç¯ã€åœæ­¢çº¿
    'timestamp': float,
    'scene_id': str
  }
  ```

#### 3.1.2 å®ä½“ç‰¹å¾æå–
èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ $d_{\text{feat}} = 10$ï¼š

| ç‰¹å¾ç±»å‹ | è½¦è¾†èŠ‚ç‚¹ | äº¤é€šç¯èŠ‚ç‚¹ | åœæ­¢çº¿èŠ‚ç‚¹ |
|---------|---------|----------|----------|
| ä½ç½® (x, y) | âœ“ ä¸­å¿ƒåæ ‡ | âœ“ ä¸­å¿ƒåæ ‡ | âœ“ ä¸­ç‚¹åæ ‡ |
| é€Ÿåº¦ (vx, vy) | âœ“ | âœ— (å¡«0) | âœ— (å¡«0) |
| å°ºå¯¸ (w, h) | âœ“ bbox | âœ“ bbox | âœ“ çº¿æ®µé•¿åº¦ |
| åœæ­¢çº¿è·ç¦» d_stop | âœ“ æ¬§æ°è·ç¦» | âœ— (å¡«999) | âœ— (å¡«0) |
| ç±»å‹ one-hot [3] | [1,0,0] | [0,1,0] | [0,0,1] |

**åœæ­¢çº¿è·ç¦»è®¡ç®—**ï¼ˆå‘é‡æŠ•å½±ï¼‰ï¼š
$$
d_{\text{stop}}(i) = \frac{|(\mathbf{p}_i - \mathbf{s}_1) \times (\mathbf{s}_2 - \mathbf{s}_1)|}{|\mathbf{s}_2 - \mathbf{s}_1|}
$$
å…¶ä¸­ $\mathbf{s}_1, \mathbf{s}_2$ ä¸ºåœæ­¢çº¿ç«¯ç‚¹ã€‚

#### 3.1.3 æ•°æ®å¢å¼º
- ç©ºé—´å¢å¼ºï¼šéšæœºè£å‰ª (0.8~1.0)ã€æ°´å¹³ç¿»è½¬ (p=0.5)
- å…‰ç…§å¢å¼ºï¼šäº®åº¦ (Â±0.2)ã€å¯¹æ¯”åº¦ (Â±0.2)
- å®ä½“æ‰°åŠ¨ï¼šè½¦è¾†/äº¤é€šç¯æ•°é‡ Â±1ï¼ˆåˆæˆæ•°æ®ï¼‰
- æ‰€æœ‰å¢å¼ºå‚æ•°è®°å½•åˆ° `entities.augmentation_log`

### 3.2 åœºæ™¯å›¾æ„å»º

#### 3.2.1 å›¾å®šä¹‰
å¼‚æ„æ—¶ç©ºå›¾ $\mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathbf{X}, \mathbf{A})$ï¼š
- èŠ‚ç‚¹é›† $\mathcal{V} = V_{\text{car}} \cup V_{\text{light}} \cup V_{\text{stop}}$ï¼Œ$|\mathcal{V}| \approx 5\text{~}15$
- ç‰¹å¾çŸ©é˜µ $\mathbf{X} \in \mathbb{R}^{|\mathcal{V}| \times 10}$
- é‚»æ¥çŸ©é˜µ $\mathbf{A} \in \{0,1\}^{|\mathcal{V}| \times |\mathcal{V}|}$

#### 3.2.2 è¾¹æ„å»ºç­–ç•¥
```python
# ç©ºé—´é‚»è¿‘è¾¹ï¼ˆå¼‚æ„è¿æ¥ï¼‰
for i, j in combinations(nodes, 2):
    dist = ||pos[i] - pos[j]||
    if dist < r_spatial (50m) and type[i] != type[j]:
        A[i, j] = A[j, i] = 1

# è¯­ä¹‰è¾¹ï¼ˆè½¦è¾†â†’æœ€è¿‘äº¤é€šç¯ï¼‰
for car in cars:
    nearest_light = argmin(||car.pos - light.pos|| for light in lights)
    A[car, nearest_light] = 1

# åœæ­¢çº¿è¾¹ï¼ˆè½¦è¾†â†’åœæ­¢çº¿ï¼Œå¦‚æœè·ç¦»<100mï¼‰
for car in cars:
    if car.d_stop < 100:
        A[car, stopline] = 1
```

### 3.3 å¤šé˜¶æ®µæ³¨æ„åŠ›æ¶æ„ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰

> **æŠ€æœ¯å‹˜è¯¯ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šæœ¬èŠ‚å¢åŠ ä¸‰é˜¶æ®µæ³¨æ„åŠ›çš„è¯¦ç»†å®ç°ç»†èŠ‚ï¼Œæ˜ç¡®å±€éƒ¨â†’å…¨å±€â†’è§„åˆ™èšç„¦çš„å…·ä½“æœºåˆ¶ã€‚  
> è¯¦è§ï¼š`docs/archive/design/TECHNICAL_CORRECTIONS.md` é—®é¢˜5

#### 3.3.1 é˜¶æ®µ1ï¼šå±€éƒ¨å…³ç³»ç¼–ç ï¼ˆMulti-Head GATï¼‰

**å®šä¹‰**ï¼šåŸºäºç©ºé—´é‚»è¿‘æ€§å’Œå®ä½“ç±»å‹çš„**ç¨€ç–å›¾æ³¨æ„åŠ›**ã€‚

**é‚»æ¥çŸ©é˜µæ„å»ºï¼ˆç¨€ç–è¿æ¥ï¼‰**ï¼š
```python
# å±€éƒ¨é‚»æ¥ï¼šä»…è¿æ¥ç©ºé—´é‚»è¿‘ä¸”å¼‚æ„çš„å®ä½“
def build_local_adjacency(entities, r_spatial=50.0):
    """
    è¾¹ç±»å‹ï¼š
    1. è½¦è¾†-è½¦è¾†ï¼ˆè·ç¦»<30mï¼‰
    2. è½¦è¾†-äº¤é€šç¯ï¼ˆè·ç¦»<50mï¼‰
    3. è½¦è¾†-åœæ­¢çº¿ï¼ˆè·ç¦»<100mï¼‰
    """
    edges = []
    for i, e_i in enumerate(entities):
        for j, e_j in enumerate(entities):
            if i >= j:
                continue
            
            dist = ||e_i.pos - e_j.pos||
            
            # å¼‚æ„è¿æ¥
            if e_i.type != e_j.type:
                if e_i.type == 'car' and e_j.type == 'light' and dist < 50:
                    edges.append((i, j))
                elif e_i.type == 'car' and e_j.type == 'stop' and dist < 100:
                    edges.append((i, j))
            # åŒæ„è¿æ¥ï¼ˆä»…è½¦è¾†ï¼‰
            elif e_i.type == 'car' and e_j.type == 'car' and dist < 30:
                edges.append((i, j))
    
    return torch.tensor(edges).T  # [2, E]
```

**è¾“å…¥æŠ•å½±**ï¼š
$$
\mathbf{h}_i^{(0)} = \text{LayerNorm}(\mathbf{W}_0 \mathbf{x}_i + \mathbf{b}_0), \quad \mathbf{h}_i^{(0)} \in \mathbb{R}^{128}
$$

**å¤šå¤´æ³¨æ„åŠ›**ï¼ˆ$K=8$ å¤´ï¼Œ$L=3$ å±‚ï¼‰ï¼š
$$
\begin{aligned}
\alpha_{ij}^{(l,k)} &= \frac{\exp\left(\text{LeakyReLU}\left(\mathbf{a}_k^\top [\mathbf{W}_k^{(l)} \mathbf{h}_i^{(l-1)} \| \mathbf{W}_k^{(l)} \mathbf{h}_j^{(l-1)}]\right)\right)}{\sum_{j' \in \mathcal{N}(i)} \exp\left(\text{LeakyReLU}\left(\mathbf{a}_k^\top [\mathbf{W}_k^{(l)} \mathbf{h}_i^{(l-1)} \| \mathbf{W}_k^{(l)} \mathbf{h}_{j'}^{(l-1)}]\right)\right)} \\
\mathbf{h}_i^{(l,k)} &= \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^{(l,k)} \mathbf{W}_k^{(l)} \mathbf{h}_j^{(l-1)} \\
\mathbf{h}_i^{(l)} &= \text{GELU}\left(\frac{1}{K} \sum_{k=1}^K \mathbf{h}_i^{(l,k)}\right) + \mathbf{h}_i^{(l-1)} \quad \text{(å¤šå¤´å¹³å‡ + æ®‹å·®)}
\end{aligned}
$$

**ç‰¹ç‚¹**ï¼š
- âœ… ç¨€ç–è¿æ¥ï¼ˆè¾¹æ•° $E \ll N^2$ï¼‰
- âœ… ç©ºé—´å±€éƒ¨æ€§ï¼ˆä¸åŒç±»å‹å®ä½“æœ‰ä¸åŒè¿æ¥åŠå¾„ï¼‰
- âœ… å¤šè·³ä¼ æ’­ï¼ˆ3å±‚GAT â†’ 3è·³æ„Ÿå—é‡ï¼‰

è¶…å‚æ•°ï¼š$d_h = 128$ï¼ŒLeakyReLU æ–œç‡ $\alpha = 0.2$ï¼Œdropout $p = 0.1$ã€‚

#### 3.3.2 é˜¶æ®µ2ï¼šå…¨å±€ä¸Šä¸‹æ–‡èåˆ

**å®šä¹‰**ï¼šé€šè¿‡**è™šæ‹Ÿå…¨å±€èŠ‚ç‚¹**èšåˆåœºæ™¯çº§ä¸Šä¸‹æ–‡ï¼ˆç±»ä¼¼Transformerçš„[CLS] tokenï¼‰ã€‚

**å®ç°æœºåˆ¶**ï¼š
```python
class GlobalSceneAttention(nn.Module):
    def __init__(self, hidden_dim=128):
        super().__init__()
        # å…¨å±€èŠ‚ç‚¹åˆå§‹åŒ–ï¼ˆå¯å­¦ä¹ ï¼‰
        self.global_query = nn.Parameter(torch.randn(1, hidden_dim))
        
        # Transformerå¼å¤šå¤´è‡ªæ³¨æ„åŠ›
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=4, dropout=0.1
        )
        
        # èåˆMLP
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.GELU(),
            nn.LayerNorm(hidden_dim)
        )
    
    def forward(self, h_local):
        # h_local: [N, hidden_dim] - å±€éƒ¨GATè¾“å‡º
        
        # Step 1: å…¨å±€èŠ‚ç‚¹èšåˆæ‰€æœ‰å±€éƒ¨èŠ‚ç‚¹ä¿¡æ¯
        global_context, attn_weights = self.multihead_attn(
            query=self.global_query.unsqueeze(0),  # [1, 1, D]
            key=h_local.unsqueeze(0),              # [1, N, D]
            value=h_local.unsqueeze(0)             # [1, N, D]
        )
        
        # Step 2: å¹¿æ’­å…¨å±€ä¿¡æ¯åˆ°æ¯ä¸ªå±€éƒ¨èŠ‚ç‚¹
        global_context = global_context.squeeze(0).expand(N, -1)
        
        # Step 3: èåˆå±€éƒ¨+å…¨å±€
        h_fused = torch.cat([h_local, global_context], dim=-1)
        h_global = self.fusion(h_fused) + h_local  # æ®‹å·®è¿æ¥
        
        return h_global, attn_weights.squeeze()
```

**æ•°å­¦å½¢å¼**ï¼š
$$
\begin{aligned}
\mathbf{Q}_g &= \mathbf{W}_q \mathbf{g}, \quad \mathbf{K}_h = \mathbf{W}_k [\mathbf{h}_1^{(L)}, \dots, \mathbf{h}_N^{(L)}], \quad \mathbf{V}_h = \mathbf{W}_v [\mathbf{h}_1^{(L)}, \dots, \mathbf{h}_N^{(L)}] \\
\mathbf{g} &= \text{softmax}\left(\frac{\mathbf{Q}_g \mathbf{K}_h^\top}{\sqrt{d_h}}\right) \mathbf{V}_h \\
\tilde{\mathbf{h}}_i &= \mathbf{h}_i^{(L)} + \text{MLP}_{\text{fuse}}([\mathbf{h}_i^{(L)} \| \mathbf{g}])
\end{aligned}
$$

**ç‰¹ç‚¹**ï¼š
- âœ… å…¨è¿æ¥ï¼ˆå…¨å±€èŠ‚ç‚¹ä¸æ‰€æœ‰å±€éƒ¨èŠ‚ç‚¹äº¤äº’ï¼‰
- âœ… åœºæ™¯çº§ä¿¡æ¯ï¼ˆäº¤é€šå¯†åº¦ã€æ•´ä½“æµåŠ¨æ€§ç­‰ï¼‰
- âœ… å¯è§£é‡Šæ€§ï¼ˆattn_weightsæ˜¾ç¤ºå“ªäº›å®ä½“å¯¹åœºæ™¯é‡è¦ï¼‰

**ä¸Transformerå¯¹æ¯”**ï¼š

| ç»´åº¦ | Transformer | æœ¬æ–¹æ¡ˆå…¨å±€æ³¨æ„åŠ› |
|------|-------------|-----------------|
| è¿æ¥æ–¹å¼ | å…¨è¿æ¥ï¼ˆNÃ—Nï¼‰ | æ˜Ÿå‹ï¼ˆ1Ã—Nï¼‰ |
| è®¡ç®—å¤æ‚åº¦ | O(NÂ²) | O(N) |
| è¯­ä¹‰ | Tokené—´äº¤äº’ | åœºæ™¯çº§æ±‡æ€» |

å…¶ä¸­ $\text{MLP}_{\text{fuse}}$ ä¸º 2å±‚å…¨è¿æ¥ç½‘ç»œï¼š$\mathbb{R}^{256} \rightarrow \mathbb{R}^{128}$ã€‚

#### 3.3.3 é˜¶æ®µ3ï¼šè§„åˆ™èšç„¦æ³¨æ„åŠ›

**å®šä¹‰**ï¼šåŸºäº**è§„åˆ™è¯­ä¹‰**çš„åŠ æƒæ³¨æ„åŠ›é‡åˆ†é…ï¼Œå°†æ³¨æ„åŠ›å¼•å¯¼åˆ°ä¸è§„åˆ™ç›¸å…³çš„å®ä½“ï¼ˆäº¤é€šç¯ã€åœæ­¢çº¿ï¼‰ã€‚

**å®ç°æœºåˆ¶**ï¼š
```python
class RuleFocusedAttention(nn.Module):
    def __init__(self, hidden_dim=128):
        super().__init__()
        # è§„åˆ™ç›¸å…³æ€§è¯„åˆ†ç½‘ç»œ
        self.rule_scorer = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),  # [h_car || h_light || h_stop]
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # è§„åˆ™åµŒå…¥ï¼ˆå¯å­¦ä¹ ï¼ŒåŒºåˆ†ä¸åŒè§„åˆ™ç±»å‹ï¼‰
        self.rule_embeddings = nn.Embedding(
            num_embeddings=5,  # çº¢ç¯åœã€è½¦é€Ÿã€è½¦é“ã€å®‰å…¨è·ç¦»ç­‰
            embedding_dim=hidden_dim
        )
    
    def forward(self, h_fused, entity_types, entity_masks, rule_id=0):
        # æå–è§„åˆ™ç›¸å…³å®ä½“
        car_mask = (entity_types == 0) & entity_masks
        light_mask = (entity_types == 1) & entity_masks
        stop_mask = (entity_types == 2) & entity_masks
        
        h_cars = h_fused[car_mask]
        h_lights = h_fused[light_mask]
        h_stops = h_fused[stop_mask]
        
        # è·å–è§„åˆ™åµŒå…¥
        rule_emb = self.rule_embeddings(torch.tensor([rule_id]))
        
        # è®¡ç®—æ¯ä¸ªè½¦è¾†ä¸è§„åˆ™ç›¸å…³å®ä½“çš„æ³¨æ„åŠ›
        rule_attention = []
        for h_car in h_cars:
            h_light_nearest = h_lights.mean(dim=0) if len(h_lights) > 0 else torch.zeros_like(h_car)
            h_stop_nearest = h_stops.mean(dim=0) if len(h_stops) > 0 else torch.zeros_like(h_car)
            
            # æ‹¼æ¥ç‰¹å¾
            concat_feat = torch.cat([h_car, h_light_nearest, h_stop_nearest], dim=0)
            
            # è®¡ç®—è§„åˆ™ç›¸å…³æ€§åˆ†æ•°
            rule_score = self.rule_scorer(concat_feat)
            rule_attention.append(rule_score)
            
            # åŠ æƒèåˆï¼ˆè§„åˆ™åµŒå…¥ä½œä¸ºè½¯çº¦æŸï¼‰
            h_weighted = h_car * rule_score + rule_emb.squeeze(0) * (1 - rule_score)
        
        return h_rule_focused, rule_attention
```

**æ•°å­¦å½¢å¼**ï¼š

å¯¹äºæ¯ä¸ªè½¦è¾†èŠ‚ç‚¹$i$ï¼Œè®¡ç®—å…¶è§„åˆ™èšç„¦æ³¨æ„åŠ›åˆ†æ•°ï¼š
$$
\begin{aligned}
\mathbf{h}_{\text{light}}^{(i)} &= \text{avg}(\{\tilde{\mathbf{h}}_j : j \in V_{\text{light}}\}) \quad \text{(æœ€è¿‘äº¤é€šç¯è¡¨å¾)} \\
\mathbf{h}_{\text{stop}}^{(i)} &= \text{avg}(\{\tilde{\mathbf{h}}_j : j \in V_{\text{stop}}\}) \quad \text{(æœ€è¿‘åœæ­¢çº¿è¡¨å¾)} \\
\beta_i &= \text{sigmoid}\left(\mathbf{w}_{\text{rule}}^\top [\tilde{\mathbf{h}}_i \| \mathbf{h}_{\text{light}}^{(i)} \| \mathbf{h}_{\text{stop}}^{(i)}]\right) \quad \in [0,1] \\
\mathbf{h}_i^{\text{rule}} &= \beta_i \cdot \tilde{\mathbf{h}}_i + (1-\beta_i) \cdot \mathbf{e}_{\text{rule}}
\end{aligned}
$$

å…¶ä¸­ï¼š
- $\beta_i \in [0,1]$ï¼š**è§„åˆ™èšç„¦æ³¨æ„åŠ›åˆ†æ•°**ï¼Œè¡¨ç¤ºè½¦è¾†$i$å¯¹è§„åˆ™ç›¸å…³å®ä½“çš„å…³æ³¨ç¨‹åº¦
- $\mathbf{e}_{\text{rule}}$ï¼šå¯å­¦ä¹ çš„è§„åˆ™åµŒå…¥å‘é‡ï¼ˆé€šè¿‡`rule_embeddings`è·å–ï¼‰
- $\mathbf{w}_{\text{rule}} \in \mathbb{R}^{3 \times d_h}$ï¼šè§„åˆ™è¯„åˆ†å™¨çš„æƒé‡

**ç‰¹ç‚¹**ï¼š
- âœ… è§„åˆ™è¯­ä¹‰æ³¨å…¥ï¼ˆé€šè¿‡å¯å­¦ä¹ çš„rule embeddingï¼‰
- âœ… åŠ¨æ€èšç„¦ï¼ˆä¸åŒè½¦è¾†æ ¹æ®ä¸è§„åˆ™ç›¸å…³å®ä½“çš„å…³ç³»è·å¾—ä¸åŒæƒé‡ï¼‰
- âœ… å¯æ‰©å±•ï¼ˆæ”¯æŒå¤šç§è§„åˆ™ï¼Œé€šè¿‡rule_idåˆ‡æ¢ï¼‰
- âœ… å¯è§£é‡Šï¼ˆ$\beta_i$ç›´æ¥è¡¨ç¤ºå¯¹è§„åˆ™çš„å…³æ³¨ç¨‹åº¦ï¼Œç”¨äºæŸå¤±å‡½æ•°ç›‘ç£ï¼‰

**è¾“å‡º**ï¼š
- $\mathbf{h}_i^{\text{rule}} \in \mathbb{R}^{d_h}$ï¼šè§„åˆ™èšç„¦åçš„èŠ‚ç‚¹è¡¨å¾ï¼ˆç”¨äºå¼‚å¸¸åˆ†æ•°è®¡ç®—ï¼‰
- $\beta_i \in [0,1]$ï¼šè§„åˆ™æ³¨æ„åŠ›åˆ†æ•°ï¼ˆç”¨äº$\mathcal{L}_{\text{attn}}^{\text{rule}}$æŸå¤±ï¼‰

**ä¸GATæ³¨æ„åŠ›çš„å…³ç³»**ï¼š
- $\alpha_{ij}^{(L)}$ï¼šGATå±€éƒ¨æ³¨æ„åŠ›ï¼Œæ•è·ç©ºé—´é‚»åŸŸä¿¡æ¯ï¼ˆè¾¹çº§åˆ«ï¼‰
- $\beta_i$ï¼šè§„åˆ™èšç„¦æ³¨æ„åŠ›ï¼Œæ•è·è§„åˆ™è¯­ä¹‰ä¿¡æ¯ï¼ˆèŠ‚ç‚¹çº§åˆ«ï¼‰
- ä¸¤è€…äº’è¡¥ï¼š$\alpha$æä¾›åº•å±‚ç©ºé—´å…³ç³»ï¼Œ$\beta$æä¾›é«˜å±‚è¯­ä¹‰å…³æ³¨

**å¼‚å¸¸åˆ†æ•°å¤´**ï¼š
$$
s_i^{\text{model}} = \sigma\left(\text{MLP}_{\text{score}}(\mathbf{h}_i^{\text{rule}})\right) \in [0,1]
$$

#### 3.3.4 æ¢¯åº¦æµè®¾è®¡ä¸å‚æ•°å…±äº«ç­–ç•¥

> **æ–°å¢ç« èŠ‚ï¼ˆ2025-12-03ï¼‰**ï¼šè§£å†³é—®é¢˜4 - ä¸‰é˜¶æ®µæ³¨æ„åŠ›çš„æ¢¯åº¦æµæ–­è£‚é£é™©ã€‚é€šè¿‡æ®‹å·®è¿æ¥å’Œå‚æ•°å…±äº«ç¡®ä¿æ¢¯åº¦é¡ºç•…ä¼ æ’­ã€‚

**é—®é¢˜åˆ†æ**ï¼š
- ä¸‰ä¸ªé˜¶æ®µä¸²è”å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼ˆå¤šæ¬¡éçº¿æ€§å˜æ¢ï¼‰
- å„é˜¶æ®µå‚æ•°ç‹¬ç«‹ï¼Œå¯èƒ½å¯¼è‡´æŸé˜¶æ®µè®­ç»ƒä¸è¶³ï¼ˆæ¢¯åº¦ç«äº‰ï¼‰
- è§„åˆ™åµŒå…¥$\mathbf{e}_{\text{rule}}$ä¸GATå±‚æ— ç›´æ¥è¿æ¥

**è§£å†³æ–¹æ¡ˆï¼šå¤šè·¯å¾„æ¢¯åº¦æµè®¾è®¡**

**1. è·¨é˜¶æ®µæ®‹å·®è¿æ¥**ï¼š

$$
\begin{aligned}
\mathbf{h}^{(L)}_{\text{local}} &= \text{GAT}_{\text{layers}}(\mathbf{x}) \quad \text{(é˜¶æ®µ1è¾“å‡º)} \\
\tilde{\mathbf{h}}_{\text{global}} &= \text{GlobalAttn}(\mathbf{h}^{(L)}_{\text{local}}) \quad \text{(é˜¶æ®µ2è¾“å‡º)} \\
\mathbf{h}_{\text{rule}} &= \text{RuleFocus}(\tilde{\mathbf{h}}_{\text{global}}) \quad \text{(é˜¶æ®µ3è¾“å‡º)} \\
\\
\mathbf{h}_{\text{final}} &= \gamma_1 \mathbf{h}^{(L)}_{\text{local}} + \gamma_2 \tilde{\mathbf{h}}_{\text{global}} + \gamma_3 \mathbf{h}_{\text{rule}} \quad \text{(å¤šè·¯å¾„èåˆ)}
\end{aligned}
$$

å…¶ä¸­$\gamma_1, \gamma_2, \gamma_3$ä¸ºå¯å­¦ä¹ æƒé‡ï¼ˆåˆå§‹åŒ–ä¸º$[0.2, 0.3, 0.5]$ï¼‰ã€‚

**ç‰©ç†æ„ä¹‰**ï¼š
- $\gamma_1$è·¯å¾„ï¼šç›´æ¥ä»GATä¼ æ’­æ¢¯åº¦ï¼ˆçŸ­è·¯å¾„ï¼Œæ¢¯åº¦å¼ºï¼‰
- $\gamma_2$è·¯å¾„ï¼šç»è¿‡å…¨å±€æ³¨æ„åŠ›ï¼ˆä¸­è·¯å¾„ï¼‰
- $\gamma_3$è·¯å¾„ï¼šç»è¿‡å®Œæ•´ä¸‰é˜¶æ®µï¼ˆé•¿è·¯å¾„ï¼Œè¯­ä¹‰ä¸°å¯Œï¼‰

**2. å‚æ•°å…±äº«ç­–ç•¥**ï¼š

| ç»„ä»¶ | å‚æ•° | æ˜¯å¦å…±äº« | å…±äº«å¯¹è±¡ | ç†ç”± |
|------|------|---------|---------|------|
| **GATå±‚** | $\mathbf{W}_k^{(l)}, \mathbf{a}_k$ | âŒ ç‹¬ç«‹ | - | å„å±‚å­¦ä¹ ä¸åŒæŠ½è±¡çº§åˆ« |
| **å…¨å±€æ³¨æ„åŠ›** | $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v$ | âš ï¸ éƒ¨åˆ†å…±äº« | ä¸GATç¬¬3å±‚å…±äº«$\mathbf{W}_k$ | å‡å°‘å‚æ•°ï¼ŒåŠ å¼ºæ¢¯åº¦æµ |
| **è§„åˆ™èšç„¦** | $\mathbf{w}_{\text{rule}}$ | âŒ ç‹¬ç«‹ | - | è§„åˆ™è¯­ä¹‰ç‹¬ç‰¹ |
| **è§„åˆ™åµŒå…¥** | $\mathbf{e}_{\text{rule}}$ | âœ… å…¨å±€å…±äº« | æ‰€æœ‰è½¦è¾†å…±äº«åŒä¸€è§„åˆ™åµŒå…¥ | è§„åˆ™ä¸€è‡´æ€§ |
| **Scoring Head** | MLPæƒé‡ | âŒ ç‹¬ç«‹ | - | æœ€ç»ˆåˆ¤åˆ«å™¨ |

**å®ç°ä»£ç ï¼ˆä¿®æ­£ç‰ˆï¼‰**ï¼š
```python
class MultiStageAttentionGAT(nn.Module):
    def __init__(self, hidden_dim=128, num_gat_layers=3, num_heads=8):
        super().__init__()
        
        # é˜¶æ®µ1ï¼šå±€éƒ¨GAT
        self.gat_layers = nn.ModuleList([...])
        
        # é˜¶æ®µ2ï¼šå…¨å±€æ³¨æ„åŠ›ï¼ˆå‚æ•°å…±äº«ï¼‰
        self.global_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=4
        )
        # å…±äº«GATç¬¬3å±‚çš„KeyæŠ•å½±æƒé‡
        with torch.no_grad():
            self.global_attn.in_proj_weight[hidden_dim:2*hidden_dim] = \
                self.gat_layers[2].lin_key.weight  # å…±äº«KæŠ•å½±
        
        # é˜¶æ®µ3ï¼šè§„åˆ™èšç„¦
        self.rule_focus = RuleFocusedAttention(hidden_dim)
        
        # å¤šè·¯å¾„èåˆæƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
        self.path_weights = nn.Parameter(torch.tensor([0.2, 0.3, 0.5]))
        
        # Scoring head
        self.score_head = nn.Sequential(...)
    
    def forward(self, x, edge_index, entity_types, return_attention=False):
        # é˜¶æ®µ1
        h_local = self.encode_local_gat(x, edge_index)  # [N, 128]
        
        # é˜¶æ®µ2
        h_global, global_attn = self.global_attn(...)
        h_global = h_global + h_local  # æ®‹å·®è¿æ¥
        
        # é˜¶æ®µ3
        h_rule, beta = self.rule_focus(h_global, entity_types)
        h_rule = h_rule + h_global  # æ®‹å·®è¿æ¥
        
        # å¤šè·¯å¾„èåˆï¼ˆå…³é”®ï¼šæ¢¯åº¦å‡è¡¡ï¼‰
        gamma = F.softmax(self.path_weights, dim=0)
        h_final = gamma[0] * h_local + gamma[1] * h_global + gamma[2] * h_rule
        
        # æœ€ç»ˆè¯„åˆ†
        scores = self.score_head(h_final[entity_types == 0])
        
        return scores, ...
```

**3. æ¢¯åº¦æµå‘å›¾**ï¼š

```
æŸå¤± L_total
    â”‚
    â”œâ”€â†’ L_recon (BCE)
    â”‚      â”‚
    â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      â”‚                    â†“
    â”‚      â”‚              score_head (MLP)
    â”‚      â”‚                    â†‘
    â”‚      â”‚               h_final (èåˆ)
    â”‚      â”‚            â†— (Î³1)  â†‘ (Î³2)  â†– (Î³3)
    â”‚      â”‚       h_local   h_global   h_rule
    â”‚      â”‚          â†‘          â†‘          â†‘
    â”‚      â”‚       GAT(L3)  GlobalAttn  RuleFocus
    â”‚      â”‚          â†‘          â†‘          â†‘
    â”‚      â”‚       [å‚æ•°å…±äº«] â†â”˜          â”‚
    â”‚      â”‚                              â”‚
    â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         (ä¸»æ¢¯åº¦è·¯å¾„ï¼šçŸ­è·¯å¾„Î³1 + é•¿è·¯å¾„Î³3)
    â”‚
    â”œâ”€â†’ L_rule (MSE)
    â”‚      â””â”€â†’ åŒä¸Š
    â”‚
    â”œâ”€â†’ L_attn^GAT
    â”‚      â””â”€â†’ Î±_ij^(L) â† GATç¬¬Lå±‚ï¼ˆç›´æ¥æ¢¯åº¦ï¼‰
    â”‚
    â””â”€â†’ L_attn^rule
           â””â”€â†’ Î²_i â† RuleFocusï¼ˆç»è¿‡h_ruleè·¯å¾„ï¼‰
```

**æ¢¯åº¦å¹³è¡¡æœºåˆ¶**ï¼š
- **è‡ªåŠ¨æƒé‡è°ƒæ•´**ï¼š$\gamma_1, \gamma_2, \gamma_3$é€šè¿‡softmaxå½’ä¸€åŒ–ï¼Œè‡ªåŠ¨å­¦ä¹ æœ€ä¼˜èåˆæ¯”ä¾‹
- **æ®‹å·®è¿æ¥**ï¼šæ¯ä¸ªé˜¶æ®µéƒ½æœ‰åˆ°å‰ä¸€é˜¶æ®µçš„æ®‹å·®è¿æ¥ï¼Œç¡®ä¿æ¢¯åº¦çŸ­è·¯å¾„
- **ç›´æ¥ç›‘ç£**ï¼š$\mathcal{L}_{\text{attn}}^{\text{GAT}}$ç›´æ¥ç›‘ç£GATå±‚ï¼Œ$\mathcal{L}_{\text{attn}}^{\text{rule}}$ç›´æ¥ç›‘ç£è§„åˆ™èšç„¦å±‚
- **å‚æ•°å…±äº«**ï¼šå…¨å±€æ³¨æ„åŠ›ä¸GATç¬¬3å±‚å…±äº«KæŠ•å½±ï¼Œå¢å¼ºæ¢¯åº¦æµ

**éªŒè¯æŒ‡æ ‡**ï¼ˆè®­ç»ƒæ—¶ç›‘æ§ï¼‰ï¼š
```python
# è®°å½•å„é˜¶æ®µæ¢¯åº¦èŒƒæ•°
grad_norms = {
    'gat_layers': compute_grad_norm(model.gat_layers.parameters()),
    'global_attn': compute_grad_norm(model.global_attn.parameters()),
    'rule_focus': compute_grad_norm(model.rule_focus.parameters()),
    'score_head': compute_grad_norm(model.score_head.parameters()),
}

# ç†æƒ³çŠ¶æ€ï¼šå„é˜¶æ®µæ¢¯åº¦èŒƒæ•°åœ¨åŒä¸€æ•°é‡çº§ï¼ˆ1e-3 ~ 1e-2ï¼‰
# å¦‚æœæŸé˜¶æ®µ<1e-4ï¼Œè¯´æ˜æ¢¯åº¦æ¶ˆå¤±ï¼Œéœ€è¦å¢åŠ å…¶æƒé‡Î³
```

**æ¢¯åº¦æµä¿è¯**ï¼š
âœ… çŸ­è·¯å¾„ï¼ˆ$\gamma_1$ï¼‰ç¡®ä¿GATå±‚å§‹ç»ˆå¾—åˆ°æ¢¯åº¦  
âœ… é•¿è·¯å¾„ï¼ˆ$\gamma_3$ï¼‰æä¾›è¯­ä¹‰ä¸°å¯Œçš„æ¢¯åº¦ä¿¡å·  
âœ… æ®‹å·®è¿æ¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±  
âœ… å‚æ•°å…±äº«å¢å¼ºè·¨é˜¶æ®µæ¢¯åº¦æµ

#### 3.3.5 Memory Bankä¸å¼‚å¸¸æ£€æµ‹å¢å¼º

> **æ–°å¢ç« èŠ‚ï¼ˆ2025-12-03ï¼‰**ï¼šè¡¥å……memoryæ¨¡å—çš„è¯¦ç»†è®¾è®¡ï¼Œæ˜ç¡®å…¶åœ¨å¤šé˜¶æ®µæ³¨æ„åŠ›æ¶æ„ä¸­çš„ä½œç”¨ã€‚  
> è§£å†³é—®é¢˜7ï¼šmemoryæ¨¡å—ç¼ºå¤±è®¾è®¡

**è®¾è®¡åŠ¨æœº**ï¼š
- GAT + è§„åˆ™çº¦æŸå¯ä»¥æ£€æµ‹æ˜æ˜¾è¿è§„ï¼Œä½†å¯¹è¾¹ç•Œæƒ…å†µï¼ˆå¦‚ç¼“æ…¢é€šè¿‡çº¢ç¯ï¼‰å¯èƒ½ä¸æ•æ„Ÿ
- Memory Bankå­˜å‚¨**æ­£å¸¸é©¾é©¶è¡Œä¸ºçš„åŸå‹è¡¨å¾**ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¢å¼ºå¼‚å¸¸æ£€æµ‹èƒ½åŠ›
- ä¸æ–¹æ¡ˆ2çš„åŒºåˆ«ï¼šæœ¬æ–¹æ¡ˆçš„memoryæ˜¯**å¯é€‰å¢å¼ºæ¨¡å—**ï¼Œä¸å½±å“æ ¸å¿ƒGATæ¶æ„

**æ¶æ„é›†æˆä½ç½®**ï¼š
```
GATå±€éƒ¨ç¼–ç  (h_local)
    â†“
å…¨å±€ä¸Šä¸‹æ–‡èåˆ (h_global)
    â†“
Memoryæ£€ç´¢ä¸å¯¹æ¯” (h_mem, distance)  â† æœ¬èŠ‚è®¾è®¡
    â†“
è§„åˆ™èšç„¦æ³¨æ„åŠ› (h_rule)
    â†“
å¼‚å¸¸åˆ†æ•°è®¡ç®— (s_model, s_mem)
```

**Memory Bankå®šä¹‰**ï¼š

$$
\mathbf{M} = [\mathbf{m}_1, \dots, \mathbf{m}_K] \in \mathbb{R}^{K \times d_h}
$$

å…¶ä¸­$K$ä¸ºè®°å¿†æ§½æ•°é‡ï¼ˆæ¨è$K=512$ï¼‰ï¼Œ$d_h=128$ä¸ºéšè—ç»´åº¦ã€‚

**åˆå§‹åŒ–ç­–ç•¥**ï¼ˆK-Meansèšç±»æ­£å¸¸æ ·æœ¬ï¼‰ï¼š
```python
def initialize_memory_bank(normal_samples: List[SceneGraph], K: int = 512):
    """
    ä½¿ç”¨K-Meansèšç±»åˆå§‹åŒ–Memory Bank
    
    Args:
        normal_samples: æ­£å¸¸é©¾é©¶åœºæ™¯åˆ—è¡¨ï¼ˆç»¿ç¯é€šè¿‡ã€çº¢ç¯åœè½¦ç­‰ï¼‰
        K: è®°å¿†æ§½æ•°é‡
    
    Returns:
        memory_bank: [K, hidden_dim]
    """
    # 1. ç¼–ç æ‰€æœ‰æ­£å¸¸æ ·æœ¬
    model.eval()
    embeddings = []
    with torch.no_grad():
        for scene in normal_samples:
            h_global = model.encode_to_global(scene.features, scene.edge_index)
            # å–è½¦è¾†èŠ‚ç‚¹çš„å¹³å‡è¡¨å¾
            h_cars = h_global[scene.entity_types == 0]
            embeddings.append(h_cars.mean(dim=0))
    
    embeddings = torch.stack(embeddings)  # [N_normal, hidden_dim]
    
    # 2. K-Meansèšç±»
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=K, random_state=42)
    kmeans.fit(embeddings.cpu().numpy())
    
    # 3. èšç±»ä¸­å¿ƒä½œä¸ºè®°å¿†åŸå‹
    memory_bank = torch.from_numpy(kmeans.cluster_centers_).float()
    
    return memory_bank
```

**æ£€ç´¢æœºåˆ¶**ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ + Softmaxï¼‰ï¼š
$$
\begin{aligned}
\text{sim}_{ik} &= \frac{\tilde{\mathbf{h}}_i^\top \mathbf{m}_k}{\|\tilde{\mathbf{h}}_i\| \cdot \|\mathbf{m}_k\|} \quad \text{(ä½™å¼¦ç›¸ä¼¼åº¦)} \\
w_{ik} &= \frac{\exp(\text{sim}_{ik} / \tau_{\text{mem}})}{\sum_{k'=1}^K \exp(\text{sim}_{ik'} / \tau_{\text{mem}})} \quad \text{(æ£€ç´¢æƒé‡)} \\
\mathbf{h}_i^{\text{mem}} &= \sum_{k=1}^K w_{ik} \mathbf{m}_k \quad \text{(æ£€ç´¢åˆ°çš„è®°å¿†è¡¨å¾)}
\end{aligned}
$$

å…¶ä¸­$\tau_{\text{mem}} = 0.07$ä¸ºæ¸©åº¦ç³»æ•°ï¼ˆæ§åˆ¶æ£€ç´¢é”åº¦ï¼‰ã€‚

**å¼‚å¸¸åˆ†æ•°è®¡ç®—**ï¼ˆé©¬æ°è·ç¦»ï¼‰ï¼š
$$
\begin{aligned}
\mathbf{d}_i &= \tilde{\mathbf{h}}_i - \mathbf{h}_i^{\text{mem}} \quad \text{(è¡¨å¾å·®å¼‚)} \\
s_i^{\text{mem}} &= \sigma\left(\sqrt{\mathbf{d}_i^\top \mathbf{\Sigma}^{-1} \mathbf{d}_i}\right) \quad \text{(é©¬æ°è·ç¦»å½’ä¸€åŒ–)}
\end{aligned}
$$

å…¶ä¸­$\mathbf{\Sigma} \in \mathbb{R}^{d_h \times d_h}$ä¸ºè®°å¿†åº“çš„åæ–¹å·®çŸ©é˜µï¼ˆåœ¨çº¿ä¼°è®¡ï¼‰ã€‚

**æ›´æ–°ç­–ç•¥**ï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰ï¼š
```python
# æ¯ä¸ªepochç»“æŸåï¼Œç”¨æ­£å¸¸æ ·æœ¬æ›´æ–°è®°å¿†åº“
def update_memory_bank(
    memory_bank: torch.Tensor,
    new_normal_embeddings: torch.Tensor,
    momentum: float = 0.9
):
    """
    EMAæ›´æ–°è®°å¿†åº“
    
    Args:
        memory_bank: [K, hidden_dim] å½“å‰è®°å¿†åº“
        new_normal_embeddings: [N, hidden_dim] æ–°çš„æ­£å¸¸æ ·æœ¬è¡¨å¾
        momentum: EMAåŠ¨é‡ç³»æ•°
    """
    with torch.no_grad():
        # 1. ä¸ºæ¯ä¸ªæ–°æ ·æœ¬æ‰¾åˆ°æœ€è¿‘çš„è®°å¿†æ§½
        similarities = F.cosine_similarity(
            new_normal_embeddings.unsqueeze(1),  # [N, 1, D]
            memory_bank.unsqueeze(0),            # [1, K, D]
            dim=-1
        )  # [N, K]
        nearest_slots = similarities.argmax(dim=1)  # [N]
        
        # 2. EMAæ›´æ–°å¯¹åº”çš„è®°å¿†æ§½
        for i, slot_idx in enumerate(nearest_slots):
            memory_bank[slot_idx] = (
                momentum * memory_bank[slot_idx] + 
                (1 - momentum) * new_normal_embeddings[i]
            )
        
        # 3. L2å½’ä¸€åŒ–ï¼ˆä¿æŒå•ä½çƒé¢ï¼‰
        memory_bank = F.normalize(memory_bank, p=2, dim=-1)
    
    return memory_bank
```

**ä¸å¼‚å¸¸åˆ†æ•°çš„èåˆ**ï¼š
$$
s_i^{\text{final}} = \lambda_{\text{model}} \cdot s_i^{\text{model}} + \lambda_{\text{mem}} \cdot s_i^{\text{mem}} + \lambda_{\text{rule}} \cdot s_i^{\text{rule}}
$$

æ¨èæƒé‡ï¼š$\lambda_{\text{model}} = 0.5$ï¼Œ$\lambda_{\text{mem}} = 0.2$ï¼Œ$\lambda_{\text{rule}} = 0.3$ï¼ˆè§„åˆ™æœ€å¯ä¿¡ï¼‰ã€‚

**å†…å­˜å¼€é”€ä¼°ç®—**ï¼š
```python
# Memory Bank: K * hidden_dim * 4 bytes
# 512 * 128 * 4 = 262,144 bytes â‰ˆ 256 KB

# åæ–¹å·®çŸ©é˜µ: hidden_dim * hidden_dim * 4 bytes
# 128 * 128 * 4 = 65,536 bytes = 64 KB

# æ€»è®¡ï¼š< 1 MBï¼ˆå‡ ä¹å¯å¿½ç•¥ï¼‰
```

**å¯é€‰æ€§è¯´æ˜**ï¼š
- Memoryæ¨¡å—é»˜è®¤**ç¦ç”¨**ï¼ˆMVPé˜¶æ®µï¼‰
- é€šè¿‡é…ç½®æ–‡ä»¶å¯ç”¨ï¼š`model.use_memory_bank: true`
- å¯ç”¨åå¢åŠ çº¦5%çš„è®­ç»ƒæ—¶é—´ï¼ˆæ£€ç´¢å¼€é”€ï¼‰
- åœ¨Week 2ä¼˜åŒ–é˜¶æ®µè¯„ä¼°å…¶å¯¹AUCçš„æå‡ï¼ˆé¢„æœŸ+2-3%ï¼‰

### 3.4 è§„åˆ™å¼•æ“ä¸çº¦æŸæŸå¤±

> **æŠ€æœ¯å‹˜è¯¯ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šåŸè§„åˆ™åˆ†æ•°å…¬å¼ä½¿ç”¨ç¦»æ•£æŒ‡ç¤ºå‡½æ•° $\mathbb{1}[\text{red}]$ï¼Œä¸å¯å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚ç°æ”¹ç”¨Gumbel-Softmaxè½¯åŒ–ï¼Œç¡®ä¿å…¨ç¨‹å¯å¯¼ã€‚  
> è¯¦è§ï¼š`docs/archive/design/TECHNICAL_CORRECTIONS.md` é—®é¢˜1

#### 3.4.1 è§„åˆ™å½¢å¼åŒ–å®šä¹‰

> **é‡å¤§ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šåŸå…¬å¼å­˜åœ¨è·ç¦»é¡¹é€»è¾‘é”™è¯¯å’Œé€Ÿåº¦é¡¹è¾¹ç•Œæ¡ä»¶é”™è¯¯ã€‚ç°é‡æ–°è®¾è®¡ç‰©ç†æ­£ç¡®çš„è§„åˆ™åˆ†æ•°å…¬å¼ï¼ŒåŒºåˆ†"æ¥è¿‘åœæ­¢çº¿"å’Œ"é—¯è¿‡åœæ­¢çº¿"ä¸¤ç§æƒ…å†µã€‚

**ç‰©ç†æ¨¡å‹è¯´æ˜**ï¼š
- **è§„åˆ™åˆ†æ•°è¯­ä¹‰**ï¼šè¿è§„ç¨‹åº¦ï¼ˆ0=æ— è¿è§„ï¼Œ1=ä¸¥é‡è¿è§„ï¼‰
- **è·ç¦»çº¦å®š**ï¼š$d > 0$è¡¨ç¤ºè½¦è¾†åœ¨åœæ­¢çº¿å‰ï¼Œ$d < 0$è¡¨ç¤ºè½¦è¾†å·²è¿‡åœæ­¢çº¿ï¼ˆé—¯è¿‡ï¼‰
- **é€Ÿåº¦çº¦å®š**ï¼š$v = 0$è¡¨ç¤ºå®Œå…¨åœæ­¢ï¼ˆæ— è¿è§„ï¼‰ï¼Œ$v > 0$è¡¨ç¤ºç§»åŠ¨ä¸­

**çº¢ç¯åœè§„åˆ™**ï¼ˆç¡¬é˜ˆå€¼ç‰ˆï¼Œç”¨äºéªŒæ”¶æµ‹è¯•ï¼‰ï¼š
$$
\text{violation}_{\text{hard}}(i) = \begin{cases}
1, & \text{if } \text{light}_{\text{state}} = \text{red} \land \left(d_{\text{stop}}(i) < 0 \lor (0 \le d_{\text{stop}}(i) < \tau_d \land v(i) > \tau_v)\right) \\
0, & \text{otherwise}
\end{cases}
$$

å…¶ä¸­ $\tau_d = 5m$ï¼Œ$\tau_v = 0.5 m/s$ã€‚

**è§„åˆ™åˆ†æ•°**ï¼ˆè½¯åŒ–ç‰ˆï¼Œå®Œå…¨å¯å¾®åˆ†ï¼‰ï¼š

ä½¿ç”¨Gumbel-Softmaxè½¯åŒ–ç¦»æ•£äº¤é€šç¯çŠ¶æ€ï¼š
$$
\text{light\_state} = [p_{\text{red}}, p_{\text{yellow}}, p_{\text{green}}] \quad \text{(softmaxæ¦‚ç‡)}
$$
$$
w_{\text{light}} = \text{GumbelSoftmax}(\text{light\_state}, \tau_{\text{temp}}=0.5)[0] \quad \text{(redé€šé“æƒé‡)}
$$

**åˆ†æ®µè·ç¦»-é€Ÿåº¦è¯„åˆ†å‡½æ•°**ï¼š
$$
f_{\text{dv}}(d, v) = \begin{cases}
\sigma\left(\alpha_{\text{cross}} \cdot (-d)\right) \cdot \sigma\left(\alpha_v \cdot v\right), & \text{if } d < 0 \quad \text{(å·²è¿‡çº¿)} \\
\sigma\left(\alpha_d \cdot (\tau_d - d)\right) \cdot \sigma\left(\alpha_v \cdot (v - \tau_v)\right), & \text{if } 0 \le d < \tau_d \quad \text{(æ¥è¿‘åœæ­¢çº¿)} \\
0, & \text{if } d \ge \tau_d \quad \text{(è¿œç¦»åœæ­¢çº¿)}
\end{cases}
$$

**æœ€ç»ˆè§„åˆ™åˆ†æ•°**ï¼š
$$
s_i^{\text{rule}} = w_{\text{light}} \cdot f_{\text{dv}}(d_{\text{stop}}(i), v(i))
$$

**å‚æ•°è¯´æ˜**ï¼š
- $w_{\text{light}} \in (0, 1)$ï¼šäº¤é€šç¯æƒé‡ï¼Œçº¢ç¯æ—¶æ¥è¿‘1ï¼Œç»¿ç¯æ—¶æ¥è¿‘0
- $\alpha_{\text{cross}} = 3.0$ï¼šè¿‡çº¿è¿è§„æ•æ„Ÿåº¦ï¼ˆé—¯è¿‡åœæ­¢çº¿åï¼Œè¿è§„ç¨‹åº¦éšè·ç¦»å¢åŠ ï¼‰
- $\alpha_d = 2.0$ï¼šæ¥è¿‘åœæ­¢çº¿æ•æ„Ÿåº¦ï¼ˆè·ç¦»è¶Šè¿‘ï¼Œè¿è§„é£é™©è¶Šé«˜ï¼‰
- $\alpha_v = 5.0$ï¼šé€Ÿåº¦æ•æ„Ÿåº¦
- $\tau_d = 5.0m$ï¼šå®‰å…¨åœè½¦è·ç¦»é˜ˆå€¼
- $\tau_v = 0.5 m/s$ï¼šåœè½¦é€Ÿåº¦é˜ˆå€¼ï¼ˆæ¥è¿‘é™æ­¢ï¼‰
- $\sigma(x) = \frac{1}{1+e^{-x}}$ï¼šSigmoidå‡½æ•°

**ç‰©ç†æ„ä¹‰éªŒè¯**ï¼š
1. **å®Œå…¨åœæ­¢**ï¼ˆ$v=0$ï¼Œ$d>0$ï¼‰ï¼š
   - $f_{\text{dv}}(d, 0) = \sigma(\alpha_d(\tau_d-d)) \cdot \sigma(-\alpha_v \tau_v) \approx \sigma(\alpha_d(\tau_d-d)) \cdot 0.076$
   - å½“$d$è¾ƒå¤§æ—¶ï¼ˆå¦‚$d=10m>\tau_d$ï¼‰ï¼š$f_{\text{dv}} = 0$
   - âœ… åœè½¦ç­‰å¾…æ—¶è¿è§„åˆ†æ•°å¾ˆä½

2. **è¿œç¦»åœæ­¢çº¿**ï¼ˆ$d \ge \tau_d$ï¼‰ï¼š
   - $f_{\text{dv}}(d, v) = 0$
   - âœ… æ— è®ºé€Ÿåº¦å¤šå°‘ï¼Œè¿œç¦»åœæ­¢çº¿éƒ½ä¸è¿è§„

3. **é—¯è¿‡åœæ­¢çº¿**ï¼ˆ$d<0$ï¼Œ$v>0$ï¼‰ï¼š
   - $f_{\text{dv}}(-2, 2) = \sigma(3.0 \cdot 2) \cdot \sigma(5.0 \cdot 2) \approx 0.998 \cdot 1.0 \approx 0.998$
   - âœ… é—¯çº¢ç¯ä¸¥é‡è¿è§„

4. **æ¥è¿‘åœæ­¢çº¿ä¸”é€Ÿåº¦è¿‡å¿«**ï¼ˆ$0<d<\tau_d$ï¼Œ$v>\tau_v$ï¼‰ï¼š
   - $f_{\text{dv}}(2, 2) = \sigma(2.0 \cdot 3) \cdot \sigma(5.0 \cdot 1.5) \approx 0.998 \cdot 0.999 \approx 0.997$
   - âœ… å†²å‘çº¢ç¯é«˜è¿è§„åˆ†æ•°

**æ¢¯åº¦åˆ†æ**ï¼ˆéªŒè¯å¯å¯¼æ€§ï¼‰ï¼š

å¯¹äº$d<0$æƒ…å†µï¼š
$$
\frac{\partial f_{\text{dv}}}{\partial d} = -\alpha_{\text{cross}} \cdot \sigma'(\alpha_{\text{cross}} \cdot (-d)) \cdot \sigma(\alpha_v \cdot v) \neq 0
$$

å¯¹äº$0 \le d < \tau_d$æƒ…å†µï¼š
$$
\frac{\partial f_{\text{dv}}}{\partial d} = -\alpha_d \cdot \sigma'(\alpha_d (\tau_d - d)) \cdot \sigma(\alpha_v (v - \tau_v)) \neq 0
$$

å¯¹äºé€Ÿåº¦ï¼š
$$
\frac{\partial f_{\text{dv}}}{\partial v} = \alpha_v \cdot \sigma(\cdots) \cdot \sigma'(\alpha_v \cdot v \text{ or } \alpha_v(v-\tau_v)) \neq 0
$$

âœ… **å…¨ç¨‹å¯å¯¼ï¼Œæ— æ¢¯åº¦æ¶ˆå¤±**

**å®ç°ä»£ç **ï¼š
```python
import torch
import torch.nn.functional as F

def compute_rule_score_differentiable(
    light_probs: torch.Tensor,  # [B, 3] - [red, yellow, green]
    distances: torch.Tensor,    # [B] - distance to stop line (æ­£æ•°=æœªè¿‡çº¿ï¼Œè´Ÿæ•°=å·²è¿‡çº¿)
    velocities: torch.Tensor,   # [B] - vehicle velocity
    tau_d: float = 5.0,         # å®‰å…¨åœè½¦è·ç¦»
    tau_v: float = 0.5,         # åœè½¦é€Ÿåº¦é˜ˆå€¼
    alpha_d: float = 2.0,       # æ¥è¿‘åœæ­¢çº¿æ•æ„Ÿåº¦
    alpha_v: float = 5.0,       # é€Ÿåº¦æ•æ„Ÿåº¦
    alpha_cross: float = 3.0,   # è¿‡çº¿è¿è§„æ•æ„Ÿåº¦
    temperature: float = 0.5,   # Gumbel-Softmaxæ¸©åº¦
    training: bool = True,      # è®­ç»ƒæ¨¡å¼æ ‡å¿—
):
    """
    å®Œå…¨å¯å¯¼çš„è§„åˆ™è¯„åˆ†å‡½æ•°ï¼ˆç‰©ç†æ­£ç¡®ç‰ˆï¼‰
    
    è¿”å›ï¼š
        rule_scores: [B] - è¿è§„åˆ†æ•°ï¼Œ0=æ— è¿è§„ï¼Œ1=ä¸¥é‡è¿è§„
    """
    # Step 1: Gumbel-Softmaxè½¯åŒ–äº¤é€šç¯çŠ¶æ€
    if training:
        light_weights = F.gumbel_softmax(
            torch.log(light_probs + 1e-10), 
            tau=temperature, 
            hard=False
        )[:, 0]  # æå–redé€šé“
    else:
        light_weights = light_probs[:, 0]  # æ¨ç†æ—¶ç›´æ¥ä½¿ç”¨redæ¦‚ç‡
    
    # Step 2: è®¡ç®—åˆ†æ®µè·ç¦»-é€Ÿåº¦è¯„åˆ†
    B = distances.size(0)
    f_dv = torch.zeros(B, device=distances.device)
    
    # æƒ…å†µ1ï¼šå·²è¿‡çº¿ï¼ˆd < 0ï¼‰
    crossed_mask = (distances < 0)
    if crossed_mask.any():
        # è¿‡çº¿åï¼Œè·ç¦»è¶Šè¿œï¼ˆè´Ÿå¾—è¶Šå¤šï¼‰ï¼Œè¿è§„è¶Šä¸¥é‡
        f_dv[crossed_mask] = (
            torch.sigmoid(alpha_cross * (-distances[crossed_mask])) *
            torch.sigmoid(alpha_v * velocities[crossed_mask])
        )
    
    # æƒ…å†µ2ï¼šæ¥è¿‘åœæ­¢çº¿ï¼ˆ0 <= d < tau_dï¼‰
    approaching_mask = (distances >= 0) & (distances < tau_d)
    if approaching_mask.any():
        # è·ç¦»è¶Šè¿‘ä¸”é€Ÿåº¦è¶Šé«˜ï¼Œè¿è§„é£é™©è¶Šå¤§
        f_dv[approaching_mask] = (
            torch.sigmoid(alpha_d * (tau_d - distances[approaching_mask])) *
            torch.sigmoid(alpha_v * (velocities[approaching_mask] - tau_v))
        )
    
    # æƒ…å†µ3ï¼šè¿œç¦»åœæ­¢çº¿ï¼ˆd >= tau_dï¼‰
    # f_dvä¿æŒä¸º0ï¼ˆå·²åˆå§‹åŒ–ï¼‰
    
    # Step 3: ç»„åˆäº¤é€šç¯æƒé‡
    rule_scores = light_weights * f_dv
    
    return rule_scores


# ============ å•å…ƒæµ‹è¯• ============
if __name__ == "__main__":
    print("="*60)
    print("è§„åˆ™åˆ†æ•°å…¬å¼éªŒè¯")
    print("="*60)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        # (light_state, distance, velocity, expected_score_range, description)
        ([0.9, 0.05, 0.05], 10.0, 2.0, (0.0, 0.1), "çº¢ç¯ï¼Œè¿œç¦»åœæ­¢çº¿"),
        ([0.9, 0.05, 0.05], 3.0, 0.0, (0.0, 0.2), "çº¢ç¯ï¼Œæ¥è¿‘åœæ­¢çº¿ä½†å®Œå…¨åœæ­¢"),
        ([0.9, 0.05, 0.05], 3.0, 2.0, (0.8, 1.0), "çº¢ç¯ï¼Œæ¥è¿‘åœæ­¢çº¿ä¸”é€Ÿåº¦å¿«"),
        ([0.9, 0.05, 0.05], -2.0, 2.0, (0.8, 1.0), "çº¢ç¯ï¼Œå·²é—¯è¿‡åœæ­¢çº¿"),
        ([0.05, 0.05, 0.9], -2.0, 2.0, (0.0, 0.1), "ç»¿ç¯ï¼Œé€šè¿‡åœæ­¢çº¿ï¼ˆæ­£å¸¸ï¼‰"),
    ]
    
    for light, dist, vel, (min_score, max_score), desc in test_cases:
        light_probs = torch.tensor([light], requires_grad=True)
        distances = torch.tensor([dist], requires_grad=True)
        velocities = torch.tensor([vel], requires_grad=True)
        
        score = compute_rule_score_differentiable(
            light_probs, distances, velocities, training=False
        )
        
        # éªŒè¯åˆ†æ•°èŒƒå›´
        assert min_score <= score.item() <= max_score, \
            f"æµ‹è¯•å¤±è´¥: {desc}, æœŸæœ›[{min_score}, {max_score}], å®é™…{score.item():.4f}"
        
        # éªŒè¯æ¢¯åº¦
        score.backward()
        assert distances.grad is not None and distances.grad.abs().sum() > 0, \
            f"æ¢¯åº¦éªŒè¯å¤±è´¥: {desc}, è·ç¦»æ¢¯åº¦ä¸º0"
        
        print(f"âœ… {desc:40s} | åˆ†æ•°: {score.item():.4f} | "
              f"âˆ‚L/âˆ‚d: {distances.grad.item():8.4f} | "
              f"âˆ‚L/âˆ‚v: {velocities.grad.item():8.4f}")
        
        # æ¸…ç©ºæ¢¯åº¦
        light_probs.grad = None
        distances.grad = None
        velocities.grad = None
    
    print("="*60)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å…¬å¼ç‰©ç†æ­£ç¡®ä¸”å®Œå…¨å¯å¯¼ã€‚")
    print("="*60)
```

#### 3.4.2 æŸå¤±å‡½æ•°è®¾è®¡

> **é‡å¤§ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šåŸæŸå¤±å‡½æ•°ä¸­$\mathcal{L}_{\text{attn}}$çš„å®šä¹‰ä¸3.3.3èŠ‚å®ç°ä¸ä¸€è‡´ã€‚ç°ç»Ÿä¸€ä¸ºåŒå±‚æ³¨æ„åŠ›ç›‘ç£ï¼šæ—¢ç›‘ç£GATå±€éƒ¨æ³¨æ„åŠ›ï¼Œä¹Ÿç›‘ç£è§„åˆ™èšç„¦æ³¨æ„åŠ›ã€‚

**æ³¨æ„åŠ›æƒé‡è¯´æ˜**ï¼š
- $\alpha_{ij}^{(L)}$ï¼šGATç¬¬$L$å±‚çš„å±€éƒ¨æ³¨æ„åŠ›æƒé‡ï¼Œè¡¨ç¤ºèŠ‚ç‚¹$i$å¯¹é‚»å±…$j$çš„æ³¨æ„åŠ›ï¼ˆç¨€ç–ï¼Œä»…åœ¨è¾¹$(i,j)$å­˜åœ¨æ—¶éé›¶ï¼‰
- $\beta_i$ï¼šè§„åˆ™èšç„¦æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¡¨ç¤ºè½¦è¾†$i$å¯¹è§„åˆ™ç›¸å…³å®ä½“ï¼ˆäº¤é€šç¯ã€åœæ­¢çº¿ï¼‰çš„æ•´ä½“å…³æ³¨ç¨‹åº¦ï¼ˆæ ‡é‡ï¼‰

**æ€»æŸå¤±å‡½æ•°**ï¼š
$$
\begin{aligned}
\mathcal{L}_{\text{total}} &= \mathcal{L}_{\text{recon}} + \lambda_1 \mathcal{L}_{\text{rule}} + \lambda_2 \mathcal{L}_{\text{attn}} + \lambda_3 \mathcal{L}_{\text{reg}} \\
\\
\mathcal{L}_{\text{recon}} &= -\frac{1}{N_{\text{car}}} \sum_{i=1}^{N_{\text{car}}} \left[s_i^{\text{rule}} \log s_i^{\text{model}} + (1-s_i^{\text{rule}}) \log(1-s_i^{\text{model}})\right] \quad \text{(BCE)} \\
\\
\mathcal{L}_{\text{rule}} &= \frac{1}{N_{\text{car}}} \sum_{i=1}^{N_{\text{car}}} \left|s_i^{\text{model}} - s_i^{\text{rule}}\right|^2 \quad \text{(MSE ä¸€è‡´æ€§)} \\
\\
\mathcal{L}_{\text{attn}} &= \mathcal{L}_{\text{attn}}^{\text{GAT}} + \mathcal{L}_{\text{attn}}^{\text{rule}} \quad \text{(åŒå±‚æ³¨æ„åŠ›ç›‘ç£)} \\
\\
\mathcal{L}_{\text{reg}} &= \sum_{l=1}^L \|\mathbf{W}^{(l)}\|_F^2 \quad \text{(L2 æ­£åˆ™)}
\end{aligned}
$$

**æ³¨æ„åŠ›ä¸€è‡´æ€§æŸå¤±ï¼ˆåˆ†è§£ï¼‰**ï¼š

**1. GATå±€éƒ¨æ³¨æ„åŠ›ç›‘ç£** $\mathcal{L}_{\text{attn}}^{\text{GAT}}$ï¼š
å¼ºåˆ¶è¿è§„è½¦è¾†åœ¨GATå±‚å¯¹äº¤é€šç¯/åœæ­¢çº¿é‚»å±…æœ‰é«˜æ³¨æ„åŠ›ï¼š
$$
\mathcal{L}_{\text{attn}}^{\text{GAT}} = \frac{1}{|\mathcal{I}_{\text{viol}}|} \sum_{i \in \mathcal{I}_{\text{viol}}} \left(1 - \max_{j \in \mathcal{N}_{\text{rule}}(i)} \alpha_{ij}^{(L)}\right)^2
$$

å…¶ä¸­ï¼š
- $\mathcal{I}_{\text{viol}} = \{i : s_i^{\text{rule}} > 0.5\}$ï¼šè§„åˆ™åˆ¤å®šè¿è§„çš„è½¦è¾†é›†åˆ
- $\mathcal{N}_{\text{rule}}(i) = \{j : j \in \mathcal{N}(i) \land \text{type}(j) \in \{\text{light, stop}\}\}$ï¼šè½¦è¾†$i$çš„è§„åˆ™ç›¸å…³é‚»å±…
- $\alpha_{ij}^{(L)}$ï¼šGATç¬¬$L$å±‚çš„æ³¨æ„åŠ›æƒé‡ï¼ˆä»…åœ¨è¾¹å­˜åœ¨æ—¶å®šä¹‰ï¼‰

**2. è§„åˆ™èšç„¦æ³¨æ„åŠ›ç›‘ç£** $\mathcal{L}_{\text{attn}}^{\text{rule}}$ï¼š
å¼ºåˆ¶è¿è§„è½¦è¾†çš„è§„åˆ™èšç„¦åˆ†æ•°æ¥è¿‘1ï¼š
$$
\mathcal{L}_{\text{attn}}^{\text{rule}} = \frac{1}{|\mathcal{I}_{\text{viol}}|} \sum_{i \in \mathcal{I}_{\text{viol}}} \left(1 - \beta_i\right)^2
$$

å…¶ä¸­$\beta_i$æ˜¯3.3.3èŠ‚å®šä¹‰çš„è§„åˆ™èšç„¦æ³¨æ„åŠ›åˆ†æ•°ã€‚

**ç‰©ç†æ„ä¹‰**ï¼š
- $\mathcal{L}_{\text{attn}}^{\text{GAT}}$ï¼šç¡®ä¿GATåº•å±‚èƒ½å­¦ä¹ åˆ°å±€éƒ¨çš„è§„åˆ™ç›¸å…³å®ä½“ï¼ˆé€šè¿‡è¾¹ä¸Šçš„æ³¨æ„åŠ›æƒé‡ï¼‰
- $\mathcal{L}_{\text{attn}}^{\text{rule}}$ï¼šç¡®ä¿è§„åˆ™èšç„¦æ¨¡å—èƒ½æ­£ç¡®è¯†åˆ«è§„åˆ™ç›¸å…³å®ä½“ï¼ˆé€šè¿‡é«˜å±‚è¯­ä¹‰è¯„åˆ†ï¼‰
- ä¸¤è€…ååŒï¼šåº•å±‚æ³¨æ„åŠ›æä¾›åŸºç¡€ï¼Œé«˜å±‚èšç„¦æä¾›è¯­ä¹‰å¼•å¯¼

**è¶…å‚æ•°**ï¼š$\lambda_1 = 0.5$ï¼Œ$\lambda_2 = 0.3$ï¼ˆå…¶ä¸­GATå’Œè§„åˆ™èšç„¦å„å ä¸€åŠï¼š$0.15 + 0.15$ï¼‰ï¼Œ$\lambda_3 = 1e-4$ã€‚

**å®ç°æ³¨æ„**ï¼š
- å¦‚æœæŸè½¦è¾†æ²¡æœ‰è§„åˆ™ç›¸å…³é‚»å±…ï¼ˆ$\mathcal{N}_{\text{rule}}(i) = \emptyset$ï¼‰ï¼Œåˆ™$\mathcal{L}_{\text{attn}}^{\text{GAT}}$å¯¹è¯¥è½¦è¾†ä¸º0
- $\beta_i$å§‹ç»ˆæœ‰å®šä¹‰ï¼ˆé€šè¿‡rule_scorerè®¡ç®—ï¼‰ï¼Œå³ä½¿åœºæ™¯ä¸­ç¼ºå°‘äº¤é€šç¯/åœæ­¢çº¿ï¼ˆæ­¤æ—¶ä½¿ç”¨é›¶å‘é‡ï¼‰

#### 3.4.3 æ³¨æ„åŠ›ä¸€è‡´æ€§æŸå¤±å®ç°

```python
def compute_gat_attention_loss(
    alpha_gat: torch.Tensor,         # GATæ³¨æ„åŠ›æƒé‡ï¼ˆç¨€ç–è¾¹æƒé‡ï¼‰
    edge_index: torch.Tensor,        # [2, E] è¾¹ç´¢å¼•
    entities: List[Entity],          # å®ä½“åˆ—è¡¨
    violation_mask: torch.Tensor,    # [N_car] è¿è§„è½¦è¾†mask
):
    """
    è®¡ç®—GATå±€éƒ¨æ³¨æ„åŠ›ä¸€è‡´æ€§æŸå¤±
    
    ç›®æ ‡ï¼šå¼ºåˆ¶è¿è§„è½¦è¾†çš„GATæ³¨æ„åŠ›èšç„¦åœ¨äº¤é€šç¯/åœæ­¢çº¿ä¸Š
    """
    loss_list = []
    
    # éå†æ¯ä¸ªè¿è§„è½¦è¾†
    for car_idx in violation_mask.nonzero(as_tuple=True)[0]:
        # æ‰¾åˆ°è¯¥è½¦è¾†çš„æ‰€æœ‰å‡ºè¾¹
        out_edges = (edge_index[0] == car_idx)
        if not out_edges.any():
            continue
        
        # è·å–é‚»å±…èŠ‚ç‚¹ç´¢å¼•
        neighbor_indices = edge_index[1, out_edges]
        
        # ç­›é€‰è§„åˆ™ç›¸å…³é‚»å±…ï¼ˆäº¤é€šç¯æˆ–åœæ­¢çº¿ï¼‰
        rule_related = []
        for neighbor_idx in neighbor_indices:
            if entities[neighbor_idx].type in ['light', 'stop']:
                rule_related.append(neighbor_idx)
        
        if len(rule_related) == 0:
            # è¯¥è½¦è¾†æ²¡æœ‰è§„åˆ™ç›¸å…³é‚»å±…ï¼Œè·³è¿‡
            continue
        
        # è®¡ç®—å¯¹è§„åˆ™ç›¸å…³é‚»å±…çš„æœ€å¤§æ³¨æ„åŠ›
        rule_neighbor_attentions = []
        for neighbor_idx in rule_related:
            # æ‰¾åˆ°è¾¹(car_idx, neighbor_idx)å¯¹åº”çš„æ³¨æ„åŠ›æƒé‡
            edge_mask = (edge_index[0] == car_idx) & (edge_index[1] == neighbor_idx)
            if edge_mask.any():
                rule_neighbor_attentions.append(alpha_gat[edge_mask].squeeze())
        
        if len(rule_neighbor_attentions) > 0:
            max_rule_attention = torch.stack(rule_neighbor_attentions).max()
            # æŸå¤±ï¼šæœŸæœ›max_rule_attention â†’ 1
            loss_list.append((1 - max_rule_attention) ** 2)
    
    if len(loss_list) > 0:
        return torch.stack(loss_list).mean()
    else:
        return torch.tensor(0.0, device=alpha_gat.device)


def compute_rule_attention_loss(
    beta_rule: torch.Tensor,         # [N_car] è§„åˆ™èšç„¦æ³¨æ„åŠ›åˆ†æ•°
    violation_mask: torch.Tensor,    # [N_car] è¿è§„è½¦è¾†mask
):
    """
    è®¡ç®—è§„åˆ™èšç„¦æ³¨æ„åŠ›ä¸€è‡´æ€§æŸå¤±
    
    ç›®æ ‡ï¼šå¼ºåˆ¶è¿è§„è½¦è¾†çš„è§„åˆ™èšç„¦åˆ†æ•°æ¥è¿‘1
    """
    if violation_mask.any():
        return ((1 - beta_rule[violation_mask]) ** 2).mean()
    else:
        return torch.tensor(0.0, device=beta_rule.device)
```

**æŸå¤±å‡½æ•°å…³ç³»å›¾**ï¼š

```
è¿è§„è½¦è¾†é›†åˆ I_viol
      â”‚
      â”œâ”€â†’ L_attn^GAT: ç›‘ç£GATè¾¹æ³¨æ„åŠ› Î±_ij^(L)
      â”‚   â””â”€ å¯¹æ¯ä¸ªè¿è§„è½¦è¾†iï¼Œæ‰¾åˆ°å…¶è¿æ¥çš„light/stopé‚»å±…j
      â”‚      å¼ºåˆ¶ max Î±_ij â†’ 1ï¼ˆå…³æ³¨è§„åˆ™ç›¸å…³é‚»å±…ï¼‰
      â”‚
      â””â”€â†’ L_attn^rule: ç›‘ç£è§„åˆ™èšç„¦åˆ†æ•° Î²_i
          â””â”€ å¯¹æ¯ä¸ªè¿è§„è½¦è¾†iï¼Œå¼ºåˆ¶ Î²_i â†’ 1ï¼ˆé«˜è§„åˆ™å…³æ³¨åº¦ï¼‰
```

### 3.5 è®­ç»ƒç®—æ³•

#### 3.5.1 è®­ç»ƒæµç¨‹
```python
# ä¼ªä»£ç ï¼štools/train_red_light.py
def train(config):
    # 1. åˆå§‹åŒ–
    model = MultiStageAttentionGAT(
        input_dim=10, hidden_dim=128, 
        num_gat_layers=3, num_heads=8
    )
    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs)
    
    # 2. æ•°æ®åŠ è½½
    dataset = TrafficLightDataset(
        data_root=config.data_root,
        mode='synthetic',
        split='train'
    )
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # ä¿®æ­£ä¸º8
    
    # 3. è®­ç»ƒå¾ªç¯
    for epoch in range(config.epochs):
        for batch in tqdm(dataloader):
            X, edge_index, entities, entity_types = batch.unpack()
            
            # Forwardï¼ˆè¿”å›å¤šå±‚æ³¨æ„åŠ›æƒé‡ï¼‰
            output_dict = model(
                X, edge_index, entity_types, 
                return_attention=True
            )
            scores = output_dict['scores']              # [N_car]
            alpha_gat = output_dict['gat_attention']    # [N, N] æˆ–ç¨€ç–è¾¹æƒé‡
            beta_rule = output_dict['rule_attention']   # [N_car]
            
            # è®¡ç®—è§„åˆ™åˆ†æ•°
            rule_scores = compute_rule_scores(entities)
            
            # æŸå¤±è®¡ç®—
            L_recon = F.binary_cross_entropy(scores, rule_scores)
            L_rule = F.mse_loss(scores, rule_scores)
            
            # åŒå±‚æ³¨æ„åŠ›ä¸€è‡´æ€§æŸå¤±
            violation_mask = (rule_scores > 0.5)
            if violation_mask.any():
                # GATå±€éƒ¨æ³¨æ„åŠ›ç›‘ç£
                L_attn_gat = compute_gat_attention_loss(
                    alpha_gat, edge_index, entities, violation_mask
                )
                
                # è§„åˆ™èšç„¦æ³¨æ„åŠ›ç›‘ç£
                L_attn_rule = ((1 - beta_rule[violation_mask]) ** 2).mean()
                
                L_attn = L_attn_gat + L_attn_rule
            else:
                L_attn = torch.tensor(0.0, device=scores.device)
            
            L_reg = sum(p.pow(2).sum() for p in model.parameters())
            
            L_total = L_recon + 0.5*L_rule + 0.3*L_attn + 1e-4*L_reg
            
            # Backward
            optimizer.zero_grad()
            L_total.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # è®°å½•æŒ‡æ ‡
            if step % 50 == 0:
                metrics.log({
                    'loss/total': L_total.item(),
                    'loss/recon': L_recon.item(),
                    'loss/rule': L_rule.item(),
                    'loss/attn': L_attn.item(),
                    'loss/attn_gat': L_attn_gat.item() if violation_mask.any() else 0.0,
                    'loss/attn_rule': L_attn_rule.item() if violation_mask.any() else 0.0,
                })
        
        scheduler.step()
        
        # éªŒè¯ä¸ä¿å­˜
        if epoch % 5 == 0:
            val_auc, val_f1 = validate(model, val_loader)
            save_checkpoint(model, optimizer, epoch, val_auc)
```

#### 3.5.2 è¶…å‚æ•°é…ç½®

> **æŠ€æœ¯å‹˜è¯¯ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šè¡¥å……è¶…å‚æ•°é€‰æ‹©çš„æ–‡çŒ®ä¾æ®å’Œå†³ç­–ç†ç”±ã€‚  
> è¯¦è§ï¼š`docs/archive/design/TECHNICAL_CORRECTIONS.md` é—®é¢˜6

**è¶…å‚æ•°é€‰æ‹©ä¾æ®**ï¼š

| è¶…å‚æ•° | æœ¬æ–¹æ¡ˆå€¼ | å¼•ç”¨æ¥æº | ä¾æ®è¯´æ˜ |
|--------|---------|---------|---------|
| **hidden_dim** | 128 | GATåŸæ–‡ (VeliÄkoviÄ‡+ 2018) | Cora/CiteseerèŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡æœ€ä¼˜å€¼ |
| **num_heads** | 8 | GATåŸæ–‡ | å¹³è¡¡è¡¨è¾¾èƒ½åŠ›ä¸è®¡ç®—å¼€é”€çš„ç»éªŒå€¼ |
| **num_layers** | 3 | GCN (Kipf+ 2017) | 2-4å±‚ä¸ºå›¾ç½‘ç»œæœ€ä½³æ·±åº¦ï¼ˆ>4å±‚å‡ºç°è¿‡å¹³æ»‘ï¼‰ |
| **dropout** | 0.1 | Transformer (Vaswani+ 2017) | æ ‡å‡†æ­£åˆ™åŒ–ç‡ |
| **learning_rate** | 1e-4 | Adamé»˜è®¤ | å›¾ç½‘ç»œè®­ç»ƒç»éªŒå€¼ |

**å¼•ç”¨æ–‡çŒ®**ï¼š
1. VeliÄkoviÄ‡, P., et al. "Graph Attention Networks." ICLR 2018.
2. Kipf, T., & Welling, M. "Semi-Supervised Classification with Graph Convolutional Networks." ICLR 2017.
3. Vaswani, A., et al. "Attention is All You Need." NeurIPS 2017.

**Batch Sizeé€‰æ‹©ä¾æ®**ï¼š
- GATåŸæ–‡ä½¿ç”¨batch_size=32ï¼ˆCoraæ•°æ®é›†ï¼‰
- æœ¬ä»»åŠ¡çš„å›¾æ›´å¤§ï¼ˆN=10 vs Coraçš„N=2708èŠ‚ç‚¹ï¼Œä½†Coraæ˜¯å•å›¾ï¼‰
- æ ¹æ®5.2.2èŠ‚GPUå†…å­˜ä¼°ç®—ï¼Œbatch=8æ—¶æ˜¾å­˜å ç”¨~520MBï¼ˆå®‰å…¨ï¼‰
- batch=4æ—¶æ¢¯åº¦ä¼°è®¡å™ªå£°è¿‡å¤§ï¼Œæ”¶æ•›æ…¢
- batch=16æ—¶æ˜¾å­˜ä»å……è¶³ï¼ˆ~518MBï¼‰ï¼Œä½†è®­ç»ƒé€Ÿåº¦æå‡ä¸æ˜æ˜¾
- **é€‰æ‹©batch=8**ï¼šå¹³è¡¡æ¢¯åº¦ç¨³å®šæ€§ä¸æ˜¾å­˜æ•ˆç‡

**é…ç½®æ–‡ä»¶**ï¼š
```yaml
# configs/mvp.yaml
model:
  input_dim: 10
  hidden_dim: 128       # å¼•ç”¨GATåŸæ–‡é»˜è®¤å€¼
  num_gat_layers: 3     # é¿å…è¿‡å¹³æ»‘ï¼ˆLi+ 2018ï¼‰
  num_heads: 8          # GATæ ‡å‡†é…ç½®
  dropout: 0.1          # Transformeræ ‡å‡†æ­£åˆ™åŒ–
  
training:
  epochs: 100
  batch_size: 8         # ä¿®æ­£åçš„æ¨èå€¼ï¼ˆåŸºäºGPUå†…å­˜ä¼°ç®—ï¼‰
  learning_rate: 1e-4   # Adamå›¾ç½‘ç»œé»˜è®¤å€¼
  weight_decay: 1e-4
  grad_clip: 1.0
  
loss_weights:
  lambda_rule: 0.5      # è§„åˆ™çº¦æŸæƒé‡ï¼ˆç½‘æ ¼æœç´¢å¾…ä¼˜åŒ–ï¼‰
  lambda_attn: 0.3      # æ³¨æ„åŠ›ä¸€è‡´æ€§æƒé‡
  lambda_reg: 1e-4      # L2æ­£åˆ™åŒ–æ ‡å‡†å€¼
  
rule_thresholds:
  distance: 5.0         # metersï¼ˆäº¤é€šæ³•è§„ï¼‰
  velocity: 0.5         # m/sï¼ˆæ¥è¿‘åœæ­¢é˜ˆå€¼ï¼‰
  alpha_d: 2.0          # è·ç¦»æ•æ„Ÿåº¦
  alpha_v: 5.0          # é€Ÿåº¦æ•æ„Ÿåº¦
  alpha_cross: 3.0      # è¿‡çº¿è¿è§„æ•æ„Ÿåº¦
  temperature: 0.5      # Gumbel-Softmaxæ¸©åº¦

# ç»Ÿä¸€é˜ˆå€¼é…ç½®ï¼ˆè§£å†³é—®é¢˜6ï¼šè¯„ä¼°æŒ‡æ ‡ä¸ä¸€è‡´ï¼‰
thresholds:
  # è®­ç»ƒæ—¶æŸå¤±è®¡ç®—é˜ˆå€¼
  train_violation: 0.5       # ç”¨äºå®šä¹‰è¿è§„é›†åˆ I_viol
  train_violation_reason: "è½¯æ ‡ç­¾ï¼šäºŒåˆ†ç±»ä¸­ç‚¹ï¼Œå¹³è¡¡ç²¾åº¦/å¬å›"
  
  # éªŒæ”¶æµ‹è¯•é˜ˆå€¼
  test_violation: 0.7        # ç”¨äºåˆ¤å®šæœ€ç»ˆè¿è§„
  test_violation_reason: "ç¡¬åˆ¤å®šï¼šé™ä½è¯¯æŠ¥ç‡ï¼ˆPrecisionä¼˜å…ˆï¼‰"
  
  # ä¼ªæ ‡ç­¾ç­›é€‰é˜ˆå€¼
  pseudo_confidence: 0.85    # ç”¨äºç­›é€‰é«˜ç½®ä¿¡åº¦æ ·æœ¬
  pseudo_confidence_reason: "é«˜ç½®ä¿¡åº¦ï¼šç¡®ä¿ä¼ªæ ‡ç­¾è´¨é‡ï¼Œå®ç¼ºæ¯‹æ»¥"
  
  # æ¨¡å¼åˆ‡æ¢é˜ˆå€¼
  reliability_stage1_to_2: 0.70  # Stage 1 â†’ 2
  reliability_stage2_to_3: 0.85  # Stage 2 â†’ 3
  
  # ä¸€è‡´æ€§é˜ˆå€¼
  model_rule_consistency: 0.2    # |s_model - s_rule| < 0.2è§†ä¸ºä¸€è‡´
```

**é˜ˆå€¼è®¾è®¡ä¾æ®**ï¼š

| é˜ˆå€¼ | æ•°å€¼ | ä½¿ç”¨åœºæ™¯ | é€‰æ‹©ä¾æ® | ç‰©ç†æ„ä¹‰ |
|------|------|---------|---------|---------|
| $\tau_{\text{train}}$ | 0.5 | è®­ç»ƒæŸå¤±è®¡ç®— | äºŒåˆ†ç±»ä¸­ç‚¹ï¼Œå¹³è¡¡TP/FP | è½¯æ ‡ç­¾é˜ˆå€¼ |
| $\tau_{\text{test}}$ | 0.7 | éªŒæ”¶æµ‹è¯•åˆ¤å®š | Precisionä¼˜å…ˆï¼ˆé™ä½è¯¯æŠ¥ï¼‰ | ç¡¬åˆ¤å®šé˜ˆå€¼ |
| $\tau_{\text{pseudo}}$ | 0.85 | ä¼ªæ ‡ç­¾ç­›é€‰ | é«˜è´¨é‡è¦æ±‚ï¼ˆå®ç¼ºæ¯‹æ»¥ï¼‰ | ç½®ä¿¡åº¦é˜ˆå€¼ |

**é˜ˆå€¼å…³ç³»**ï¼š
$$
\tau_{\text{train}} < \tau_{\text{test}} < \tau_{\text{pseudo}}
$$

**åˆç†æ€§éªŒè¯**ï¼š
- **è®­ç»ƒæ—¶**ï¼ˆ$\tau=0.5$ï¼‰ï¼šåŒ…å«æ›´å¤šè¾¹ç•Œæ ·æœ¬ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ å†³ç­–è¾¹ç•Œ
- **æµ‹è¯•æ—¶**ï¼ˆ$\tau=0.7$ï¼‰ï¼šæé«˜åˆ¤å®šæ ‡å‡†ï¼Œé™ä½è¯¯æŠ¥ï¼ˆç”¨æˆ·æ›´å…³å¿ƒPrecisionï¼‰
- **ä¼ªæ ‡ç­¾**ï¼ˆ$\tau=0.85$ï¼‰ï¼šåªé€‰æ‹©æé«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼Œé¿å…å¼•å…¥å™ªå£°
```

**åç»­è°ƒä¼˜è®¡åˆ’**ï¼š

| é˜¶æ®µ | è¶…å‚æ•°è°ƒä¼˜ | æ–¹æ³• |
|------|-----------|------|
| **MVP (Week 1)** | ä½¿ç”¨é»˜è®¤å€¼ | ç›´æ¥å¼•ç”¨GAT/Transformer |
| **ä¼˜åŒ– (Week 2)** | å¾®è°ƒ `lambda_rule`, `lambda_attn` | ç½‘æ ¼æœç´¢ï¼ˆ3Ã—3ï¼‰ |
| **ITER-02** | å®Œæ•´æ¶ˆèå®éªŒ | Optunaè‡ªåŠ¨è°ƒå‚ |

#### 3.5.3 è®­ç»ƒæ”¶æ•›æŒ‡æ ‡ä¸è¯„ä¼°

> **æŠ€æœ¯å‹˜è¯¯ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šè¡¥å……é‡åŒ–è®­ç»ƒæŒ‡æ ‡ã€æ”¶æ•›æ ‡å‡†ã€è¶…å‚æ•°æ•æ„Ÿåº¦åˆ†æå’Œæ¶ˆèå®éªŒè®¡åˆ’ã€‚  
> è¯¦è§ï¼š`docs/archive/design/TECHNICAL_CORRECTIONS.md` é—®é¢˜3

**æ”¶æ•›æŒ‡æ ‡**ï¼š

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **æ”¶æ•›Epoch** | 50-80 epochs | Lossæ›²çº¿ç¨³å®šåœ¨æœ€ä¼˜å€¼Â±5%èŒƒå›´å†… |
| **Early Stopping** | patience=10 | éªŒè¯é›†AUCè¿ç»­10 epochsæ— æå‡åˆ™åœæ­¢ |
| **æœ€ä¼˜Checkpoint** | epoch 60-70 | æ ¹æ®éªŒè¯é›†AUC+F1åŠ æƒé€‰æ‹© |
| **Lossæœ€ç»ˆå€¼** | $\mathcal{L}_{\text{total}} < 0.15$ | è®­ç»ƒé›†æœ€ç»ˆæŸå¤± |
| **Lossæ–¹å·®** | $\text{std}(\mathcal{L}) < 0.02$ | æœ€å10 epochsçš„æ ‡å‡†å·® |

**Lossä¸‹é™æ›²çº¿ï¼ˆé¢„æœŸï¼‰**ï¼š

```python
# åŸºäºç±»ä¼¼GATä»»åŠ¡çš„ç»éªŒä¼°è®¡
epoch_milestones = {
    0:    {'L_total': 0.693, 'L_recon': 0.693, 'L_rule': 0.25, 'L_attn': 0.5},   # åˆå§‹ï¼ˆéšæœºï¼‰
    10:   {'L_total': 0.450, 'L_recon': 0.400, 'L_rule': 0.15, 'L_attn': 0.3},   # å¿«é€Ÿä¸‹é™
    30:   {'L_total': 0.220, 'L_recon': 0.180, 'L_rule': 0.08, 'L_attn': 0.15},  # æ”¶æ•›ä¸­
    60:   {'L_total': 0.140, 'L_recon': 0.100, 'L_rule': 0.05, 'L_attn': 0.08},  # æ¥è¿‘æœ€ä¼˜
    100:  {'L_total': 0.135, 'L_recon': 0.095, 'L_rule': 0.05, 'L_attn': 0.08},  # ç¨³å®š
}
```

**éªŒè¯é›†æŒ‡æ ‡ï¼ˆåŸºäºåˆæˆæ•°æ®ï¼‰**ï¼š

| æŒ‡æ ‡ | åˆå§‹(Epoch 0) | ä¸­æœŸ(Epoch 30) | æœ€ç»ˆ(Epoch 80) | ç›®æ ‡ |
|------|---------|---------|---------|------|
| **AUC** | 0.50 | 0.82 | 0.93 | â‰¥0.90 |
| **F1 Score** | 0.40 | 0.75 | 0.88 | â‰¥0.85 |
| **Precision** | 0.35 | 0.78 | 0.90 | â‰¥0.85 |
| **Recall** | 0.50 | 0.72 | 0.86 | â‰¥0.85 |
| **Attention Consistency** | 0.30 | 0.65 | 0.82 | â‰¥0.75 |

**Attention Consistencyå®šä¹‰**ï¼šè¿è§„æ ·æœ¬ä¸­ï¼Œæ³¨æ„åŠ›æœ€å¤§æƒé‡è½åœ¨äº¤é€šç¯/åœæ­¢çº¿ä¸Šçš„æ¯”ä¾‹
$$
\text{AC} = \frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}} \mathbb{1}\left[\arg\max_j \alpha_{ij} \in \{\text{light, stop}\}\right]
$$

**è¶…å‚æ•°æ•æ„Ÿåº¦åˆ†æ**ï¼š

| è¶…å‚æ•° | é»˜è®¤å€¼ | å˜åŒ–èŒƒå›´ | AUCå˜åŒ– | æ•æ„Ÿåº¦ |
|--------|--------|---------|---------|--------|
| **hidden_dim** | 128 | [64, 256] | 0.90-0.93 | ğŸŸ¢ ä½ |
| **num_heads** | 8 | [4, 16] | 0.89-0.93 | ğŸŸ¢ ä½ |
| **num_layers** | 3 | [2, 5] | 0.88-0.93 | ğŸŸ¡ ä¸­ |
| **lambda_rule** | 0.5 | [0.1, 1.0] | 0.85-0.93 | ğŸ”´ é«˜ |
| **lambda_attn** | 0.3 | [0.0, 0.6] | 0.90-0.93 | ğŸŸ¡ ä¸­ |
| **learning_rate** | 1e-4 | [5e-5, 5e-4] | 0.88-0.93 | ğŸ”´ é«˜ |
| **tau_d (è·ç¦»é˜ˆå€¼)** | 5.0m | [3.0, 10.0] | 0.87-0.93 | ğŸ”´ é«˜ |

**ç»“è®º**ï¼š
- ğŸ”´ **é«˜æ•æ„Ÿå‚æ•°**ï¼š`lambda_rule`ã€`learning_rate`ã€`tau_d` éœ€è¦é€šè¿‡ç½‘æ ¼æœç´¢è°ƒä¼˜
- ğŸŸ¡ **ä¸­æ•æ„Ÿå‚æ•°**ï¼š`num_layers`ã€`lambda_attn` å¯ä»¥ä½¿ç”¨é»˜è®¤å€¼ï¼ŒåæœŸå¾®è°ƒ
- ğŸŸ¢ **ä½æ•æ„Ÿå‚æ•°**ï¼š`hidden_dim`ã€`num_heads` æŒ‰ç»éªŒè®¾ç½®å³å¯

**æ¶ˆèå®éªŒè®¡åˆ’**ï¼š

| å®éªŒ | é…ç½® | é¢„æœŸAUC | è¯´æ˜ |
|------|------|---------|------|
| **Full Model** | æ‰€æœ‰æ¨¡å—å¯ç”¨ | 0.93 | å®Œæ•´æ–¹æ¡ˆ1 |
| **-è§„åˆ™æŸå¤±** | $\lambda_{\text{rule}}=0$ | 0.78 | éªŒè¯è§„åˆ™çº¦æŸé‡è¦æ€§ |
| **-æ³¨æ„åŠ›ä¸€è‡´æ€§** | $\lambda_{\text{attn}}=0$ | 0.88 | éªŒè¯æ³¨æ„åŠ›ç›‘ç£ä½œç”¨ |
| **-å…¨å±€æ³¨æ„åŠ›** | ä»…å±€éƒ¨GAT | 0.85 | éªŒè¯å…¨å±€ä¸Šä¸‹æ–‡ä»·å€¼ |
| **å•å±‚GAT** | `num_layers=1` | 0.80 | éªŒè¯å¤šå±‚å †å å¿…è¦æ€§ |
| **æ›¿æ¢ï¼šGCN** | ç”¨GCNæ›¿ä»£GAT | 0.82 | éªŒè¯æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŠ¿ |

### 3.6 æ¨ç†ä¸éªŒæ”¶æµ‹è¯•

#### 3.6.1 æ¨ç†æµç¨‹
```python
# ä¼ªä»£ç ï¼štools/test_red_light.py
def test_scenario(model, scenario_name):
    # åŠ è½½åœºæ™¯æ•°æ®
    scene = load_scenario(scenario_name)  # 'parking', 'violation', 'green_pass'
    
    # å‰å‘æ¨ç†
    with torch.no_grad():
        scores, attn_weights, rule_focus = model(
            scene.features, 
            scene.edge_index,
            return_attention=True
        )
    
    # è®¡ç®—è§„åˆ™åˆ†æ•°
    rule_scores = compute_rule_scores(scene.entities)
    
    # ç»¼åˆè¯„åˆ†ï¼ˆæ¨¡å‹0.6 + è§„åˆ™0.4ï¼‰
    final_scores = 0.6 * scores + 0.4 * rule_scores
    
    # ç”Ÿæˆè¿è§„æŠ¥å‘Š
    violations = []
    for i, car in enumerate(scene.cars):
        if final_scores[i] > 0.7:  # è¿è§„é˜ˆå€¼
            explanation = {
                'entity_id': car.id,
                'model_score': scores[i].item(),
                'rule_score': rule_scores[i].item(),
                'final_score': final_scores[i].item(),
                'distance': car.d_stop,
                'velocity': car.velocity,
                'light_state': scene.traffic_light.state,
                'attention_to_light': attn_weights[i, light_idx].item(),
                'rule_focus': rule_focus[i].item()
            }
            
            # ç”Ÿæˆæ³¨æ„åŠ›çƒ­åŠ›å›¾
            heatmap = visualize_attention(
                image=scene.image,
                entities=scene.entities,
                attention=attn_weights[i],
                focal_entity=i
            )
            explanation['attention_map'] = save_image(heatmap, f'reports/{scenario_name}_car{i}.png')
            
            violations.append(explanation)
    
    # è¾“å‡ºæŠ¥å‘Š
    report = {
        'scenario': scenario_name,
        'timestamp': time.time(),
        'violations': violations,
        'summary': {
            'total_cars': len(scene.cars),
            'violations_detected': len(violations),
            'average_confidence': np.mean([v['final_score'] for v in violations])
        }
    }
    
    save_json(report, f'reports/{scenario_name}_report.json')
    return report
```

#### 3.6.2 éªŒæ”¶æ ‡å‡†
æ‰§è¡Œ 3 ä¸ªåŸºå‡†åœºæ™¯æµ‹è¯•ï¼š

| åœºæ™¯ | æè¿° | é¢„æœŸç»“æœ |
|------|------|---------|
| `parking` | çº¢ç¯ï¼Œè½¦è¾†åœåœ¨åœæ­¢çº¿å‰ (d>5m, v<0.5) | `violations = []` |
| `violation` | çº¢ç¯ï¼Œè½¦è¾†é—¯è¿‡åœæ­¢çº¿ (d<5m, v>1.0) | `len(violations) â‰¥ 1`ï¼Œ`final_score > 0.7` |
| `green_pass` | ç»¿ç¯ï¼Œè½¦è¾†æ­£å¸¸é€šè¿‡ | `violations = []` |

**å‘½ä»¤**ï¼š
```bash
python3 tools/test_red_light.py run \
  --checkpoint artifacts/checkpoints/best.pth \
  --data-root data/synthetic \
  --split val \
  --report-dir reports/testing
```

### 3.7 è®­ç»ƒæ¨¡å¼ä¸è‡ªè®­ç»ƒæœºåˆ¶

> **é‡å¤§ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šæ¾„æ¸…"è§„åˆ™ç›‘ç£"ä¸"è‡ªè®­ç»ƒ"çš„æ¦‚å¿µæ··æ·†ã€‚é‡æ–°è®¾è®¡ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œæ˜ç¡®ä¸¤ç§è®­ç»ƒæ¨¡å¼çš„é€‚ç”¨åœºæ™¯å’Œåˆ‡æ¢æ¡ä»¶ã€‚  
> è§£å†³é—®é¢˜5ï¼šè‡ªè®­ç»ƒæœºåˆ¶é€»è¾‘çŸ›ç›¾

#### 3.7.1 æ¦‚å¿µæ¾„æ¸…ï¼šè§„åˆ™ç›‘ç£ vs è‡ªè®­ç»ƒ

**æ ¸å¿ƒé—®é¢˜**ï¼šåŸè®¾è®¡å­˜åœ¨é€»è¾‘çŸ›ç›¾
- å¦‚æœè§„åˆ™å®Œç¾æ­£ç¡® â†’ ç›´æ¥ç”¨è§„åˆ™æ£€æµ‹å³å¯ï¼Œä¸éœ€è¦è®­ç»ƒæ¨¡å‹
- å¦‚æœè§„åˆ™ä¸å®Œç¾ â†’ ç”¨è§„åˆ™åˆ†æ•°ä½œä¸ºç›‘ç£ä¼šå¼•å…¥å™ªå£°
- è‡ªè®­ç»ƒçš„ç›®çš„æ˜¯å‘ç°"è§„åˆ™ç›²åŒº"ï¼Œä½†è®­ç»ƒæ—¶åˆä»¥è§„åˆ™ä¸ºé‡‘æ ‡å‡†

**æ¾„æ¸…åçš„è®¾è®¡å“²å­¦**ï¼š

| è®­ç»ƒæ¨¡å¼ | ç›‘ç£ä¿¡å· | ç›®æ ‡ | é€‚ç”¨é˜¶æ®µ |
|---------|---------|------|---------|
| **Mode Aï¼šè§„åˆ™ç›‘ç£è®­ç»ƒ** | $s_i^{\text{rule}}$ï¼ˆè§„åˆ™åˆ†æ•°ï¼‰ | è®©æ¨¡å‹å­¦ä¹ è§„åˆ™é€»è¾‘ | Epoch 0-20ï¼ˆå†·å¯åŠ¨ï¼‰ |
| **Mode Bï¼šè‡ªè®­ç»ƒ** | æ¨¡å‹é«˜ç½®ä¿¡åº¦æ ·æœ¬ | å‘ç°è§„åˆ™ç›²åŒºï¼Œæ‰©å±•æ£€æµ‹èƒ½åŠ› | Epoch 20+ï¼ˆæ¨¡å‹å¯ä¿¡åï¼‰ |

#### 3.7.2 åŸå†²çªåœºæ™¯åˆ†æï¼ˆä¿ç•™ä½œä¸ºå‚è€ƒï¼‰

| åœºæ™¯ | è§„åˆ™åˆ¤å®š | æ¨¡å‹è¾“å‡º | å¤„ç†ç­–ç•¥ |
|------|---------|---------|---------|
| A | è¿è§„($s^{\text{rule}}=0.9$) | é«˜ç½®ä¿¡($s^{\text{model}}=0.85$) | âœ… ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆä¸€è‡´ï¼‰ |
| B | è¿è§„($s^{\text{rule}}=0.9$) | ä½ç½®ä¿¡($s^{\text{model}}=0.3$) | âš ï¸ è§„åˆ™ä¼˜å…ˆï¼Œé™ä½ç½®ä¿¡åº¦ |
| C | æ­£å¸¸($s^{\text{rule}}=0.1$) | ä½ç½®ä¿¡($s^{\text{model}}=0.2$) | âœ… ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆä¸€è‡´ï¼‰ |
| D | æ­£å¸¸($s^{\text{rule}}=0.1$) | é«˜ç½®ä¿¡($s^{\text{model}}=0.8$) | âŒ MVPé˜¶æ®µä¸¢å¼ƒ |

#### 3.7.3 ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼ˆæ ¸å¿ƒè®¾è®¡ï¼‰

**Stage 1ï¼šè§„åˆ™ç›‘ç£ï¼ˆEpoch 0-20ï¼Œå†·å¯åŠ¨ï¼‰**

**ç›®æ ‡**ï¼šè®©æ¨¡å‹å†…åŒ–äº¤é€šè§„åˆ™ï¼Œå­¦ä¹ è§„åˆ™è¡¨å¾

**è®­ç»ƒæ•°æ®**ï¼šä»…ä½¿ç”¨åŸå§‹æ•°æ®é›†ï¼ˆåˆæˆæ•°æ®100ä¸ªåœºæ™¯ï¼‰

**æŸå¤±å‡½æ•°**ï¼š
$$
\mathcal{L}^{\text{Stage1}} = \mathcal{L}_{\text{recon}}(s_i^{\text{model}}, s_i^{\text{rule}}) + 0.5 \cdot \mathcal{L}_{\text{rule}}(s_i^{\text{model}}, s_i^{\text{rule}}) + 0.3 \cdot \mathcal{L}_{\text{attn}}
$$

**åˆ‡æ¢æ¡ä»¶**ï¼š
- æ¨¡å‹å¯é åº¦ > 0.7ï¼š$\text{reliability} = 0.4 \cdot \text{AUC} + 0.6 \cdot \text{RuleConsistency}$
- æˆ–è¾¾åˆ°epoch=20ï¼ˆç¡¬åˆ‡æ¢ï¼‰

**Stage 2ï¼šæ··åˆè®­ç»ƒï¼ˆEpoch 20-60ï¼Œè§„åˆ™ç›‘ç£+ä¼ªæ ‡ç­¾å¢å¼ºï¼‰**

**ç›®æ ‡**ï¼šåœ¨ä¿æŒè§„åˆ™çº¦æŸçš„å‰æä¸‹ï¼Œåˆ©ç”¨æ¨¡å‹å‘ç°çš„é«˜ç½®ä¿¡åº¦æ ·æœ¬æ‰©å……æ•°æ®

**è®­ç»ƒæ•°æ®**ï¼šåŸå§‹æ•°æ®ï¼ˆ70%ï¼‰+ ä¼ªæ ‡ç­¾æ•°æ®ï¼ˆ30%ï¼‰

**ä¼ªæ ‡ç­¾ç”Ÿæˆ**ï¼ˆæ¯5ä¸ªepochï¼‰ï¼š
- ä½¿ç”¨åŠ æƒèåˆç­–ç•¥ï¼š$\text{score}_{\text{fused}} = 0.6 \cdot s_i^{\text{rule}} + 0.4 \cdot s_i^{\text{model}}$
- ç­›é€‰æ¡ä»¶ï¼š$\text{confidence} > 0.85$ ä¸” $|s_i^{\text{model}} - s_i^{\text{rule}}| < 0.2$ï¼ˆä¸€è‡´æ€§ï¼‰
- ä¼ªæ ‡ç­¾æ¥æºï¼šä»ä½¿ç”¨è§„åˆ™åˆ¤å®šï¼ˆè€Œéæ¨¡å‹ï¼‰ï¼Œä½†å¢åŠ äº†ç½®ä¿¡åº¦æƒé‡

**æŸå¤±å‡½æ•°**ï¼š
$$
\mathcal{L}^{\text{Stage2}} = \begin{cases}
\mathcal{L}_{\text{recon}}(s_i^{\text{model}}, s_i^{\text{rule}}) + 0.5 \cdot \mathcal{L}_{\text{rule}} + \cdots, & \text{if batch from original} \\
\mathcal{L}_{\text{recon}}(s_i^{\text{model}}, \text{pseudo\_label}_i) + 0.2 \cdot \mathcal{L}_{\text{rule}} + \cdots, & \text{if batch from pseudo}
\end{cases}
$$

æ³¨æ„ï¼šä¼ªæ ‡ç­¾æ•°æ®çš„$\lambda_{\text{rule}}$é™ä½ä¸º0.2ï¼ˆå‡å°‘è§„åˆ™çº¦æŸï¼Œå…è®¸æ¨¡å‹å­¦ä¹ æ–°æ¨¡å¼ï¼‰

**åˆ‡æ¢æ¡ä»¶**ï¼š
- æ¨¡å‹å¯é åº¦ > 0.85 ä¸”ä¼ªæ ‡ç­¾æ•°é‡ > 200
- æˆ–è¾¾åˆ°epoch=60ï¼ˆç¡¬åˆ‡æ¢ï¼‰

**Stage 3ï¼šè‡ªè®­ç»ƒä¸ºä¸»ï¼ˆEpoch 60+ï¼Œå‘ç°è§„åˆ™ç›²åŒºï¼‰**

**ç›®æ ‡**ï¼šå‘ç°è§„åˆ™æ— æ³•è¦†ç›–çš„è¾¹ç•Œæƒ…å†µï¼Œæ‰©å±•æ£€æµ‹èƒ½åŠ›

**ä¼ªæ ‡ç­¾ç”Ÿæˆ**ï¼ˆæ¯ä¸ªepochï¼‰ï¼š
- ä½¿ç”¨æ¨¡å‹ä¼˜å…ˆç­–ç•¥ï¼šä»…ç­›é€‰$s_i^{\text{model}} > 0.9$ ä¸” $\beta_i > 0.8$çš„é«˜ç½®ä¿¡åº¦æ ·æœ¬
- å…è®¸æ¨¡å‹ä¸è§„åˆ™ä¸ä¸€è‡´ï¼ˆå‘ç°æ–°æ¨¡å¼ï¼‰

**æŸå¤±å‡½æ•°**ï¼š
$$
\mathcal{L}^{\text{Stage3}} = \mathcal{L}_{\text{recon}}(s_i^{\text{model}}, \tilde{y}_i) + 0.1 \cdot \mathcal{L}_{\text{rule}}(s_i^{\text{model}}, s_i^{\text{rule}}) + \cdots
$$

å…¶ä¸­$\tilde{y}_i$ä¸ºä¼ªæ ‡ç­¾ï¼ˆæ¨¡å‹é¢„æµ‹ï¼‰ï¼Œ $\lambda_{\text{rule}}$è¿›ä¸€æ­¥é™ä½ä¸º0.1ï¼ˆä»…ä½œä¸ºè½¯çº¦æŸï¼Œé˜²æ­¢å®Œå…¨æ¼‚ç§»ï¼‰

**å®‰å…¨æœºåˆ¶**ï¼š
- å¦‚æœéªŒè¯é›†AUCè¿ç»­3ä¸ªepochä¸‹é™ï¼Œ**å›é€€åˆ°Stage 2**
- ä¼ªæ ‡ç­¾æ•°é‡ä¸Šé™ï¼šåŸå§‹æ•°æ®çš„50%
- å®šæœŸäººå·¥å¤æ ¸ä¼ªæ ‡ç­¾ï¼ˆæ¯10ä¸ªepoché‡‡æ ·æ£€æŸ¥ï¼‰

#### 3.7.4 ä¼ªæ ‡ç­¾ç­–ç•¥è¯¦ç»†ï¼ˆåŸç­–ç•¥1ï¼šè§„åˆ™ä¼˜å…ˆï¼‰

**é€‚ç”¨Stage**ï¼šStage 1-2

ç»¼åˆæ¨¡å‹ã€è§„åˆ™ã€æ³¨æ„åŠ›ä¸‰æ–¹é¢ä¿¡æ¯ï¼Œä½†åœ¨å†²çªæ—¶ä¿¡ä»»è§„åˆ™ï¼š
$$
\text{confidence}_i = \sigma(s_i^{\text{model}}) \cdot s_i^{\text{rule}} \cdot \max_{j \in \{\text{light, stop}\}} \alpha_{ij}
$$

ä¼ªæ ‡ç­¾ç­›é€‰æ¡ä»¶ï¼ˆANDé€»è¾‘ï¼‰ï¼š
- $\text{confidence}_i > 0.85$ï¼ˆé»˜è®¤é˜ˆå€¼ï¼‰
- $|s_i^{\text{model}} - s_i^{\text{rule}}| < 0.2$ï¼ˆæ¨¡å‹ä¸è§„åˆ™ä¸€è‡´ï¼‰
- $\max \alpha_{ij} > 0.3$ï¼ˆæ³¨æ„åŠ›èšç„¦ï¼‰

**å®ç°ä»£ç **ï¼š
```python
# ä¼ªä»£ç ï¼šsrc/self_training/pseudo_labeler.py
def generate_pseudo_labels_rule_priority(
    model_scores: torch.Tensor,
    rule_scores: torch.Tensor,
    attention_weights: torch.Tensor,
    threshold_conf: float = 0.85,
    threshold_consistency: float = 0.2,
):
    """
    è§„åˆ™ä¼˜å…ˆç­–ç•¥ï¼šä»…å½“æ¨¡å‹ä¸è§„åˆ™ä¸€è‡´æ—¶æ‰ç”Ÿæˆä¼ªæ ‡ç­¾
    
    é€‚ç”¨åœºæ™¯ï¼š
    - MVPé˜¶æ®µï¼ˆè§„åˆ™æ˜ç¡®ï¼Œæ¨¡å‹å°šæœªæ”¶æ•›ï¼‰
    - å†·å¯åŠ¨é˜¶æ®µï¼ˆå‰10-20 epochsï¼‰
    - å®‰å…¨å…³é”®åœºæ™¯ï¼ˆå®å¯æ¼æŠ¥ï¼Œä¸èƒ½è¯¯æŠ¥ï¼‰
    """
    pseudo_labels = []
    
    for i in range(len(model_scores)):
        # è®¡ç®—ç½®ä¿¡åº¦
        attention_focus = attention_weights[i].max().item()
        confidence = (
            torch.sigmoid(model_scores[i]).item() * 
            rule_scores[i].item() * 
            attention_focus
        )
        
        # ä¸€è‡´æ€§æ£€æŸ¥
        consistency = abs(model_scores[i].item() - rule_scores[i].item())
        
        # ç”Ÿæˆæ¡ä»¶ï¼ˆANDé€»è¾‘ï¼‰
        if (confidence > threshold_conf and 
            consistency < threshold_consistency and
            attention_focus > 0.3):
            
            # è§„åˆ™ä¼˜å…ˆï¼šä½¿ç”¨è§„åˆ™åˆ¤å®šä½œä¸ºä¼ªæ ‡ç­¾
            pseudo_labels.append({
                'label': 1 if rule_scores[i] > 0.5 else 0,  # è§„åˆ™åˆ¤å®š
                'confidence': confidence,
                'source': 'rule_priority',
                'model_score': model_scores[i].item(),
                'rule_score': rule_scores[i].item(),
            })
        
        # å†²çªåœºæ™¯å¤„ç†ï¼ˆåœºæ™¯Bï¼‰
        elif rule_scores[i] > 0.7 and model_scores[i] < 0.3:
            # è§„åˆ™åˆ¤è¿è§„ï¼Œæ¨¡å‹ä½ç½®ä¿¡ â†’ ä¿¡ä»»è§„åˆ™ï¼Œä½†é™ä½ç½®ä¿¡åº¦
            pseudo_labels.append({
                'label': 1,  # è¿è§„
                'confidence': 0.6,  # é™ä½æƒé‡ï¼ˆåŸè§„åˆ™0.9 â†’ 0.6ï¼‰
                'source': 'rule_override',
                'flag': 'model_disagree'  # æ ‡è®°ä¸ºå¾…äººå·¥å¤æ ¸
            })
    
    return pseudo_labels
```

#### 3.7.5 ä¼ªæ ‡ç­¾ç­–ç•¥è¯¦ç»†ï¼ˆåŸç­–ç•¥2ï¼šåŠ æƒèåˆï¼‰

**é€‚ç”¨Stage**ï¼šStage 2

```python
def generate_pseudo_labels_weighted(
    model_scores: torch.Tensor,
    rule_scores: torch.Tensor,
    attention_weights: torch.Tensor,
    weight_rule: float = 0.6,  # è§„åˆ™æƒé‡
    weight_model: float = 0.4, # æ¨¡å‹æƒé‡
    threshold_conf: float = 0.85,
):
    """
    åŠ æƒèåˆç­–ç•¥ï¼šç»¼åˆæ¨¡å‹ä¸è§„åˆ™
    
    é€‚ç”¨åœºæ™¯ï¼š
    - ä¸­æœŸè®­ç»ƒï¼ˆepoch 30-60ï¼‰
    - æ¨¡å‹é€æ¸å¯ä¿¡æ—¶
    - æ•°æ®é‡è¾ƒå¤§æ—¶ï¼ˆ>1000æ ·æœ¬ï¼‰
    """
    pseudo_labels = []
    
    for i in range(len(model_scores)):
        # åŠ æƒè¯„åˆ†
        fused_score = (
            weight_rule * rule_scores[i] + 
            weight_model * torch.sigmoid(model_scores[i])
        )
        
        # ç½®ä¿¡åº¦ï¼ˆè€ƒè™‘ä¸€è‡´æ€§å¥–åŠ±ï¼‰
        consistency_bonus = 1.0 - abs(model_scores[i] - rule_scores[i]) / 2.0
        confidence = fused_score * attention_weights[i].max() * consistency_bonus
        
        if confidence > threshold_conf:
            pseudo_labels.append({
                'label': 1 if fused_score > 0.5 else 0,
                'confidence': confidence.item(),
                'source': 'weighted_fusion',
                'fused_score': fused_score.item(),
            })
    
    return pseudo_labels
```

#### 3.7.6 ä¼ªæ ‡ç­¾ç­–ç•¥è¯¦ç»†ï¼ˆåŸç­–ç•¥3ï¼šåŠ¨æ€åˆ‡æ¢ï¼‰

**é€‚ç”¨Stage**ï¼šStage 2-3ï¼ˆè‡ªé€‚åº”ï¼‰

```python
class AdaptivePseudoLabeler:
    def __init__(self):
        self.epoch = 0
        self.model_reliability = 0.0  # æ¨¡å‹å¯é åº¦è¯„ä¼°
    
    def select_strategy(self):
        """æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€é€‰æ‹©ç­–ç•¥"""
        if self.epoch < 20 or self.model_reliability < 0.7:
            return 'rule_priority'  # æ—©æœŸï¼šè§„åˆ™ä¼˜å…ˆ
        elif self.epoch < 60 or self.model_reliability < 0.85:
            return 'weighted_fusion'  # ä¸­æœŸï¼šåŠ æƒèåˆ
        else:
            return 'model_priority'  # åæœŸï¼šæ¨¡å‹ä¼˜å…ˆï¼ˆè‡ªè®­ç»ƒè§£é”ï¼‰
    
    def update_reliability(self, val_auc, val_f1, rule_consistency):
        """è¯„ä¼°æ¨¡å‹å¯é åº¦"""
        self.model_reliability = (
            0.4 * val_auc + 
            0.3 * val_f1 + 
            0.3 * rule_consistency
        )
```

#### 3.7.7 ä¼ªæ ‡ç­¾ä¿å­˜ä¸å®‰å…¨çº¦æŸ

```python
class PseudoLabeler:
    def __init__(self, strategy='rule_priority'):
        self.strategy = strategy
        self.pseudo_labels = []
    
    def save_epoch(self, epoch):
        df = pd.DataFrame(self.pseudo_labels)
        df.to_parquet(f'artifacts/pseudo_labels/epoch_{epoch:03d}.parquet')
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total': len(self.pseudo_labels),
            'violations': sum(1 for p in self.pseudo_labels if p['label'] == 1),
            'avg_confidence': np.mean([p['confidence'] for p in self.pseudo_labels]),
            'strategy': self.strategy
        }
        with open(f'artifacts/pseudo_labels/epoch_{epoch:03d}_stats.json', 'w') as f:
            json.dump(stats, f, indent=2)
        
        self.pseudo_labels.clear()
```

**å®‰å…¨çº¦æŸ**ï¼š
- **æ•°é‡ä¸Šé™**ï¼šæ¯ä¸ª epoch æœ€å¤šç”Ÿæˆ 20% çš„ä¼ªæ ‡ç­¾ï¼ˆç›¸å¯¹äºåŸå§‹æ•°æ®é›†ï¼‰
- **æŸå¤±ç›‘æ§**ï¼šè‹¥ $\mathcal{L}_{\text{attn}}$ è¿ç»­ 3 ä¸ª epoch ä¸Šå‡ï¼Œè‡ªåŠ¨é™ä½é˜ˆå€¼ 0.05
- **äººå·¥å¤æ ¸**ï¼šæ¯ 10 ä¸ª epoch é‡‡æ · 10 ä¸ªä¼ªæ ‡ç­¾ï¼Œè¾“å‡ºåˆ° `reports/pseudo_review/` ä¾›äººå·¥æ£€æŸ¥
- **å†²çªæ ‡è®°**ï¼šæ ‡è®°ä¸º `flag='model_disagree'` çš„æ ·æœ¬è‡ªåŠ¨å¯¼å‡ºä¾›ä¸“å®¶å¤æ ¸

### 3.8 å¯è§£é‡Šæ€§ä¸ç›‘æ§

#### 3.8.1 æ³¨æ„åŠ›å¯è§†åŒ–
```python
# src/explain/attention_viz.py
def visualize_attention(image, entities, attention_weights, focal_entity_idx):
    """
    åœ¨åŸå§‹å›¾åƒä¸Šå åŠ æ³¨æ„åŠ›çƒ­åŠ›å›¾
    
    Args:
        image: [H, W, 3] numpy array
        entities: List[Entity]
        attention_weights: [N] æ³¨æ„åŠ›æƒé‡å‘é‡
        focal_entity_idx: ä¸­å¿ƒå®ä½“ç´¢å¼•ï¼ˆé€šå¸¸æ˜¯å¾…æ£€æµ‹è½¦è¾†ï¼‰
    
    Returns:
        annotated_image: å¸¦æ³¨é‡Šçš„å›¾åƒ
    """
    # 1. ç»˜åˆ¶æ‰€æœ‰å®ä½“ bbox
    for i, entity in enumerate(entities):
        color = get_color_by_attention(attention_weights[i])
        cv2.rectangle(image, entity.bbox, color, thickness=2)
    
    # 2. ç»˜åˆ¶æ³¨æ„åŠ›è¿çº¿ï¼ˆfocal â†’ å…¶ä»–å®ä½“ï¼‰
    focal_pos = entities[focal_entity_idx].center
    for i, entity in enumerate(entities):
        if i == focal_entity_idx:
            continue
        alpha = attention_weights[i].item()
        if alpha > 0.1:  # ä»…æ˜¾ç¤ºæ˜¾è‘—è¿çº¿
            thickness = int(alpha * 5)
            cv2.line(image, focal_pos, entity.center, (255, 0, 0), thickness)
            cv2.putText(image, f'{alpha:.2f}', entity.center, 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    # 3. ç»˜åˆ¶è§„åˆ™ä¿¡æ¯
    car = entities[focal_entity_idx]
    info_text = [
        f'Distance: {car.d_stop:.1f}m',
        f'Velocity: {car.velocity:.1f}m/s',
        f'Light: {entities.traffic_light.state}',
        f'Max Attn: {attention_weights.max():.3f}'
    ]
    for i, text in enumerate(info_text):
        cv2.putText(image, text, (10, 30 + i*30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    
    return image
```

#### 3.8.2 Prometheus æŒ‡æ ‡
```python
# src/monitoring/meters.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# å®šä¹‰æŒ‡æ ‡
train_loss = Histogram('traffic_train_loss', 'Training loss', ['loss_type'])
violation_detected = Counter('traffic_violations_detected', 'Number of violations')
attention_consistency = Gauge('traffic_attention_consistency', 'Attention consistency score')
pseudo_label_count = Counter('traffic_pseudo_labels', 'Pseudo labels generated', ['confidence_bin'])

# å¯åŠ¨ç›‘æ§æœåŠ¡
start_http_server(8000)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­è®°å½•
train_loss.labels(loss_type='total').observe(L_total.item())
train_loss.labels(loss_type='recon').observe(L_recon.item())
violation_detected.inc(len(violations))
attention_consistency.set(attn_consistency_score)
```

**ç›‘æ§ç«¯ç‚¹**ï¼š`http://localhost:8000/metrics`

#### 3.8.3 ç»“æ„åŒ–æ—¥å¿—
```python
import structlog

logger = structlog.get_logger()

# è®­ç»ƒæ—¥å¿—
logger.info(
    "training_step",
    epoch=epoch,
    step=step,
    trace_id=trace_id,
    loss_total=L_total.item(),
    loss_recon=L_recon.item(),
    loss_rule=L_rule.item(),
    loss_attn=L_attn.item(),
    grad_norm=grad_norm
)

# è¿è§„æ£€æµ‹æ—¥å¿—
logger.warning(
    "violation_detected",
    trace_id=trace_id,
    scene_id=scene_id,
    entity_id=car.id,
    model_score=score.item(),
    rule_score=rule_score.item(),
    distance=car.d_stop,
    velocity=car.velocity,
    light_state=light.state,
    attention_max=attention_weights.max().item()
)
```

## 4. é…ç½®ä¸æ•°æ®
- `configs/mvp.yaml` éœ€åŒ…å«ï¼šæ•°æ®è·¯å¾„ã€batch sizeã€GAT å¤´æ•°ã€memory bank å°ºå¯¸ã€è§„åˆ™é˜ˆå€¼ã€ç›‘æ§ç«¯å£ã€‚
- æ•æ„Ÿè·¯å¾„é€šè¿‡ç¯å¢ƒå˜é‡ä¼ å…¥ï¼ˆ`DATA_ROOT`, `WANDB_API_KEY` ç­‰ï¼‰ï¼Œæ–‡æ¡£ä¸­ä¸å¾—å‡ºç°çœŸå®å‡­æ®ã€‚
- æ•°æ®ç¼“å­˜æ”¾åœ¨ `data/cache/`ï¼ŒCI ç¯å¢ƒåªä½¿ç”¨åˆæˆæ ·æœ¬ã€‚

## 5. å®‰å…¨ / æ€§èƒ½ / å¯è¿ç»´æ€§

> **æŠ€æœ¯å‹˜è¯¯ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šè¡¥å……GPUæ˜¾å­˜/CPUå†…å­˜éœ€æ±‚çš„è¯¦ç»†ä¼°ç®—å’Œæ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚  
> è¯¦è§ï¼š`docs/archive/design/TECHNICAL_CORRECTIONS.md` é—®é¢˜2

### 5.1 å®‰å…¨
- ç¦æ­¢ç¡¬ç¼–ç å‡­æ®ï¼›è§„åˆ™ DSL åœ¨åŠ è½½æ—¶è¿›è¡Œ schema æ ¡éªŒï¼Œé˜²æ­¢æ‰§è¡Œä»»æ„ä»£ç ã€‚
- æ•°æ®ç¼“å­˜æ”¾åœ¨ `data/cache/`ï¼ŒCI ç¯å¢ƒåªä½¿ç”¨åˆæˆæ ·æœ¬ã€‚
- æ•æ„Ÿè·¯å¾„é€šè¿‡ç¯å¢ƒå˜é‡ä¼ å…¥ï¼ˆ`DATA_ROOT`, `WANDB_API_KEY` ç­‰ï¼‰ã€‚

### 5.2 æ€§èƒ½éœ€æ±‚ä¸ä¼°ç®—

#### 5.2.1 æ¨¡å‹å‚æ•°é‡
```python
# å‡è®¾åœºæ™¯å›¾å¹³å‡èŠ‚ç‚¹æ•° N_avg = 10 (5è½¦ + 3ç¯ + 1åœæ­¢çº¿ + 1å…¨å±€èŠ‚ç‚¹)
# Batch size B = 4

# 1. GATå±‚å‚æ•°
# æ¯å±‚ï¼šLinear(128, 128*8) + Attention(128*8) â‰ˆ 128*128*8*2 = 262K params/layer
# 3å±‚ï¼š262K * 3 = 786K params

# 2. å…¨å±€æ³¨æ„åŠ›
# MultiheadAttention(128, 4 heads): 128*128*4*3 = 196K params

# 3. Scoring head
# MLP: (128*2 â†’ 128 â†’ 64 â†’ 1): 128*128*2 + 128*64 + 64*1 â‰ˆ 41K params

# æ€»å‚æ•°é‡ï¼š786K + 196K + 41K â‰ˆ 1.02M params
# FP32å­˜å‚¨ï¼š1.02M * 4 bytes = 4.08 MB
# FP16æ··åˆç²¾åº¦ï¼š1.02M * 2 bytes = 2.04 MB
```

#### 5.2.2 GPUæ˜¾å­˜éœ€æ±‚ï¼ˆä¿®æ­£ç‰ˆï¼‰

> **é‡å¤§ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šåŸä¼°ç®—å­˜åœ¨å¤šå¤„è®¡ç®—é”™è¯¯ï¼ˆç¨€ç–æ³¨æ„åŠ›ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€ä¸­é—´æ¿€æ´»ï¼‰ã€‚ç°æä¾›ç‰©ç†æ­£ç¡®çš„å®Œæ•´ä¼°ç®—ã€‚

**å‡è®¾æ¡ä»¶**ï¼š
- Batch size: $B = 8$ï¼ˆä¿®æ­£åçš„æ¨èå€¼ï¼‰
- å¹³å‡èŠ‚ç‚¹æ•°: $N_{\text{avg}} = 10$ï¼ˆ5è½¦ + 3ç¯ + 1åœæ­¢çº¿ + 1å…¨å±€èŠ‚ç‚¹ï¼‰
- å¹³å‡è¾¹æ•°: $E_{\text{avg}} = 30$ï¼ˆå‡è®¾å¹³å‡åº¦3ï¼Œç¨€ç–å›¾ï¼‰
- éšè—ç»´åº¦: $d_h = 128$
- GATå±‚æ•°: $L = 3$ï¼Œå¤´æ•°: $K = 8$
- æ•°æ®ç±»å‹: FP32ï¼ˆ4 bytesï¼‰

**è¯¦ç»†æ˜¾å­˜åˆ†é…è¡¨**ï¼š

| ç»„ä»¶ | è®¡ç®—å…¬å¼ | ç»“æœ | è¯´æ˜ |
|------|---------|------|------|
| **1. æ¨¡å‹å‚æ•°** | | **4.08 MB** | |
| - GATå±‚ | $L \times (d_h \times d_h \times K \times 2)$ | $3 \times 128 \times 128 \times 8 \times 2 \times 4 = 3.15$ MB | æ¯å±‚W_kå’Œa_k |
| - å…¨å±€æ³¨æ„åŠ› | $d_h \times d_h \times 4 \times 3$ | $128 \times 128 \times 4 \times 3 \times 4 = 0.79$ MB | Q,K,VæŠ•å½± |
| - è§„åˆ™èšç„¦ | $3 \times d_h \times d_h$ | $3 \times 128 \times 128 \times 4 = 0.20$ MB | rule_scorer |
| - Scoring head | $2 \times d_h \times d_h$ | $\sim 0.13$ MB | ä¸¤å±‚MLP |
| **2. ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamWï¼‰** | | **12.24 MB** | |
| - æ¢¯åº¦ (grad) | åŒæ¨¡å‹å‚æ•° | 4.08 MB | $\nabla_\theta$ |
| - ä¸€é˜¶åŠ¨é‡ (m) | åŒæ¨¡å‹å‚æ•° | 4.08 MB | AdamWçš„$m_t$ |
| - äºŒé˜¶åŠ¨é‡ (v) | åŒæ¨¡å‹å‚æ•° | 4.08 MB | AdamWçš„$v_t$ |
| **å°è®¡ï¼ˆé™æ€ï¼‰** | | **16.32 MB** | æ¨¡å‹+ä¼˜åŒ–å™¨ |
| **3. å‰å‘ä¼ æ’­æ¿€æ´»** | | **per batch** | |
| - è¾“å…¥ç‰¹å¾ | $B \times N_{\text{avg}} \times 10 \times 4$ | $8 \times 10 \times 10 \times 4 = 3.2$ KB | è¾“å…¥å±‚ |
| - GATä¸­é—´æ¿€æ´» | $B \times N_{\text{avg}} \times d_h \times L \times 4$ | $8 \times 10 \times 128 \times 3 \times 4 = 122.88$ KB | æ¯å±‚è¾“å‡º |
| - **æ³¨æ„åŠ›æƒé‡ï¼ˆç¨€ç–ï¼‰** | $B \times E_{\text{avg}} \times K \times L \times 4$ | $8 \times 30 \times 8 \times 3 \times 4 = 23.04$ KB | **ä¿®æ­£ï¼šè¾¹æƒé‡** |
| - å…¨å±€æ³¨æ„åŠ› | $B \times N_{\text{avg}} \times d_h \times 4$ | $8 \times 10 \times 128 \times 4 = 40.96$ KB | å…¨å±€èåˆ |
| - è§„åˆ™èšç„¦ | $B \times N_{\text{car}} \times d_h \times 4$ | $8 \times 5 \times 128 \times 4 = 20.48$ KB | è§„åˆ™æ³¨æ„åŠ› |
| - å‰å‘æ€»è®¡ | | $\sim 210$ KB | per batch |
| **4. åå‘ä¼ æ’­æ¢¯åº¦ç¼“å­˜** | | | |
| - æ¿€æ´»æ¢¯åº¦ | $\approx 2 \times$ å‰å‘æ¿€æ´» | $\sim 420$ KB | PyTorchè‡ªåŠ¨å¾®åˆ† |
| - ä¸­é—´å˜é‡ | $\approx$ å‰å‘æ¿€æ´» | $\sim 210$ KB | Sigmoid/GELUç­‰ |
| - åå‘æ€»è®¡ | | $\sim 630$ KB | per batch |
| **5. PyTorch CUDAå¼€é”€** | | **500 MB** | å›ºå®šå¼€é”€ |
| - CUDAä¸Šä¸‹æ–‡ | | $\sim 400$ MB | cuBLAS, cuDNNç­‰ |
| - å†…å­˜æ± ç¢ç‰‡ | | $\sim 100$ MB | ç¼“å­˜åˆ†é…å™¨ |
| **æ€»è®¡ï¼ˆbatch=8ï¼‰** | | **517.2 MB** | 16.32 + 0.21 + 0.63 + 500 |

**ä¿®æ­£è¯´æ˜**ï¼š
1. âœ… **æ³¨æ„åŠ›æƒé‡**ï¼šä»$N^2$å¯†é›†çŸ©é˜µæ”¹ä¸º$E$ç¨€ç–è¾¹æƒé‡ï¼ˆ38.4KB â†’ 23KBï¼‰
2. âœ… **ä¼˜åŒ–å™¨çŠ¶æ€**ï¼šä»2å€æ”¹ä¸º3å€å‚æ•°é‡ï¼ˆ8MB â†’ 12.24MBï¼‰
3. âœ… **ä¸­é—´æ¿€æ´»**ï¼šæ˜ç¡®åŒ…å«åå‘ä¼ æ’­çš„æ¿€æ´»ç¼“å­˜ï¼ˆ~630KBï¼‰

**ç»“è®º**ï¼š
âœ… **æ˜¾å­˜éœ€æ±‚ï¼š~520 MB**ï¼ˆbatch=8ï¼‰  
âœ… RTX 4090ï¼ˆ24GBï¼‰å¯ä»¥æ”¯æŒï¼š
- **æ¨èbatch size**: 8-16ï¼ˆç•™è¶³å¤Ÿä½™é‡ï¼‰
- **æœ€å¤§batch size**: â‰¤ 32ï¼ˆç†è®ºä¸Šé™ï¼‰
- **æç«¯åœºæ™¯èŠ‚ç‚¹æ•°**: â‰¤ 50

**ä¸åŒbatch sizeçš„æ˜¾å­˜å ç”¨**ï¼š

| Batch Size | å‰å‘+åå‘æ¿€æ´» | æ€»æ˜¾å­˜éœ€æ±‚ | æ¨èåœºæ™¯ |
|-----------|-------------|-----------|---------|
| 4 | 0.42 MB | 516.7 MB | è°ƒè¯•ã€å¿«é€Ÿè¿­ä»£ |
| 8 | 0.84 MB | 517.2 MB | **MVPé»˜è®¤é…ç½®** |
| 16 | 1.68 MB | 518.0 MB | è®­ç»ƒç¨³å®šåä¼˜åŒ– |
| 32 | 3.36 MB | 519.7 MB | ç†è®ºæœ€å¤§å€¼ |

#### 5.2.3 CPUå†…å­˜ä¼°ç®—
```python
# æ•°æ®é›†å¤§å°ï¼š100ä¸ªåœºæ™¯ï¼Œæ¯ä¸ªåœºæ™¯10ä¸ªå®ä½“ï¼Œç‰¹å¾ç»´åº¦10
# å­˜å‚¨ï¼š100 * 10 * 10 * 4 bytes = 40 KBï¼ˆå‡ ä¹å¯ä»¥å¿½ç•¥ï¼‰

# åˆæˆæ•°æ®ç”Ÿæˆè„šæœ¬å¯èƒ½éœ€è¦ï¼š
# - å›¾åƒæ•°æ®ï¼š100 * (1920*1080*3) * 4 bytes â‰ˆ 2.4 GBï¼ˆæœªå‹ç¼©ï¼‰
# - å»ºè®®ä½¿ç”¨JPEGå‹ç¼©å­˜å‚¨ï¼Œè¿è¡Œæ—¶æŒ‰éœ€åŠ è½½

# è®­ç»ƒæ—¶CPUå†…å­˜ï¼š
# - æ•°æ®åŠ è½½å™¨ï¼ˆprefetch=2ï¼‰ï¼š~50 MB
# - æ€»è®¡ï¼š<100 MB
```

#### 5.2.4 æ€§èƒ½ä¼˜åŒ–å»ºè®®
```python
# 1. æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)
scaler.scale(loss).backward()
scaler.step(optimizer)

# æ˜¾å­˜é™ä½è‡³ï¼š~350 MB

# 2. æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§batchï¼‰
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. Checkpointï¼ˆç‰ºç‰²10%é€Ÿåº¦æ¢å–30%æ˜¾å­˜ï¼‰
from torch.utils.checkpoint import checkpoint

def forward_with_checkpoint(self, x):
    return checkpoint(self.gat_layers[0], x)
```

### 5.3 è®­ç»ƒæ—¶é—´ç›®æ ‡
- å•æœºè®­ç»ƒéœ€åœ¨ 1Ã—RTX 4090ï¼ˆCUDA 12.1ï¼‰ä¸‹ <=2h
- è‹¥ GPU ä¸å¯ç”¨ï¼Œé€€åŒ–ä¸º CPU modeï¼ˆbatch size è‡ªåŠ¨ç¼©å‡ï¼‰ï¼Œæ—¥å¿—éœ€æç¤ºé™çº§åŸå› 

### 5.4 è¿ç»´
- Prometheus æŒ‡æ ‡ + Grafana é¢æ¿ç”¨äºè§‚å¯Ÿ loss/è¿è§„è¶‹åŠ¿
- `scripts/render_attention_maps.py` ç”±è¿ç»´/ä¸šåŠ¡äººå‘˜å¤ç°å¯è§£é‡Šç»“æœ

## 6. ä¾èµ–ä¸å·¥å…·

> **é‡å¤§ä¿®æ­£ï¼ˆ2025-12-03ï¼‰**ï¼šé”å®šæ‰€æœ‰ä¾èµ–ç‰ˆæœ¬ï¼Œè§£å†³å·²çŸ¥å…¼å®¹æ€§é—®é¢˜ï¼Œæä¾›å®Œæ•´requirements.txtç¤ºä¾‹ã€‚  
> è§£å†³é—®é¢˜10ï¼šä¾èµ–ç‰ˆæœ¬å†²çªé£é™©

### 6.1 æ ¸å¿ƒä¾èµ–ï¼ˆç‰ˆæœ¬é”å®šï¼‰

**GPUæ·±åº¦å­¦ä¹ æ¡†æ¶**ï¼š
```
torch==2.4.1+cu121
torchvision==0.19.1+cu121
torchaudio==2.4.1+cu121
torch-geometric==2.5.0
```

**æ•°æ®å¤„ç†ä¸å›¾è®¡ç®—**ï¼š
```
numpy==1.26.4
opencv-python==4.9.0.80       # é”å®šç‰ˆæœ¬ï¼ˆä¸torch 2.4å…¼å®¹ï¼‰
pillow==10.2.0
networkx==3.2.1
scikit-learn==1.4.0           # ç”¨äºK-Meansï¼ˆmemoryåˆå§‹åŒ–ï¼‰
```

**é…ç½®ä¸éªŒè¯**ï¼š
```
pydantic==2.6.1
pyyaml==6.0.1
```

**CLIä¸UI**ï¼š
```
typer==0.9.0                  # æ¨èä½¿ç”¨typerï¼ˆæ¯”clickæ›´ç°ä»£ï¼‰
rich==13.7.0
tqdm==4.66.1
```

**ç›‘æ§ä¸æ—¥å¿—**ï¼š
```
prometheus-client==0.19.0
structlog==24.1.0
```

**å¯è§†åŒ–**ï¼š
```
matplotlib==3.8.2
seaborn==0.13.1
```

### 6.2 å·²çŸ¥å…¼å®¹æ€§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

| é—®é¢˜ | å½±å“åº“ | è§£å†³æ–¹æ¡ˆ |
|------|--------|---------|
| **CUDAç‰ˆæœ¬** | torch 2.4.1éœ€è¦CUDA 12.1 | ç¡®ä¿ç³»ç»ŸCUDAç‰ˆæœ¬â‰¥12.1ï¼Œæˆ–ä½¿ç”¨CPUç‰ˆï¼ˆè®­ç»ƒæ…¢ï¼‰ |
| **opencvä¸numpy** | opencv-python 4.10+éœ€è¦numpy 2.0 | é”å®šopencv==4.9.0.80ï¼ˆå…¼å®¹numpy 1.26ï¼‰ |
| **torch-geometric** | ä¾èµ–torchå…·ä½“ç‰ˆæœ¬ | ä½¿ç”¨pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.4.1+cu121.html |
| **prometheuså¤šå®ä¾‹** | åŒä¸€ç«¯å£é‡å¤å¯åŠ¨ | åœ¨ä»£ç ä¸­æ·»åŠ `try-except`å¤„ç†ç«¯å£å ç”¨ |

### 6.3 å®Œæ•´requirements.txt

```txt
# ========== GPUæ·±åº¦å­¦ä¹ æ¡†æ¶ ==========
--find-links https://download.pytorch.org/whl/cu121
torch==2.4.1+cu121
torchvision==0.19.1+cu121
torchaudio==2.4.1+cu121

# ========== å›¾ç¥ç»ç½‘ç»œ ==========
--find-links https://data.pyg.org/whl/torch-2.4.1+cu121.html
torch-geometric==2.5.0
torch-scatter==2.1.2+pt24cu121
torch-sparse==0.6.18+pt24cu121
pyg-lib==0.4.0+pt24cu121

# ========== æ•°æ®å¤„ç† ==========
numpy==1.26.4
opencv-python==4.9.0.80
pillow==10.2.0
networkx==3.2.1
scikit-learn==1.4.0
pandas==2.2.0

# ========== é…ç½®ä¸éªŒè¯ ==========
pydantic==2.6.1
pyyaml==6.0.1

# ========== CLIä¸UI ==========
typer==0.9.0
rich==13.7.0
tqdm==4.66.1
click==8.1.7  # typerä¾èµ–

# ========== ç›‘æ§ä¸æ—¥å¿— ==========
prometheus-client==0.19.0
structlog==24.1.0

# ========== å¯è§†åŒ– ==========
matplotlib==3.8.2
seaborn==0.13.1

# ========== å¼€å‘å·¥å…·ï¼ˆå¯é€‰ï¼‰ ==========
pytest==8.0.0
black==24.1.1
flake8==7.0.0
```

### 6.4 å®‰è£…è¯´æ˜

```bash
# æ–¹æ³•1ï¼šä½¿ç”¨pipï¼ˆæ¨èï¼‰
pip install -r requirements.txt

# æ–¹æ³•2ï¼šä½¿ç”¨Condaï¼ˆæ¨èï¼‰
conda env create -f environment-dev.yml
conda activate traffic-rules-dev

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
python -c "import torch_geometric; print(f'PyG: {torch_geometric.__version__}')"
```

### 6.5 ç¯å¢ƒå˜é‡é…ç½®

```bash
# å¿…éœ€ç¯å¢ƒå˜é‡
export DATA_ROOT="/path/to/data/traffic"
export ARTIFACT_ROOT="./artifacts"

# å¯é€‰ç¯å¢ƒå˜é‡ï¼ˆç›‘æ§ï¼‰
export WANDB_API_KEY="your_wandb_key"  # å¦‚æœä½¿ç”¨WandB
export PROMETHEUS_PORT=8000

# CUDAé…ç½®
export CUDA_VISIBLE_DEVICES=0  # æŒ‡å®šGPU
```

**é‡è¦æç¤º**ï¼š
- ä¾èµ–æ–‡ä»¶å¾…ç”¨æˆ·ç¡®è®¤åæ‰èƒ½ç”¨äºç¯å¢ƒå®‰è£…
- å¦‚éœ€æ–°å¢åº“å¿…é¡»åŒæ­¥æ›´æ–°æœ¬èŠ‚ä¸ `lunwen/requirements.txt`
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨Dockerå›ºå®šæ‰€æœ‰ä¾èµ–ç‰ˆæœ¬

## Checklist
- [x] æ¶æ„å›¾ä¸æ¨¡å—èŒè´£æ¸…æ™°
- [x] ä¸éœ€æ±‚ã€å¼€å‘æ–‡æ¡£ä¸€è‡´
- [x] åŒ…å«ç›‘æ§/å®‰å…¨/æ€§èƒ½è¯´æ˜
- [x] ç®—æ³•æ•°å­¦æ¨¡å‹å®Œæ•´ï¼ˆå«å…¬å¼æ¨å¯¼ï¼‰
- [x] è®­ç»ƒ/æ¨ç†æµç¨‹ä¼ªä»£ç é½å…¨
- [x] æŸå¤±å‡½æ•°è®¾è®¡æ˜ç¡®ï¼ˆå«è¶…å‚æ•°ï¼‰
- [x] å¯è§£é‡Šæ€§æœºåˆ¶è¯¦ç»†ï¼ˆæ³¨æ„åŠ›å¯è§†åŒ–+æ—¥å¿—ï¼‰
- [x] è‡ªè®­ç»ƒå®‰å…¨çº¦æŸæ˜ç¡®
- [x] è¯„å®¡ç»“è®ºè®°å½•ï¼ˆ2025-12-03 ç®—æ³•ç»†åŒ–é€šè¿‡ï¼Œé€‰å®šæ–¹æ¡ˆ1ï¼‰

