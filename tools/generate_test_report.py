#!/usr/bin/env python3
"""
ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šè„šæœ¬

åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œåœ¨éªŒè¯é›†ä¸Šè¿è¡Œæµ‹è¯•ï¼Œç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import torch.nn as nn
import numpy as np
from datetime import datetime
from typing import Dict, List
import json
from rich.console import Console
from rich.table import Table
from rich import box

from src.traffic_rules.data.traffic_dataset import TrafficLightDataset
from src.traffic_rules.graph.builder import GraphBuilder
from src.traffic_rules.models.multi_stage_gat import MultiStageAttentionGAT
from src.traffic_rules.loss.constraint import StagedConstraintLoss
from src.traffic_rules.rules.red_light import RedLightRuleEngine
from src.traffic_rules.monitoring.metrics import compute_full_metrics

console = Console()


def load_model(checkpoint_path: str, device: str = 'cpu') -> nn.Module:
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    model = MultiStageAttentionGAT(
        input_dim=10,
        hidden_dim=128,
        num_gat_layers=3,
        num_heads=8,
        dropout=0.1,
    )
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    return model, checkpoint


def test_model(
    model: nn.Module,
    val_dataset,
    device: str = 'cpu',
    threshold: float = 0.7,
) -> Dict:
    """åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•æ¨¡å‹"""
    model.eval()
    
    graph_builder = GraphBuilder()
    rule_engine = RedLightRuleEngine()
    criterion = StagedConstraintLoss()
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹
    all_model_scores = []
    all_rule_scores = []
    all_attention_weights = []
    all_entity_types = []
    all_edge_indices = []
    all_losses = []
    
    # åœºæ™¯çº§åˆ«çš„ç»Ÿè®¡
    scene_results = []
    
    with torch.no_grad():
        for idx in range(len(val_dataset)):
            scene = val_dataset[idx]
            graph = graph_builder.build(scene)
            
            x = graph.x.to(device)
            edge_index = graph.edge_index.to(device)
            entity_types = graph.entity_types.to(device)
            
            if edge_index.size(1) == 0:
                continue
            
            # å‰å‘ä¼ æ’­
            output = model(x, edge_index, entity_types, return_attention=True)
            
            model_scores = output['scores']
            alpha_gat = output['gat_attention']
            beta_rule = output['rule_attention']
            
            # è·å–è½¦è¾†å®ä½“
            car_entities = scene.get_entities_by_type('car')
            if len(car_entities) == 0:
                continue
            
            # è®¡ç®—è§„åˆ™åˆ†æ•°
            light_probs = get_light_probs(scene.entities).to(device)
            distances = torch.tensor([e.d_stop for e in car_entities], device=device)
            velocities = torch.tensor([e.velocity for e in car_entities], device=device)
            
            rule_scores = rule_engine.evaluate(
                light_probs, distances, velocities, training=False
            )
            
            # è®¡ç®—æŸå¤±
            loss_total, loss_dict = criterion(
                model_scores=model_scores,
                rule_scores=rule_scores,
                alpha_gat=alpha_gat,
                beta_rule=beta_rule,
                edge_index=edge_index,
                entity_types=entity_types,
                model_parameters=list(model.parameters()),
            )
            
            # æ”¶é›†æ•°æ®
            all_model_scores.append(model_scores)
            all_rule_scores.append(rule_scores)
            all_attention_weights.append(alpha_gat)
            all_entity_types.append(entity_types)
            all_edge_indices.append(edge_index)
            all_losses.append(loss_total.item())
            
            # åœºæ™¯çº§åˆ«ç»“æœ
            scene_result = {
                'scene_id': getattr(scene, 'scene_id', f'scene_{idx}'),
                'num_cars': len(car_entities),
                'model_scores': model_scores.cpu().tolist(),
                'rule_scores': rule_scores.cpu().tolist(),
                'loss': loss_total.item(),
            }
            scene_results.append(scene_result)
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    if len(all_model_scores) > 0:
        model_scores_cat = torch.cat(all_model_scores)
        rule_scores_cat = torch.cat(all_rule_scores)
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾çš„attentionï¼ˆç®€åŒ–å¤„ç†ï¼‰
        full_metrics = compute_full_metrics(
            model_scores=model_scores_cat,
            rule_scores=rule_scores_cat,
            attention_weights=all_attention_weights[0],
            entity_types=all_entity_types[0],
            edge_index=all_edge_indices[0],
            threshold=threshold,
        )
        
        full_metrics['avg_loss'] = np.mean(all_losses)
        full_metrics['scene_results'] = scene_results
    else:
        full_metrics = {'avg_loss': 0.0, 'scene_results': []}
    
    return full_metrics


def get_light_probs(entities: List) -> torch.Tensor:
    """æå–äº¤é€šç¯çŠ¶æ€æ¦‚ç‡"""
    lights = [e for e in entities if e.type == 'light']
    
    if len(lights) == 0:
        return torch.tensor([[0.0, 0.0, 1.0]])
    
    light = lights[0]
    state_map = {'red': 0, 'yellow': 1, 'green': 2}
    probs = torch.zeros(1, 3)
    
    if hasattr(light, 'light_state') and light.light_state:
        idx = state_map.get(light.light_state, 2)
        probs[0, idx] = light.confidence if hasattr(light, 'confidence') else 0.9
        remaining = 1.0 - probs[0, idx]
        for j in range(3):
            if j != idx:
                probs[0, j] = remaining / 2
    else:
        probs = torch.tensor([[0.0, 0.0, 1.0]])
    
    return probs


def generate_report(
    checkpoint_path: str,
    data_root: str = "data/synthetic",
    device: str = "cpu",
    threshold: float = 0.7,
    output_path: str = "TEST_REPORT.md",
):
    """ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•æŠ¥å‘Š"""
    console.print("\n[bold blue]ğŸ§ª å¼€å§‹ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...[/bold blue]\n")
    
    # åŠ è½½æ¨¡å‹
    console.print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model, checkpoint = load_model(checkpoint_path, device)
    console.print(f"[green]âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (Epoch {checkpoint.get('epoch', 'unknown')})[/green]")
    
    # åŠ è½½éªŒè¯é›†
    console.print("\nğŸ“Š åŠ è½½éªŒè¯é›†...")
    val_dataset = TrafficLightDataset(
        data_root=data_root,
        mode='synthetic',
        split='val',
    )
    console.print(f"[green]âœ… éªŒè¯é›†: {len(val_dataset)} åœºæ™¯[/green]")
    
    # è¿è¡Œæµ‹è¯•
    console.print("\nğŸ”¬ è¿è¡Œæµ‹è¯•...")
    test_results = test_model(model, val_dataset, device, threshold)
    
    # ç”ŸæˆæŠ¥å‘Š
    console.print("\nğŸ“ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    # é¢„å¤„ç†checkpointæ•°æ®ï¼Œé¿å…f-stringä¸­çš„å¤æ‚è¡¨è¾¾å¼
    val_loss = checkpoint.get('val_metrics', {}).get('loss', None)
    val_loss_str = f"{val_loss:.4f}" if isinstance(val_loss, (int, float)) else 'N/A'
    
    val_auc = checkpoint.get('val_metrics', {}).get('auc', None)
    val_auc_str = f"{val_auc:.4f}" if isinstance(val_auc, (int, float)) else 'N/A'
    
    val_rule_cons = checkpoint.get('val_metrics', {}).get('rule_consistency', None)
    val_rule_cons_str = f"{val_rule_cons:.4f}" if isinstance(val_rule_cons, (int, float)) else 'N/A'
    
    # è®¡ç®—å˜åŒ–
    loss_diff = test_results.get('avg_loss', 0.0) - (val_loss if isinstance(val_loss, (int, float)) else 0.0)
    loss_diff_str = f"{loss_diff:.4f}" if isinstance(val_loss, (int, float)) else 'N/A'
    
    auc_diff = test_results.get('auc', 0.0) - (val_auc if isinstance(val_auc, (int, float)) else 0.0)
    auc_diff_str = f"{auc_diff:.4f}" if isinstance(val_auc, (int, float)) else 'N/A'
    
    rule_cons_diff = test_results.get('rule_consistency', 0.0) - (val_rule_cons if isinstance(val_rule_cons, (int, float)) else 0.0)
    rule_cons_diff_str = f"{rule_cons_diff:.4f}" if isinstance(val_rule_cons, (int, float)) else 'N/A'
    
    report = f"""# æµ‹è¯•æŠ¥å‘Š - çº¢ç¯åœå¼‚å¸¸æ£€æµ‹æ¨¡å‹

## æŠ¥å‘Šä¿¡æ¯
- **æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ¨¡å‹æ£€æŸ¥ç‚¹**: {checkpoint_path}
- **æµ‹è¯•æ•°æ®é›†**: {data_root}/val
- **æµ‹è¯•åœºæ™¯æ•°**: {len(val_dataset)}
- **è®­ç»ƒEpoch**: {checkpoint.get('epoch', 'unknown')}
- **æœ€ä½³éªŒè¯Loss**: {val_loss_str}

---

## ğŸ“Š æµ‹è¯•æŒ‡æ ‡æ€»è§ˆ

### æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **å¹³å‡æŸå¤±** | {test_results.get('avg_loss', 0.0):.4f} | æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å¹³å‡æŸå¤± |
| **AUC** | {test_results.get('auc', 0.0):.4f} | ROCæ›²çº¿ä¸‹é¢ç§¯ |
| **F1 Score** | {test_results.get('f1', 0.0):.4f} | F1åˆ†æ•°ï¼ˆé˜ˆå€¼={threshold}ï¼‰ |
| **Precision** | {test_results.get('precision', 0.0):.4f} | ç²¾ç¡®ç‡ |
| **Recall** | {test_results.get('recall', 0.0):.4f} | å¬å›ç‡ |
| **è§„åˆ™ä¸€è‡´æ€§** | {test_results.get('rule_consistency', 0.0):.4f} | æ¨¡å‹é¢„æµ‹ä¸è§„åˆ™åˆ†æ•°çš„ä¸€è‡´æ€§ |
| **æ³¨æ„åŠ›èšç„¦** | {test_results.get('attention_focus', 0.0):.4f} | æ³¨æ„åŠ›æƒé‡èšç„¦ç¨‹åº¦ |

### ä¸è®­ç»ƒæŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | è®­ç»ƒæ—¶ï¼ˆæœ€ä½³ï¼‰ | æµ‹è¯•æ—¶ | å˜åŒ– |
|------|---------------|--------|------|
| **Val Loss** | {val_loss_str} | {test_results.get('avg_loss', 0.0):.4f} | {loss_diff_str} |
| **AUC** | {val_auc_str} | {test_results.get('auc', 0.0):.4f} | {auc_diff_str} |
| **Rule Cons.** | {val_rule_cons_str} | {test_results.get('rule_consistency', 0.0):.4f} | {rule_cons_diff_str} |

---

## ğŸ“ˆ è¯¦ç»†åˆ†æ

### 1. åˆ†ç±»æ€§èƒ½åˆ†æ

**AUC = {test_results.get('auc', 0.0):.4f}**

- {'âœ… ä¼˜ç§€' if test_results.get('auc', 0.0) >= 0.9 else 'âœ… è‰¯å¥½' if test_results.get('auc', 0.0) >= 0.8 else 'âš ï¸ éœ€æ”¹è¿›' if test_results.get('auc', 0.0) >= 0.7 else 'âŒ è¾ƒå·®'} (ç›®æ ‡: â‰¥0.90)
- æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°åŒºåˆ†è¿è§„å’Œæ­£å¸¸åœºæ™¯

**F1 Score = {test_results.get('f1', 0.0):.4f}**

- {'âœ… ä¼˜ç§€' if test_results.get('f1', 0.0) >= 0.85 else 'âœ… è‰¯å¥½' if test_results.get('f1', 0.0) >= 0.75 else 'âš ï¸ éœ€æ”¹è¿›' if test_results.get('f1', 0.0) >= 0.5 else 'âŒ è¾ƒå·®'} (ç›®æ ‡: â‰¥0.85)
- Precision = {test_results.get('precision', 0.0):.4f}, Recall = {test_results.get('recall', 0.0):.4f}
- {'æ¨¡å‹å€¾å‘äºä¿å®ˆé¢„æµ‹ï¼ˆé«˜ç²¾ç¡®ç‡ï¼Œä½å¬å›ç‡ï¼‰' if test_results.get('precision', 0.0) > test_results.get('recall', 0.0) + 0.1 else 'æ¨¡å‹å€¾å‘äºæ¿€è¿›é¢„æµ‹ï¼ˆä½ç²¾ç¡®ç‡ï¼Œé«˜å¬å›ç‡ï¼‰' if test_results.get('recall', 0.0) > test_results.get('precision', 0.0) + 0.1 else 'ç²¾ç¡®ç‡å’Œå¬å›ç‡ç›¸å¯¹å¹³è¡¡'}

### 2. è§„åˆ™ä¸€è‡´æ€§åˆ†æ

**è§„åˆ™ä¸€è‡´æ€§ = {test_results.get('rule_consistency', 0.0):.4f}**

- {'âœ… ä¼˜ç§€' if test_results.get('rule_consistency', 0.0) >= 0.8 else 'âœ… è‰¯å¥½' if test_results.get('rule_consistency', 0.0) >= 0.7 else 'âš ï¸ éœ€æ”¹è¿›'} (ç›®æ ‡: â‰¥0.75)
- æ¨¡å‹é¢„æµ‹ä¸è§„åˆ™å¼•æ“è¯„åˆ†çš„ä¸€è‡´æ€§ç¨‹åº¦
- {'æ¨¡å‹å¾ˆå¥½åœ°å­¦ä¹ äº†è§„åˆ™é€»è¾‘' if test_results.get('rule_consistency', 0.0) >= 0.75 else 'æ¨¡å‹ä»éœ€è¿›ä¸€æ­¥å­¦ä¹ è§„åˆ™é€»è¾‘'}

### 3. æ³¨æ„åŠ›æœºåˆ¶åˆ†æ

**æ³¨æ„åŠ›èšç„¦ = {test_results.get('attention_focus', 0.0):.4f}**

- {'âœ… æ³¨æ„åŠ›æœºåˆ¶å·¥ä½œè‰¯å¥½' if test_results.get('attention_focus', 0.0) >= 0.6 else 'âš ï¸ æ³¨æ„åŠ›æœºåˆ¶éœ€è¦ä¼˜åŒ–'}
- æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿèšç„¦åˆ°å…³é”®çš„äº¤é€šå®ä½“ï¼ˆè½¦è¾†ã€äº¤é€šç¯ã€åœæ­¢çº¿ï¼‰

---

## ğŸ¯ åœºæ™¯çº§åˆ«ç»Ÿè®¡

### æµ‹è¯•åœºæ™¯åˆ†å¸ƒ

- **æ€»åœºæ™¯æ•°**: {len(test_results.get('scene_results', []))}
- **æ€»è½¦è¾†æ•°**: {sum(s.get('num_cars', 0) for s in test_results.get('scene_results', []))}

### åœºæ™¯ç±»å‹åˆ†æ

ï¼ˆåŸºäºåœºæ™¯IDå’Œå…ƒæ•°æ®æ¨æ–­ï¼‰

---

## ğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯

### æ¨¡å‹æ¶æ„
- **æ¨¡å‹ç±»å‹**: MultiStageAttentionGAT
- **è¾“å…¥ç»´åº¦**: 10
- **éšè—ç»´åº¦**: 128
- **GATå±‚æ•°**: 3
- **æ³¨æ„åŠ›å¤´æ•°**: 8
- **Dropout**: 0.1

### è®­ç»ƒé…ç½®
- **å­¦ä¹ ç‡**: {checkpoint.get('train_metrics', {}).get('lr', 'N/A') if 'train_metrics' in checkpoint else 'N/A'}
- **ä¼˜åŒ–å™¨**: AdamW
- **æŸå¤±å‡½æ•°**: StagedConstraintLoss
- **è®¾å¤‡**: {device}

---

## âœ… ç»“è®ºä¸å»ºè®®

### ä¸»è¦å‘ç°

1. **æ¨¡å‹æ€§èƒ½**: {'âœ… æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¡¨ç°è‰¯å¥½' if test_results.get('auc', 0.0) >= 0.8 else 'âš ï¸ æ¨¡å‹æ€§èƒ½æœ‰å¾…æå‡'}
2. **è§„åˆ™å­¦ä¹ **: {'âœ… æ¨¡å‹æˆåŠŸå­¦ä¹ äº†è§„åˆ™é€»è¾‘' if test_results.get('rule_consistency', 0.0) >= 0.75 else 'âš ï¸ æ¨¡å‹å¯¹è§„åˆ™çš„å­¦ä¹ ä»éœ€åŠ å¼º'}
3. **åˆ†ç±»èƒ½åŠ›**: {'âœ… æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†è¿è§„åœºæ™¯' if test_results.get('f1', 0.0) >= 0.5 else 'âš ï¸ æ¨¡å‹åˆ†ç±»èƒ½åŠ›éœ€è¦æ”¹è¿›'}

### æ”¹è¿›å»ºè®®

1. {'**AUCä¼˜åŒ–**: å½“å‰AUCä¸º{test_results.get("auc", 0.0):.4f}ï¼Œå»ºè®®é€šè¿‡ä»¥ä¸‹æ–¹å¼æå‡ï¼š' if test_results.get('auc', 0.0) < 0.9 else '**AUCè¡¨ç°è‰¯å¥½**: å·²è¾¾åˆ°ç›®æ ‡æ°´å¹³'}
   - å¢åŠ è®­ç»ƒæ•°æ®é‡
   - è°ƒæ•´æ¨¡å‹æ¶æ„ï¼ˆå¢åŠ å±‚æ•°æˆ–éšè—ç»´åº¦ï¼‰
   - ä¼˜åŒ–è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ­£åˆ™åŒ–ï¼‰

2. {'**F1 Scoreä¼˜åŒ–**: å½“å‰F1ä¸º{test_results.get("f1", 0.0):.4f}ï¼Œå»ºè®®ï¼š' if test_results.get('f1', 0.0) < 0.85 else '**F1 Scoreè¡¨ç°è‰¯å¥½**: å·²è¾¾åˆ°ç›®æ ‡æ°´å¹³'}
   - è°ƒæ•´åˆ†ç±»é˜ˆå€¼ï¼ˆå½“å‰{threshold}ï¼‰
   - ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡
   - å¢åŠ è¿è§„æ ·æœ¬çš„è®­ç»ƒæ•°æ®

3. **è§„åˆ™ä¸€è‡´æ€§ä¼˜åŒ–**:
   - å¢åŠ è§„åˆ™æŸå¤±æƒé‡
   - ä½¿ç”¨è§„åˆ™å¼•å¯¼çš„é¢„è®­ç»ƒ

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

- **æ¨¡å‹æ£€æŸ¥ç‚¹**: `{checkpoint_path}`
- **è®­ç»ƒæ›²çº¿**: `reports/training_curves.png`
- **æµ‹è¯•æ•°æ®**: `{data_root}/val`

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

    # ä¿å­˜æŠ¥å‘Š
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(report, encoding='utf-8')
    
    console.print(f"[green]âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {output_path}[/green]")
    
    # æ‰“å°æ‘˜è¦
    console.print("\n" + "="*60)
    console.print("[bold]æµ‹è¯•ç»“æœæ‘˜è¦[/bold]")
    console.print("="*60)
    
    table = Table(box=box.ROUNDED)
    table.add_column("æŒ‡æ ‡", style="cyan")
    table.add_column("æ•°å€¼", style="magenta")
    
    table.add_row("å¹³å‡æŸå¤±", f"{test_results.get('avg_loss', 0.0):.4f}")
    table.add_row("AUC", f"{test_results.get('auc', 0.0):.4f}")
    table.add_row("F1 Score", f"{test_results.get('f1', 0.0):.4f}")
    table.add_row("Precision", f"{test_results.get('precision', 0.0):.4f}")
    table.add_row("Recall", f"{test_results.get('recall', 0.0):.4f}")
    table.add_row("è§„åˆ™ä¸€è‡´æ€§", f"{test_results.get('rule_consistency', 0.0):.4f}")
    
    console.print(table)
    console.print("="*60 + "\n")
    
    return test_results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š")
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="artifacts/checkpoints/best.pth",
        help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„"
    )
    parser.add_argument(
        "--data-root",
        type=str,
        default="data/synthetic",
        help="æ•°æ®æ ¹ç›®å½•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="è®¾å¤‡ (cpu/cuda)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.7,
        help="åˆ†ç±»é˜ˆå€¼"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="TEST_REPORT.md",
        help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„"
    )
    
    args = parser.parse_args()
    
    generate_report(
        checkpoint_path=args.checkpoint,
        data_root=args.data_root,
        device=args.device,
        threshold=args.threshold,
        output_path=args.output,
    )

