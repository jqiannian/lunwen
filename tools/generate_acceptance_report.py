#!/usr/bin/env python3
"""
éªŒæ”¶æŠ¥å‘Šè‡ªåŠ¨åŒ–ç”Ÿæˆè„šæœ¬

åŠŸèƒ½ï¼š
1. è¯»å–test_red_light.pyè¾“å‡ºçš„JSONè¯æ®é“¾
2. æŒ‰scenarioåˆ†ç±»ç»Ÿè®¡
3. è®¡ç®—å„åœºæ™¯å‡†ç¡®ç‡ã€å¬å›ç‡ç­‰æŒ‡æ ‡
4. ç”ŸæˆMarkdownæ ¼å¼çš„éªŒæ”¶æŠ¥å‘Š
"""

from __future__ import annotations

import argparse
import json
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any


def load_scene_reports(test_results_dir: Path) -> dict[str, list[dict[str, Any]]]:
    """
    åŠ è½½æ‰€æœ‰åœºæ™¯æŠ¥å‘Šå¹¶æŒ‰scenarioåˆ†ç±»
    
    Args:
        test_results_dir: æµ‹è¯•ç»“æœç›®å½•ï¼ˆåŒ…å«scene_*.jsonæ–‡ä»¶ï¼‰
    
    Returns:
        scenarios: {scenario_type: [scene_report, ...]}
    """
    scenarios = defaultdict(list)
    
    # è¯»å–æ‰€æœ‰scene_*.jsonæ–‡ä»¶
    scene_files = sorted(test_results_dir.glob("scene_*.json"))
    
    for scene_file in scene_files:
        try:
            data = json.loads(scene_file.read_text(encoding='utf-8'))
            scenario_type = data.get('scenario_type', 'unknown')
            scenarios[scenario_type].append(data)
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•è¯»å– {scene_file}: {e}")
    
    return dict(scenarios)


def compute_scenario_metrics(scenes: list[dict[str, Any]], scenario_type: str) -> dict[str, Any]:
    """
    è®¡ç®—å•ä¸ªåœºæ™¯ç±»å‹çš„æŒ‡æ ‡
    
    Args:
        scenes: åœºæ™¯æŠ¥å‘Šåˆ—è¡¨
        scenario_type: åœºæ™¯ç±»å‹
    
    Returns:
        metrics: æŒ‡æ ‡å­—å…¸
    """
    total_scenes = len(scenes)
    total_violations_detected = sum(s['summary']['violations_detected'] for s in scenes)
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    all_scores = []
    for scene in scenes:
        for evidence in scene['evidence']:
            all_scores.append(evidence['final_score'])
    
    avg_score = sum(all_scores) / len(all_scores) if all_scores else 0.0
    max_score = max(all_scores) if all_scores else 0.0
    min_score = min(all_scores) if all_scores else 0.0
    
    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆæ ¹æ®åœºæ™¯ç±»å‹ï¼‰
    if scenario_type == "violation":
        # è¿è§„åœºæ™¯ï¼šæœŸæœ›æ£€æµ‹åˆ°è¿è§„
        # å‡†ç¡®ç‡ = æ£€æµ‹åˆ°è‡³å°‘1ä¸ªè¿è§„çš„åœºæ™¯æ•° / æ€»åœºæ™¯æ•°
        scenes_with_violations = sum(1 for s in scenes if s['summary']['violations_detected'] > 0)
        accuracy = scenes_with_violations / total_scenes if total_scenes > 0 else 0.0
        
        # å¬å›ç‡ = æ€»æ£€æµ‹åˆ°çš„è¿è§„æ•° / æ€»åœºæ™¯æ•°ï¼ˆå‡è®¾æ¯ä¸ªåœºæ™¯è‡³å°‘æœ‰1ä¸ªè¿è§„ï¼‰
        recall = total_violations_detected / total_scenes if total_scenes > 0 else 0.0
        
        # ç²¾ç¡®ç‡ï¼ˆè¿™é‡Œç®€åŒ–ä¸º1.0ï¼Œå› ä¸ºæˆ‘ä»¬åªç»Ÿè®¡çœŸæ­£çš„violationåœºæ™¯ï¼‰
        precision = 1.0
        
    elif scenario_type in ["parking", "green_pass"]:
        # æ­£å¸¸åœºæ™¯ï¼šæœŸæœ›ä¸æ£€æµ‹åˆ°è¿è§„
        # å‡†ç¡®ç‡ = æœªæ£€æµ‹åˆ°è¿è§„çš„åœºæ™¯æ•° / æ€»åœºæ™¯æ•°
        scenes_without_violations = sum(1 for s in scenes if s['summary']['violations_detected'] == 0)
        accuracy = scenes_without_violations / total_scenes if total_scenes > 0 else 0.0
        
        # å¯¹äºæ­£å¸¸åœºæ™¯ï¼Œå¬å›ç‡å’Œç²¾ç¡®ç‡ä¸é€‚ç”¨
        recall = None
        precision = None
        
    else:
        accuracy = None
        recall = None
        precision = None
    
    return {
        'total_scenes': total_scenes,
        'violations_detected': total_violations_detected,
        'accuracy': accuracy,
        'recall': recall,
        'precision': precision,
        'avg_score': avg_score,
        'max_score': max_score,
        'min_score': min_score,
    }


def generate_markdown_report(
    scenarios: dict[str, list[dict[str, Any]]],
    output_path: Path,
    screenshots_dir: Path | None = None,
    heatmaps_index: Path | None = None,
) -> None:
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„éªŒæ”¶æŠ¥å‘Š
    
    Args:
        scenarios: æŒ‰scenarioåˆ†ç±»çš„åœºæ™¯æŠ¥å‘Š
        output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
        screenshots_dir: æˆªå›¾ç›®å½•ï¼ˆå¯é€‰ï¼‰
        heatmaps_index: çƒ­åŠ›å›¾ç´¢å¼•é¡µè·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    lines = []
    
    # æ ‡é¢˜
    lines.append("# çº¢ç¯åœMVPéªŒæ”¶æŠ¥å‘Š")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    lines.append("---")
    lines.append("")
    
    # æ€»è§ˆ
    total_scenes = sum(len(scenes) for scenes in scenarios.values())
    total_violations = sum(
        sum(s['summary']['violations_detected'] for s in scenes)
        for scenes in scenarios.values()
    )
    
    lines.append("## ğŸ“Š æµ‹è¯•æ€»è§ˆ")
    lines.append("")
    lines.append(f"- **æ€»åœºæ™¯æ•°**ï¼š{total_scenes}")
    lines.append(f"- **æ€»è¿è§„æ£€å‡ºæ•°**ï¼š{total_violations}")
    lines.append(f"- **åœºæ™¯ç±»å‹æ•°**ï¼š{len(scenarios)}")
    lines.append("")
    
    # åœºæ™¯åˆ†ç±»ç»Ÿè®¡è¡¨
    lines.append("## ğŸ“ˆ åœºæ™¯åˆ†ç±»ç»Ÿè®¡")
    lines.append("")
    lines.append("| åœºæ™¯ç±»å‹ | åœºæ™¯æ•° | è¿è§„æ£€å‡º | å‡†ç¡®ç‡ | å¬å›ç‡ | å¹³å‡åˆ†æ•° |")
    lines.append("|---------|-------|---------|--------|--------|----------|")
    
    for scenario_type in sorted(scenarios.keys()):
        scenes = scenarios[scenario_type]
        metrics = compute_scenario_metrics(scenes, scenario_type)
        
        accuracy_str = f"{metrics['accuracy']:.1%}" if metrics['accuracy'] is not None else "N/A"
        recall_str = f"{metrics['recall']:.1%}" if metrics['recall'] is not None else "N/A"
        
        lines.append(
            f"| {scenario_type} | {metrics['total_scenes']} | "
            f"{metrics['violations_detected']} | {accuracy_str} | {recall_str} | "
            f"{metrics['avg_score']:.3f} |"
        )
    
    lines.append("")
    
    # å„åœºæ™¯ç±»å‹è¯¦æƒ…
    lines.append("## ğŸ“ åœºæ™¯è¯¦æƒ…")
    lines.append("")
    
    for scenario_type in sorted(scenarios.keys()):
        scenes = scenarios[scenario_type]
        metrics = compute_scenario_metrics(scenes, scenario_type)
        
        lines.append(f"### {scenario_type.upper()} åœºæ™¯")
        lines.append("")
        lines.append(f"- **åœºæ™¯æ•°**ï¼š{metrics['total_scenes']}")
        lines.append(f"- **è¿è§„æ£€å‡º**ï¼š{metrics['violations_detected']}")
        
        if metrics['accuracy'] is not None:
            lines.append(f"- **å‡†ç¡®ç‡**ï¼š{metrics['accuracy']:.1%}")
        if metrics['recall'] is not None:
            lines.append(f"- **å¬å›ç‡**ï¼š{metrics['recall']:.1%}")
        
        lines.append(f"- **å¹³å‡åˆ†æ•°**ï¼š{metrics['avg_score']:.3f}")
        lines.append(f"- **åˆ†æ•°èŒƒå›´**ï¼š[{metrics['min_score']:.3f}, {metrics['max_score']:.3f}]")
        lines.append("")
        
        # ç¤ºä¾‹åœºæ™¯ï¼ˆå‰3ä¸ªï¼‰
        if len(scenes) > 0:
            lines.append("**ç¤ºä¾‹åœºæ™¯**ï¼š")
            lines.append("")
            
            for i, scene in enumerate(scenes[:3]):
                scene_id = scene['scene_id']
                violations = scene['summary']['violations_detected']
                max_score = scene['summary']['max_final_score']
                
                lines.append(f"{i+1}. `{scene_id}` - è¿è§„æ£€å‡º: {violations}, æœ€é«˜åˆ†æ•°: {max_score:.3f}")
                
                # æ·»åŠ æˆªå›¾é“¾æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if screenshots_dir:
                    screenshot_path = screenshots_dir / f"{scene_id}_violation.png"
                    if screenshot_path.exists():
                        rel_path = screenshot_path.relative_to(output_path.parent)
                        lines.append(f"   - ğŸ“· [æŸ¥çœ‹æˆªå›¾]({rel_path})")
            
            lines.append("")
    
    # å¯è§†åŒ–èµ„æº
    lines.append("## ğŸ¨ å¯è§†åŒ–èµ„æº")
    lines.append("")
    
    if screenshots_dir and screenshots_dir.exists():
        screenshot_count = len(list(screenshots_dir.glob("*.png")))
        if screenshot_count > 0:
            rel_screenshots_dir = screenshots_dir.relative_to(output_path.parent)
            lines.append(f"- **è¿è§„æˆªå›¾**ï¼š{screenshot_count} å¼ ")
            lines.append(f"  - ç›®å½•ï¼š`{rel_screenshots_dir}/`")
            lines.append("")
    
    if heatmaps_index and heatmaps_index.exists():
        rel_heatmaps_index = heatmaps_index.relative_to(output_path.parent)
        lines.append(f"- **æ³¨æ„åŠ›çƒ­åŠ›å›¾ç´¢å¼•**ï¼š[æ‰“å¼€æµè§ˆ]({rel_heatmaps_index})")
        lines.append("")
    
    # éªŒæ”¶ç»“è®º
    lines.append("## âœ… éªŒæ”¶ç»“è®º")
    lines.append("")
    
    # æ£€æŸ¥éªŒæ”¶æ ‡å‡†
    violation_metrics = compute_scenario_metrics(scenarios.get('violation', []), 'violation')
    parking_metrics = compute_scenario_metrics(scenarios.get('parking', []), 'parking')
    green_metrics = compute_scenario_metrics(scenarios.get('green_pass', []), 'green_pass')
    
    lines.append("### éªŒæ”¶æ ‡å‡†æ£€æŸ¥")
    lines.append("")
    
    checks = []
    
    # æ ‡å‡†1ï¼šviolationåœºæ™¯å¬å›ç‡ >= 0.9
    if violation_metrics['recall'] is not None:
        if violation_metrics['recall'] >= 0.9:
            checks.append(("âœ…", f"violationåœºæ™¯å¬å›ç‡ â‰¥ 0.9: {violation_metrics['recall']:.1%}"))
        else:
            checks.append(("âš ï¸", f"violationåœºæ™¯å¬å›ç‡ < 0.9: {violation_metrics['recall']:.1%}"))
    
    # æ ‡å‡†2ï¼šparking/green_passåœºæ™¯å‡†ç¡®ç‡ >= 0.95
    if parking_metrics['accuracy'] is not None:
        if parking_metrics['accuracy'] >= 0.95:
            checks.append(("âœ…", f"parkingåœºæ™¯å‡†ç¡®ç‡ â‰¥ 0.95: {parking_metrics['accuracy']:.1%}"))
        else:
            checks.append(("âš ï¸", f"parkingåœºæ™¯å‡†ç¡®ç‡ < 0.95: {parking_metrics['accuracy']:.1%}"))
    
    if green_metrics['accuracy'] is not None:
        if green_metrics['accuracy'] >= 0.95:
            checks.append(("âœ…", f"green_passåœºæ™¯å‡†ç¡®ç‡ â‰¥ 0.95: {green_metrics['accuracy']:.1%}"))
        else:
            checks.append(("âš ï¸", f"green_passåœºæ™¯å‡†ç¡®ç‡ < 0.95: {green_metrics['accuracy']:.1%}"))
    
    # æ ‡å‡†3ï¼šæ˜¯å¦ç”Ÿæˆäº†æˆªå›¾å’Œçƒ­åŠ›å›¾
    if screenshots_dir and screenshots_dir.exists():
        screenshot_count = len(list(screenshots_dir.glob("*.png")))
        if screenshot_count > 0:
            checks.append(("âœ…", f"ç”Ÿæˆäº† {screenshot_count} å¼ è¿è§„æˆªå›¾"))
        else:
            checks.append(("âš ï¸", "æœªç”Ÿæˆè¿è§„æˆªå›¾"))
    
    if heatmaps_index and heatmaps_index.exists():
        checks.append(("âœ…", "ç”Ÿæˆäº†æ³¨æ„åŠ›çƒ­åŠ›å›¾ç´¢å¼•"))
    else:
        checks.append(("âš ï¸", "æœªç”Ÿæˆæ³¨æ„åŠ›çƒ­åŠ›å›¾"))
    
    for icon, check in checks:
        lines.append(f"- {icon} {check}")
    
    lines.append("")
    
    # æ€»ç»“
    passed_count = sum(1 for icon, _ in checks if icon == "âœ…")
    total_count = len(checks)
    
    if passed_count == total_count:
        lines.append(f"**æ€»ä½“ç»“è®º**ï¼šâœ… **é€šè¿‡éªŒæ”¶** ({passed_count}/{total_count})")
    else:
        lines.append(f"**æ€»ä½“ç»“è®º**ï¼šâš ï¸ **éƒ¨åˆ†é€šè¿‡** ({passed_count}/{total_count})")
    
    lines.append("")
    
    # å†™å…¥æ–‡ä»¶
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text("\n".join(lines), encoding='utf-8')


def main() -> None:
    parser = argparse.ArgumentParser(description="ç”ŸæˆéªŒæ”¶æŠ¥å‘Š")
    parser.add_argument(
        "--test-results",
        default=Path("reports/testing"),
        type=Path,
        help="æµ‹è¯•ç»“æœç›®å½•ï¼ˆåŒ…å«scene_*.jsonæ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--output",
        default=Path("reports/ACCEPTANCE_REPORT.md"),
        type=Path,
        help="æŠ¥å‘Šè¾“å‡ºè·¯å¾„",
    )
    parser.add_argument(
        "--screenshots-dir",
        default=Path("reports/testing/screenshots"),
        type=Path,
        help="æˆªå›¾ç›®å½•ï¼ˆå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--heatmaps-index",
        default=Path("reports/testing/heatmaps/index.html"),
        type=Path,
        help="çƒ­åŠ›å›¾ç´¢å¼•é¡µè·¯å¾„ï¼ˆå¯é€‰ï¼‰",
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ç”ŸæˆéªŒæ”¶æŠ¥å‘Š")
    print("=" * 60)
    
    # åŠ è½½åœºæ™¯æŠ¥å‘Š
    print(f"åŠ è½½æµ‹è¯•ç»“æœ: {args.test_results}")
    scenarios = load_scene_reports(args.test_results)
    
    if not scenarios:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•åœºæ™¯æŠ¥å‘Š")
        return
    
    print(f"æ‰¾åˆ° {len(scenarios)} ç§åœºæ™¯ç±»å‹ï¼š")
    for scenario_type, scenes in scenarios.items():
        print(f"  - {scenario_type}: {len(scenes)} ä¸ªåœºæ™¯")
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nç”ŸæˆæŠ¥å‘Š: {args.output}")
    generate_markdown_report(
        scenarios,
        args.output,
        screenshots_dir=args.screenshots_dir if args.screenshots_dir.exists() else None,
        heatmaps_index=args.heatmaps_index if args.heatmaps_index.exists() else None,
    )
    
    print("=" * 60)
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {args.output}")
    print("=" * 60)


if __name__ == "__main__":
    main()
