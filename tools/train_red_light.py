"""
çº¢ç¯åœå¼‚å¸¸æ£€æµ‹è®­ç»ƒç¼–æ’å™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰

è®¾è®¡ä¾æ®ï¼šDesign-ITER-2025-01.md v2.0 Â§3.5.1
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from src.traffic_rules.utils.schedulers import WarmupCosineScheduler
from tqdm import tqdm
import typer
from rich.console import Console
from rich.table import Table
from typing import Optional, List, Dict
import json
from datetime import datetime
import numpy as np

from src.traffic_rules.data.traffic_dataset import TrafficLightDataset
from src.traffic_rules.graph.builder import GraphBuilder
from src.traffic_rules.models.multi_stage_gat import MultiStageAttentionGAT
from src.traffic_rules.loss.constraint import StagedConstraintLoss
from src.traffic_rules.rules.red_light import RedLightRuleEngine
from src.traffic_rules.monitoring.gradient_monitor import GradientMonitor
from src.traffic_rules.monitoring.metrics import compute_full_metrics
from src.traffic_rules.monitoring.visualizer import TrainingVisualizer

app = typer.Typer()
console = Console()


def scene_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼ˆå¤„ç†SceneContextå¯¹è±¡ï¼‰"""
    return batch[0] if len(batch) == 1 else batch


class Trainer:
    """è®­ç»ƒç¼–æ’å™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        train_dataset,
        val_dataset=None,
        device: str = 'cpu',
        learning_rate: float = 1e-4,
        weight_decay: float = 1e-4,
        grad_clip: float = 1.0,
        epochs: int = 50,
        checkpoint_dir: str = 'artifacts/checkpoints',
    ):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.model = model.to(device)
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.device = device
        self.epochs = epochs
        self.grad_clip = grad_clip
        
        # ä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆWarmup + Cosineï¼‰
        self.scheduler = WarmupCosineScheduler(
            self.optimizer,
            warmup_epochs=min(10, epochs // 5),  # Warmupä¸ºæ€»epochsçš„20%ï¼Œæœ€å¤š10
            total_epochs=epochs,
            min_lr=1e-6,
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = StagedConstraintLoss()
        
        # è§„åˆ™å¼•æ“
        self.rule_engine = RedLightRuleEngine()
        
        # å›¾æ„å»ºå™¨
        self.graph_builder = GraphBuilder()
        
        # æ¢¯åº¦ç›‘æ§å™¨
        self.grad_monitor = GradientMonitor()
        
        # å¯è§†åŒ–å™¨
        self.visualizer = TrainingVisualizer(save_dir='reports')
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.current_stage = 1
        self.best_val_loss = float('inf')
        
        # Checkpointç›®å½•
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒå†å²
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'loss_recon': [],
            'loss_rule': [],
            'loss_attn': [],
            'grad_norms': [],
            'lr': [],
            'auc': [],
            'f1': [],
            'precision': [],
            'recall': [],
            'rule_consistency': [],
            'attention_focus': [],
        }
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = {
            'total': 0.0,
            'recon': 0.0,
            'rule': 0.0,
            'attn': 0.0,
        }
        
        num_batches = 0
        
        pbar = tqdm(range(len(self.train_dataset)), desc=f"Epoch {self.current_epoch}", leave=False)
        
        for idx in pbar:
            # ç›´æ¥è·å–å•ä¸ªåœºæ™¯ï¼ˆé¿å…DataLoader collateé—®é¢˜ï¼‰
            scene = self.train_dataset[idx]
            
            # æ„å»ºåœºæ™¯å›¾
            graph = self.graph_builder.build(scene)
            
            # è½¬ç§»åˆ°è®¾å¤‡
            x = graph.x.to(self.device)
            edge_index = graph.edge_index.to(self.device)
            entity_types = graph.entity_types.to(self.device)
            
            # è·³è¿‡æ— è¾¹çš„å›¾
            if edge_index.size(1) == 0:
                continue
            
            # å‰å‘ä¼ æ’­
            output = self.model(
                x, edge_index, entity_types,
                return_attention=True,
            )
            
            model_scores = output['scores']
            alpha_gat = output['gat_attention']
            beta_rule = output['rule_attention']
            
            # è·å–è½¦è¾†
            car_entities = scene.get_entities_by_type('car')
            
            if len(car_entities) == 0:
                continue
            
            # æå–è§„åˆ™ç›¸å…³ç‰¹å¾
            light_probs = self._get_light_probs(scene.entities).to(self.device)
            distances = torch.tensor([e.d_stop for e in car_entities], device=self.device)
            velocities = torch.tensor([e.velocity for e in car_entities], device=self.device)
            
            rule_scores = self.rule_engine.evaluate(
                light_probs, distances, velocities, training=True
            )
            
            # è®¡ç®—æŸå¤±
            loss_total, loss_dict = self.criterion(
                model_scores=model_scores,
                rule_scores=rule_scores,
                alpha_gat=alpha_gat,
                beta_rule=beta_rule,
                edge_index=edge_index,
                entity_types=entity_types,
                model_parameters=list(self.model.parameters()),
            )
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss_total.backward()
            
            # æ¢¯åº¦ç›‘æ§ï¼ˆåœ¨è£å‰ªå‰ï¼‰
            grad_stats = self.grad_monitor.monitor_step(self.model, num_batches)
            
            # æ£€æŸ¥å¼‚å¸¸
            if grad_stats['anomalies']:
                for anomaly in grad_stats['anomalies']:
                    console.print(f"[yellow]{anomaly}[/yellow]")
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                max_norm=self.grad_clip,
            )
            
            self.optimizer.step()
            
            # ç´¯ç§¯æŸå¤±
            for key in epoch_losses:
                if key in loss_dict:
                    epoch_losses[key] += loss_dict[key].item()
            
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({'loss': f"{loss_total.item():.4f}"})
        
        # å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= max(num_batches, 1)
        
        # è·å–æ¢¯åº¦ç›‘æ§æ‘˜è¦
        grad_summary = self.grad_monitor.get_summary()
        epoch_losses['grad_norm'] = grad_summary.get('grad_norm_mean', 0.0)
        
        return epoch_losses
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯é›†è¯„ä¼°ï¼ˆå«å®Œæ•´æŒ‡æ ‡ï¼‰"""
        if self.val_dataset is None:
            return {'loss': 0.0}
        
        self.model.eval()
        
        val_loss = 0.0
        num_batches = 0
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç”¨äºæŒ‡æ ‡è®¡ç®—
        all_model_scores = []
        all_rule_scores = []
        all_attention_weights = []
        all_entity_types = []
        all_edge_indices = []
        
        with torch.no_grad():
            for idx in range(len(self.val_dataset)):
                scene = self.val_dataset[idx]
                graph = self.graph_builder.build(scene)
                
                x = graph.x.to(self.device)
                edge_index = graph.edge_index.to(self.device)
                entity_types = graph.entity_types.to(self.device)
                
                if edge_index.size(1) == 0:
                    continue
                
                output = self.model(x, edge_index, entity_types, return_attention=True)
                
                car_entities = scene.get_entities_by_type('car')
                if len(car_entities) == 0:
                    continue
                
                light_probs = self._get_light_probs(scene.entities).to(self.device)
                distances = torch.tensor([e.d_stop for e in car_entities], device=self.device)
                velocities = torch.tensor([e.velocity for e in car_entities], device=self.device)
                
                rule_scores = self.rule_engine.evaluate(
                    light_probs, distances, velocities, training=False
                )
                
                loss_total, _ = self.criterion(
                    model_scores=output['scores'],
                    rule_scores=rule_scores,
                    alpha_gat=output['gat_attention'],
                    beta_rule=output['rule_attention'],
                    edge_index=edge_index,
                    entity_types=entity_types,
                    model_parameters=list(self.model.parameters()),
                )
                
                val_loss += loss_total.item()
                num_batches += 1
                
                # æ”¶é›†åˆ†æ•°ç”¨äºæŒ‡æ ‡è®¡ç®—
                all_model_scores.append(output['scores'])
                all_rule_scores.append(rule_scores)
                all_attention_weights.append(output['gat_attention'])
                all_entity_types.append(entity_types)
                all_edge_indices.append(edge_index)
        
        avg_loss = val_loss / max(num_batches, 1)
        
        # è®¡ç®—å®Œæ•´æŒ‡æ ‡
        if len(all_model_scores) > 0:
            model_scores_cat = torch.cat(all_model_scores)
            rule_scores_cat = torch.cat(all_rule_scores)
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾çš„attentionï¼ˆç®€åŒ–ï¼Œå®é™…åº”è¯¥åˆå¹¶ï¼‰
            full_metrics = compute_full_metrics(
                model_scores=model_scores_cat,
                rule_scores=rule_scores_cat,
                attention_weights=all_attention_weights[0],
                entity_types=all_entity_types[0],
                edge_index=all_edge_indices[0],
                threshold=0.7,
            )
            
            full_metrics['loss'] = avg_loss
        else:
            full_metrics = {'loss': avg_loss}
        
        return full_metrics
    
    def _check_training_health(self, epoch: int, train_metrics: Dict) -> bool:
        """
        æ£€æŸ¥è®­ç»ƒå¥åº·çŠ¶å†µ
        
        æ£€æµ‹ï¼š
            - LossæŒ¯è¡
            - æ¢¯åº¦å¼‚å¸¸
            - éªŒè¯é›†é€€åŒ–
        
        Returns:
            is_healthy: æ˜¯å¦å¥åº·
        """
        warnings = []
        
        # æ£€æµ‹LossæŒ¯è¡ï¼ˆæœ€è¿‘3ä¸ªepochsï¼‰
        if len(self.history['train_loss']) >= 3:
            recent_losses = self.history['train_loss'][-3:]
            loss_std = np.std(recent_losses)
            loss_mean = np.mean(recent_losses)
            
            if loss_std > 0.2 * loss_mean:  # æ ‡å‡†å·®è¶…è¿‡å‡å€¼çš„20%
                warnings.append(f"âš ï¸ LossæŒ¯è¡: std={loss_std:.4f}, mean={loss_mean:.4f}")
        
        # æ£€æµ‹æ¢¯åº¦å¼‚å¸¸
        grad_summary = self.grad_monitor.get_summary()
        if grad_summary.get('grad_explosion_count', 0) > 0:
            warnings.append(f"âš ï¸ æ£€æµ‹åˆ°æ¢¯åº¦çˆ†ç‚¸: {grad_summary['grad_explosion_count']}æ¬¡")
        
        if grad_summary.get('grad_vanishing_count', 0) > 0:
            warnings.append(f"âš ï¸ æ£€æµ‹åˆ°æ¢¯åº¦æ¶ˆå¤±: {grad_summary['grad_vanishing_count']}æ¬¡")
        
        # æ£€æµ‹éªŒè¯é›†é€€åŒ–ï¼ˆæœ€è¿‘3ä¸ªéªŒè¯ç‚¹ï¼‰
        if len(self.history['val_loss']) >= 3:
            recent_val = self.history['val_loss'][-3:]
            if all(recent_val[i] > recent_val[i-1] for i in range(1, len(recent_val))):
                warnings.append("âš ï¸ éªŒè¯Lossè¿ç»­ä¸Šå‡ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰")
        
        # è¾“å‡ºè­¦å‘Š
        if warnings:
            console.print(f"\n[yellow]{'='*60}[/yellow]")
            console.print(f"[bold yellow]è®­ç»ƒå¥åº·æ£€æŸ¥ (Epoch {epoch})[/bold yellow]")
            for warning in warnings:
                console.print(f"[yellow]{warning}[/yellow]")
            console.print(f"[yellow]{'='*60}[/yellow]\n")
        
        return len(warnings) == 0
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        console.print("\n[bold blue]ğŸš€ å¼€å§‹è®­ç»ƒï¼šçº¢ç¯åœå¼‚å¸¸æ£€æµ‹ MVP[/bold blue]")
        console.print(f"è®¾å¤‡: {self.device}")
        console.print(f"æ€»Epochs: {self.epochs}")
        console.print(f"è®­ç»ƒåœºæ™¯: {len(self.train_dataset)}")
        if self.val_dataset:
            console.print(f"éªŒè¯åœºæ™¯: {len(self.val_dataset)}")
        console.print()
        
        for epoch in range(self.epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_metrics = self.train_epoch()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # éªŒè¯ï¼ˆæ¯5ä¸ªepochï¼‰
            if epoch % 5 == 0 or epoch == self.epochs - 1:
                val_metrics = self.validate()
                
                # æ‰“å°æŒ‡æ ‡
                self._print_metrics(epoch, train_metrics, val_metrics)
                
                # å¥åº·æ£€æŸ¥
                self._check_training_health(epoch, train_metrics)
                
                # ä¿å­˜checkpoint
                if val_metrics['loss'] < self.best_val_loss:
                    self.best_val_loss = val_metrics['loss']
                    self.save_checkpoint(epoch, train_metrics, val_metrics, is_best=True)
            
            # è®°å½•è®­ç»ƒæŒ‡æ ‡
            self.history['train_loss'].append(train_metrics['total'])
            self.history['loss_recon'].append(train_metrics.get('recon', 0.0))
            self.history['loss_rule'].append(train_metrics.get('rule', 0.0))
            self.history['loss_attn'].append(train_metrics.get('attn', 0.0))
            self.history['grad_norms'].append(train_metrics.get('grad_norm', 0.0))
            self.history['lr'].append(self.scheduler.get_last_lr()[0])
            
            # è®°å½•éªŒè¯æŒ‡æ ‡
            if self.val_dataset:
                self.history['val_loss'].append(val_metrics.get('loss', 0.0))
                self.history['auc'].append(val_metrics.get('auc', 0.0))
                self.history['f1'].append(val_metrics.get('f1', 0.0))
                self.history['precision'].append(val_metrics.get('precision', 0.0))
                self.history['recall'].append(val_metrics.get('recall', 0.0))
                self.history['rule_consistency'].append(val_metrics.get('rule_consistency', 0.0))
                self.history['attention_focus'].append(val_metrics.get('attention_focus', 0.0))
        
        console.print("\n[bold green]âœ… è®­ç»ƒå®Œæˆï¼[/bold green]")
        
        # ç”Ÿæˆå¯è§†åŒ–
        console.print("\n[cyan]ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾...[/cyan]")
        try:
            curve_path = self.visualizer.plot_training_curves(self.history)
            console.print(f"[green]âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {curve_path}[/green]")
        except Exception as e:
            console.print(f"[yellow]âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}[/yellow]")
        
        self._print_final_summary()
    
    def _get_light_probs(self, entities: List) -> torch.Tensor:
        """æå–äº¤é€šç¯çŠ¶æ€æ¦‚ç‡"""
        lights = [e for e in entities if e.type == 'light']
        
        if len(lights) == 0:
            return torch.tensor([[0.0, 0.0, 1.0]])
        
        light = lights[0]
        state_map = {'red': 0, 'yellow': 1, 'green': 2}
        probs = torch.zeros(1, 3)
        
        if hasattr(light, 'light_state') and light.light_state:
            idx = state_map.get(light.light_state, 2)
            probs[0, idx] = light.confidence if hasattr(light, 'confidence') else 0.9
            remaining = 1.0 - probs[0, idx]
            for j in range(3):
                if j != idx:
                    probs[0, j] = remaining / 2
        else:
            probs = torch.tensor([[0.0, 0.0, 1.0]])
        
        return probs
    
    def _print_metrics(self, epoch: int, train_metrics: Dict, val_metrics: Dict):
        """æ‰“å°è®­ç»ƒæŒ‡æ ‡"""
        table = Table(title=f"Epoch {epoch} / {self.epochs}")
        
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="magenta")
        
        table.add_row("Loss (Total)", f"{train_metrics['total']:.4f}")
        table.add_row("Loss (Recon)", f"{train_metrics['recon']:.4f}")
        table.add_row("Loss (Rule)", f"{train_metrics['rule']:.4f}")
        table.add_row("Loss (Attn)", f"{train_metrics['attn']:.4f}")
        table.add_row("Grad Norm", f"{train_metrics.get('grad_norm', 0.0):.4f}")
        
        if self.val_dataset:
            table.add_row("â”€" * 12, "â”€" * 8)  # åˆ†éš”çº¿
            table.add_row("Val Loss", f"{val_metrics.get('loss', 0.0):.4f}")
            table.add_row("AUC", f"{val_metrics.get('auc', 0.0):.4f}")
            table.add_row("F1 Score", f"{val_metrics.get('f1', 0.0):.4f}")
            table.add_row("Precision", f"{val_metrics.get('precision', 0.0):.4f}")
            table.add_row("Recall", f"{val_metrics.get('recall', 0.0):.4f}")
            table.add_row("Rule Cons.", f"{val_metrics.get('rule_consistency', 0.0):.4f}")
            table.add_row("Attn Focus", f"{val_metrics.get('attention_focus', 0.0):.4f}")
        
        table.add_row("â”€" * 12, "â”€" * 8)
        table.add_row("Stage", f"{self.current_stage}")
        table.add_row("LR", f"{self.scheduler.get_last_lr()[0]:.6f}")
        
        console.print(table)
    
    def _print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        console.print("\n" + "="*60)
        console.print("[bold]è®­ç»ƒæ€»ç»“[/bold]")
        console.print("="*60)
        console.print(f"æ€»Epochs: {self.epochs}")
        console.print(f"æœ€ä½³éªŒè¯Loss: {self.best_val_loss:.4f}")
        console.print(f"Checkpointä¿å­˜åœ¨: {self.checkpoint_dir}")
        console.print("="*60)
    
    def save_checkpoint(
        self,
        epoch: int,
        train_metrics: Dict,
        val_metrics: Dict,
        is_best: bool = False,
    ):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_metrics': train_metrics,
            'val_metrics': val_metrics,
            'stage': self.current_stage,
            'history': self.history,
        }
        
        # ä¿å­˜å½“å‰epoch
        path = self.checkpoint_dir / f'checkpoint_epoch_{epoch:03d}.pth'
        torch.save(checkpoint, path)
        
        # å¦‚æœæ˜¯æœ€ä½³ï¼Œé¢å¤–ä¿å­˜
        if is_best:
            best_path = self.checkpoint_dir / 'best.pth'
            torch.save(checkpoint, best_path)
            console.print(f"[green]âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: Val Loss={val_metrics.get('loss', 0.0):.4f}[/green]")


@app.command()
def train(
    data_root: str = typer.Option("data/synthetic", help="æ•°æ®æ ¹ç›®å½•"),
    epochs: int = typer.Option(50, help="è®­ç»ƒè½®æ•°"),
    lr: float = typer.Option(1e-4, help="å­¦ä¹ ç‡"),
    device: str = typer.Option("cpu", help="è®¾å¤‡: cpu/cuda"),
    checkpoint_dir: str = typer.Option("artifacts/checkpoints", help="Checkpointç›®å½•"),
    max_samples: Optional[int] = typer.Option(None, help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰"),
):
    """
    è®­ç»ƒçº¢ç¯åœå¼‚å¸¸æ£€æµ‹æ¨¡å‹
    
    Example:
        python tools/train_red_light.py train --epochs 20 --device cpu
    """
    console.print("[bold blue]ğŸ”§ åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ...[/bold blue]")
    
    # åŠ è½½æ•°æ®é›†
    console.print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
    try:
        train_dataset = TrafficLightDataset(
            data_root=data_root,
            mode='synthetic',
            split='train',
            max_samples=max_samples,
        )
    except FileNotFoundError as e:
        console.print(f"[red]âŒ é”™è¯¯: {e}[/red]")
        console.print("[yellow]è¯·å…ˆè¿è¡Œ: python3 scripts/prepare_synthetic_data.py --num-scenes 100[/yellow]")
        raise typer.Exit(1)
    
    try:
        val_dataset = TrafficLightDataset(
            data_root=data_root,
            mode='synthetic',
            split='val',
            max_samples=max_samples,
        )
    except FileNotFoundError:
        console.print("[yellow]âš ï¸  æœªæ‰¾åˆ°éªŒè¯é›†ï¼Œä»…ä½¿ç”¨è®­ç»ƒé›†[/yellow]")
        val_dataset = None
    
    console.print(f"[green]âœ… è®­ç»ƒé›†: {len(train_dataset)} åœºæ™¯[/green]")
    if val_dataset:
        console.print(f"[green]âœ… éªŒè¯é›†: {len(val_dataset)} åœºæ™¯[/green]")
    
    # åˆå§‹åŒ–æ¨¡å‹
    console.print("\nğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
    model = MultiStageAttentionGAT(
        input_dim=10,
        hidden_dim=128,
        num_gat_layers=3,
        num_heads=8,
        dropout=0.1,
    )
    
    # ç»Ÿè®¡å‚æ•°é‡
    num_params = sum(p.numel() for p in model.parameters())
    console.print(f"[green]âœ… æ¨¡å‹å‚æ•°é‡: {num_params:,} (~{num_params/1e6:.2f}M)[/green]")
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        device=device,
        learning_rate=lr,
        epochs=epochs,
        checkpoint_dir=checkpoint_dir,
    )
    
    # å¼€å§‹è®­ç»ƒ
    console.print(f"\n[bold yellow]ğŸš€ å¼€å§‹è®­ç»ƒ...[/bold yellow]\n")
    trainer.train()
    
    console.print(f"\n[bold green]âœ… è®­ç»ƒå®Œæˆï¼Checkpointä¿å­˜åœ¨: {checkpoint_dir}[/bold green]")


@app.command()
def info():
    """æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯"""
    console.print("\n[bold]ğŸ“‹ æ¨¡å‹ä¿¡æ¯[/bold]\n")
    console.print(f"è®¾è®¡æ–‡æ¡£: Design-ITER-2025-01.md v2.0")
    console.print(f"ç®—æ³•æ–¹æ¡ˆ: å¤šé˜¶æ®µGAT + ç¡¬çº¦æŸè§„åˆ™èåˆ\n")
    console.print(f"[cyan]æ¨¡å‹æ¶æ„:[/cyan]")
    console.print(f"  â€¢ é˜¶æ®µ1: å±€éƒ¨GATï¼ˆ3å±‚Ã—8å¤´ï¼‰")
    console.print(f"  â€¢ é˜¶æ®µ2: å…¨å±€è™šæ‹ŸèŠ‚ç‚¹æ³¨æ„åŠ›ï¼ˆ4å¤´ï¼‰")
    console.print(f"  â€¢ é˜¶æ®µ3: è§„åˆ™èšç„¦æ³¨æ„åŠ›")
    console.print(f"  â€¢ è¾“å…¥ç»´åº¦: 10")
    console.print(f"  â€¢ éšè—ç»´åº¦: 128")
    console.print(f"  â€¢ æ€»å‚æ•°é‡: ~1.02M\n")


if __name__ == "__main__":
    app()
