"""
æ¢¯åº¦ç›‘æ§å™¨

å®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æµï¼Œæ£€æµ‹å¼‚å¸¸
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional
from collections import defaultdict


class GradientMonitor:
    """
    æ¢¯åº¦ç›‘æ§å™¨
    
    åŠŸèƒ½ï¼š
        - è®¡ç®—å„å±‚æ¢¯åº¦èŒƒæ•°
        - æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
        - è®°å½•æƒé‡æ›´æ–°é‡
        - ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
    
    ä½¿ç”¨æ–¹æ³•ï¼š
        monitor = GradientMonitor()
        
        # è®­ç»ƒå¾ªç¯ä¸­
        loss.backward()
        stats = monitor.monitor_step(model, optimizer, step)
        if stats['anomalies']:
            print(f"è­¦å‘Š: {stats['anomalies']}")
        optimizer.step()
    """
    
    def __init__(
        self,
        grad_explosion_threshold: float = 10.0,
        grad_vanishing_threshold: float = 1e-5,
        imbalance_ratio_threshold: float = 100.0,
    ):
        """
        åˆå§‹åŒ–ç›‘æ§å™¨
        
        Args:
            grad_explosion_threshold: æ¢¯åº¦çˆ†ç‚¸é˜ˆå€¼
            grad_vanishing_threshold: æ¢¯åº¦æ¶ˆå¤±é˜ˆå€¼
            imbalance_ratio_threshold: æ¢¯åº¦ä¸å¹³è¡¡æ¯”ä¾‹é˜ˆå€¼
        """
        self.grad_explosion_threshold = grad_explosion_threshold
        self.grad_vanishing_threshold = grad_vanishing_threshold
        self.imbalance_ratio_threshold = imbalance_ratio_threshold
        
        # å†å²è®°å½•
        self.grad_history = []
        self.weight_history = []
    
    def compute_grad_norms(self, model: nn.Module) -> Dict[str, float]:
        """
        è®¡ç®—å„å±‚æ¢¯åº¦èŒƒæ•°
        
        Args:
            model: PyTorchæ¨¡å‹
        
        Returns:
            grad_norms: å„å‚æ•°çš„æ¢¯åº¦èŒƒæ•°å­—å…¸
        """
        grad_norms = {}
        
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norms[name] = param.grad.norm().item()
            else:
                grad_norms[name] = 0.0
        
        return grad_norms
    
    def compute_layer_stats(self, grad_norms: Dict[str, float]) -> Dict[str, float]:
        """
        æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡æ¢¯åº¦
        
        Args:
            grad_norms: å„å‚æ•°çš„æ¢¯åº¦èŒƒæ•°
        
        Returns:
            layer_stats: å„å±‚çš„å¹³å‡æ¢¯åº¦èŒƒæ•°
        """
        layer_groups = defaultdict(list)
        
        # åˆ†ç»„
        for name, norm in grad_norms.items():
            # æå–å±‚åç§°ï¼ˆå¦‚ 'local_gat.gat_layers.0.W.0.weight' â†’ 'local_gat'ï¼‰
            layer_name = name.split('.')[0]
            layer_groups[layer_name].append(norm)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        layer_stats = {}
        for layer_name, norms in layer_groups.items():
            layer_stats[layer_name] = {
                'mean': np.mean(norms),
                'max': np.max(norms),
                'min': np.min(norms),
                'std': np.std(norms),
            }
        
        return layer_stats
    
    def detect_anomalies(
        self,
        total_norm: float,
        layer_stats: Dict[str, Dict[str, float]],
    ) -> List[str]:
        """
        æ£€æµ‹æ¢¯åº¦å¼‚å¸¸
        
        Args:
            total_norm: æ€»æ¢¯åº¦èŒƒæ•°
            layer_stats: å„å±‚ç»Ÿè®¡é‡
        
        Returns:
            anomalies: å¼‚å¸¸åˆ—è¡¨
        """
        anomalies = []
        
        # æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸
        if total_norm > self.grad_explosion_threshold:
            anomalies.append(f"ğŸ”´ æ¢¯åº¦çˆ†ç‚¸: total_norm={total_norm:.2f} > {self.grad_explosion_threshold}")
        
        # æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±
        if total_norm < self.grad_vanishing_threshold:
            anomalies.append(f"ğŸ”´ æ¢¯åº¦æ¶ˆå¤±: total_norm={total_norm:.2e} < {self.grad_vanishing_threshold}")
        
        # æ£€æµ‹å„å±‚æ¢¯åº¦ä¸å¹³è¡¡
        layer_means = [stats['mean'] for stats in layer_stats.values()]
        if len(layer_means) > 1:
            max_mean = max(layer_means)
            min_mean = min(layer_means) + 1e-10
            imbalance_ratio = max_mean / min_mean
            
            if imbalance_ratio > self.imbalance_ratio_threshold:
                anomalies.append(
                    f"âš ï¸ æ¢¯åº¦ä¸å¹³è¡¡: max/min={imbalance_ratio:.1f} > {self.imbalance_ratio_threshold}"
                )
        
        # æ£€æµ‹å„å±‚å†…éƒ¨æ˜¯å¦æœ‰æ¢¯åº¦æ¶ˆå¤±
        for layer_name, stats in layer_stats.items():
            if stats['mean'] < self.grad_vanishing_threshold:
                anomalies.append(f"âš ï¸ {layer_name}å±‚æ¢¯åº¦æ¶ˆå¤±: mean={stats['mean']:.2e}")
        
        return anomalies
    
    def compute_weight_updates(
        self,
        model: nn.Module,
        prev_weights: Optional[Dict[str, torch.Tensor]] = None,
    ) -> Dict[str, float]:
        """
        è®¡ç®—æƒé‡æ›´æ–°é‡
        
        Args:
            model: PyTorchæ¨¡å‹
            prev_weights: ä¸Šä¸€æ­¥çš„æƒé‡ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            update_norms: å„å‚æ•°çš„æ›´æ–°é‡èŒƒæ•°
        """
        if prev_weights is None:
            return {}
        
        update_norms = {}
        
        for name, param in model.named_parameters():
            if name in prev_weights:
                delta = param.data - prev_weights[name]
                update_norms[name] = delta.norm().item()
        
        return update_norms
    
    def monitor_step(
        self,
        model: nn.Module,
        step: int,
        prev_weights: Optional[Dict[str, torch.Tensor]] = None,
    ) -> Dict[str, any]:
        """
        å•æ­¥ç›‘æ§ï¼ˆåœ¨backward()åã€optimizer.step()å‰è°ƒç”¨ï¼‰
        
        Args:
            model: PyTorchæ¨¡å‹
            step: å½“å‰æ­¥æ•°
            prev_weights: ä¸Šä¸€æ­¥æƒé‡ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            stats: ç›‘æ§ç»Ÿè®¡ä¿¡æ¯
        """
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        grad_norms = self.compute_grad_norms(model)
        
        # æ€»æ¢¯åº¦èŒƒæ•°
        total_norm = np.sqrt(sum(v**2 for v in grad_norms.values() if v > 0))
        
        # åˆ†å±‚ç»Ÿè®¡
        layer_stats = self.compute_layer_stats(grad_norms)
        
        # å¼‚å¸¸æ£€æµ‹
        anomalies = self.detect_anomalies(total_norm, layer_stats)
        
        # æƒé‡æ›´æ–°é‡ï¼ˆå¦‚æœæä¾›äº†å‰ä¸€æ­¥æƒé‡ï¼‰
        update_norms = self.compute_weight_updates(model, prev_weights)
        
        # ç»„è£…ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'step': step,
            'total_norm': total_norm,
            'layer_stats': layer_stats,
            'grad_norms': grad_norms,
            'update_norms': update_norms,
            'anomalies': anomalies,
        }
        
        # è®°å½•å†å²
        self.grad_history.append({
            'step': step,
            'total_norm': total_norm,
            'layer_means': {k: v['mean'] for k, v in layer_stats.items()},
        })
        
        return stats
    
    def get_summary(self) -> Dict[str, any]:
        """
        è·å–ç›‘æ§æ€»ç»“
        
        Returns:
            summary: ç»Ÿè®¡æ‘˜è¦
        """
        if len(self.grad_history) == 0:
            return {}
        
        # æå–æ€»æ¢¯åº¦èŒƒæ•°å†å²
        total_norms = [h['total_norm'] for h in self.grad_history]
        
        summary = {
            'num_steps': len(self.grad_history),
            'grad_norm_mean': np.mean(total_norms),
            'grad_norm_std': np.std(total_norms),
            'grad_norm_max': np.max(total_norms),
            'grad_norm_min': np.min(total_norms),
            'grad_explosion_count': sum(1 for n in total_norms if n > self.grad_explosion_threshold),
            'grad_vanishing_count': sum(1 for n in total_norms if n < self.grad_vanishing_threshold),
        }
        
        return summary
    
    def save_weights_snapshot(self, model: nn.Module) -> Dict[str, torch.Tensor]:
        """
        ä¿å­˜å½“å‰æƒé‡å¿«ç…§ï¼ˆç”¨äºä¸‹ä¸€æ­¥è®¡ç®—æ›´æ–°é‡ï¼‰
        
        Args:
            model: PyTorchæ¨¡å‹
        
        Returns:
            weights: æƒé‡å­—å…¸
        """
        weights = {}
        for name, param in model.named_parameters():
            weights[name] = param.data.clone()
        return weights


# å¯¼å‡ºæ¥å£
__all__ = ['GradientMonitor']
